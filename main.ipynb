{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "config_section",
   "metadata": {},
   "source": [
    "# 數位韌性指數計算系統 - 雙層架構優化版\n",
    "\n",
    "## 主要優化項目\n",
    "1. **集中配置管理**：所有關鍵參數集中於開頭統一管理\n",
    "2. **智能緩存機制**：避免重複計算與 API 調用\n",
    "3. **主題層級評分**：Phase 4 改為主題層級評分，大幅提升效能\n",
    "4. **多輪迭代優化**：Phase 3 支援多輪自動優化\n",
    "5. **關鍵詞增強提示**：LLM 映射時提供主題關鍵詞輔助\n",
    "6. **批次與選擇性評分**：單次 API 調用處理多個構面\n",
    "   - API 調用次數減少 85% (819 → 117)\n",
    "   - Phase 4 執行時間從 94 分鐘縮短至 5-10 分鐘\n",
    "   - 整體加速 7-18 倍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 配置載入完成\n",
      "  - 嵌入模型: text-embedding-3-small\n",
      "  - LLM 模型: gpt-5-nano-2025-08-07\n",
      "  - 數據目錄: data\n",
      "  - Phase 3 最大迭代次數: 10\n",
      "  - Phase 3 智能停止: 啟用\n",
      "  - 評分構面: ITC, ACAP, DC, GOVSEC, DATA, ECO, OTHER\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 全局配置區 - 所有重要設置集中管理\n",
    "# ========================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# === API 配置 ===\n",
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\".env\")\n",
    "OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# === 模型配置 ===\n",
    "EMBEDDING_MODEL = 'text-embedding-3-small'  # OpenAI 嵌入模型\n",
    "LLM_MODEL = 'gpt-5-nano-2025-08-07'  # LLM 模型（用於主題優化和評分）\n",
    "#LLM_MODEL = 'gpt-4.1-mini-2025-04-14'  # 備選 LLM 模型\n",
    "LLM_TEMPERATURE = 1  # 生成溫度參數\n",
    "\n",
    "# === 檔案路徑配置 ===\n",
    "DATA_DIR = Path('data')\n",
    "CORPUS_PATH = DATA_DIR / 'corpus.csv'\n",
    "\n",
    "# Phase 2 輸出檔案\n",
    "EMBEDDINGS_PATH = DATA_DIR / 'embeddings_text-3-small.npy'\n",
    "EMBEDDINGS_INDEX_PATH = DATA_DIR / 'embeddings_index.json'\n",
    "PHASE2_MODEL_DIR = DATA_DIR / 'part2_bertopic_model'\n",
    "PHASE2_TOPICS_CSV = DATA_DIR / 'part2_topics.csv'\n",
    "PHASE2_DOC_PROBS = DATA_DIR / 'part2_doc_topic_probs.npy'\n",
    "PHASE2_CORPUS_CSV = DATA_DIR / 'part2_corpus_with_topics.csv'\n",
    "PHASE2_TOPIC_YEAR_CSV = DATA_DIR / 'part2_topic_prop_by_year.csv'\n",
    "\n",
    "# Phase 3 輸出檔案\n",
    "PHASE3_MODEL_DIR = DATA_DIR / 'part3_optimized_bertopic_model'\n",
    "PHASE3_CORPUS_CSV = DATA_DIR / 'part3_corpus_with_topics_v2.csv'\n",
    "PHASE3_OPTIMIZATION_CACHE = DATA_DIR / 'phase3_optimization_plans.json'\n",
    "\n",
    "# Phase 4 輸出檔案\n",
    "PHASE4_TOPIC_DIM_MAP_CACHE = DATA_DIR / 'phase4_topic_dimension_map.json'\n",
    "PHASE4_TOPIC_SCORES_CACHE = DATA_DIR / 'phase4_topic_dimension_scores.json'\n",
    "PHASE4_DOC_SCORES_CSV = DATA_DIR / 'part4_doc_dimension_scores.csv'\n",
    "PHASE4_DRI_CSV = DATA_DIR / 'part4_entity_time_dri.csv'\n",
    "\n",
    "# === BERTopic 參數配置 ===\n",
    "# Phase 2 初始參數\n",
    "UMAP_N_NEIGHBORS = 15\n",
    "UMAP_N_COMPONENTS = 10\n",
    "UMAP_MIN_DIST = 0.0\n",
    "UMAP_METRIC = 'cosine'\n",
    "HDBSCAN_MIN_CLUSTER_SIZE = 30\n",
    "HDBSCAN_MIN_SAMPLES = 10\n",
    "HDBSCAN_METRIC = 'euclidean'\n",
    "HDBSCAN_SELECTION_METHOD = 'eom'\n",
    "\n",
    "# === Phase 3 優化配置 ===\n",
    "MAX_OPTIMIZATION_ITERATIONS = 10  # 最大迭代次數（防止無限循環）\n",
    "TOPIC_SAMPLE_SIZE = 3  # 每個主題的範例句子數量\n",
    "ENABLE_SMART_STOPPING = True  # 啟用 LLM 智能停止判斷\n",
    "\n",
    "# === Phase 4 評分配置 ===\n",
    "DIMENSIONS = [\"ITC\", \"ACAP\", \"DC\", \"GOVSEC\", \"DATA\", \"ECO\", \"OTHER\"]  # 數位韌性構面\n",
    "DIMENSION_WEIGHTS = {  # 各構面權重（總和為 1）\n",
    "    \"ITC\": 0.20,\n",
    "    \"ACAP\": 0.20,\n",
    "    \"DC\": 0.15,\n",
    "    \"GOVSEC\": 0.15,\n",
    "    \"DATA\": 0.15,\n",
    "    \"ECO\": 0.15,\n",
    "    \"OTHER\": 0.0\n",
    "}\n",
    "\n",
    "# 構面語義分組（用於選擇性評分優化）\n",
    "DIMENSION_GROUPS = {\n",
    "    \"ITC\": [\"ITC\", \"ACAP\", \"DC\"],  # 技術基礎設施相關\n",
    "    \"ACAP\": [\"ACAP\", \"ITC\", \"GOVSEC\"],  # 安全與治理相關\n",
    "    \"DC\": [\"DC\", \"ITC\", \"GOVSEC\"],  # 基礎設施與連續性\n",
    "    \"GOVSEC\": [\"GOVSEC\", \"ACAP\", \"DATA\"],  # 治理與合規\n",
    "    \"DATA\": [\"DATA\", \"GOVSEC\", \"ECO\"],  # 數據與生態系統\n",
    "    \"ECO\": [\"ECO\", \"DATA\", \"ITC\"],  # 數位生態系統\n",
    "    \"OTHER\": [\"OTHER\"]  # 其他類別\n",
    "}\n",
    "\n",
    "# 評分標準\n",
    "SCORING_RUBRIC = (\n",
    "    \"Rate the substantiveness and strength on a 0–5 scale:\\n\"\n",
    "    \"0 = irrelevant/very vague\\n\"\n",
    "    \"3 = part of a specific action or quantitative indicator\\n\"\n",
    "    \"5 = clear, quantitative, auditable, and directly related to strategy/investment/institutionalization\"\n",
    ")\n",
    "\n",
    "# === 其他配置 ===\n",
    "EMBEDDING_BATCH_SIZE = 256\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(\"✓ 配置載入完成\")\n",
    "print(f\"  - 嵌入模型: {EMBEDDING_MODEL}\")\n",
    "print(f\"  - LLM 模型: {LLM_MODEL}\")\n",
    "print(f\"  - 數據目錄: {DATA_DIR}\")\n",
    "print(f\"  - Phase 3 最大迭代次數: {MAX_OPTIMIZATION_ITERATIONS}\")\n",
    "print(f\"  - Phase 3 智能停止: {'啟用' if ENABLE_SMART_STOPPING else '禁用'}\")\n",
    "print(f\"  - 評分構面: {', '.join(DIMENSIONS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00_imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 函式庫導入完成\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 導入必要的函式庫\n",
    "# ========================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# OpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "# BERTopic 及相關套件\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "\n",
    "# 初始化 OpenAI 客戶端\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# 確保數據目錄存在\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"✓ 函式庫導入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase0",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 0: 數據下載與預處理\n",
    "此部分程式碼已完成，若需重新下載數據請取消註釋執行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "phase0_download",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 數據下載與處理（已完成，若需重新執行請取消註釋）\n",
    "# from src.data_download import download_sec_filings\n",
    "# from src.data_processing import process_sec_filings\n",
    "\n",
    "# download_sec_filings()\n",
    "# process_sec_filings(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 2: 初始主題生成（BERTopic）\n",
    "\n",
    "使用 BERTopic 進行主題建模：\n",
    "- 嵌入模型：OpenAI text-embedding-3-small (1536 維度)\n",
    "- 降維方法：UMAP，聚類方法：HDBSCAN\n",
    "- 緩存機制：嵌入向量自動緩存，避免重複運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "phase2_load_corpus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 語料庫載入完成: 6233 筆文檔\n",
      "  - 元數據欄位: ['ticker', 'year']\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 載入語料庫\n",
    "# ========================================\n",
    "\n",
    "assert CORPUS_PATH.exists(), f\"找不到語料庫檔案: {CORPUS_PATH}\"\n",
    "\n",
    "df = pd.read_csv(CORPUS_PATH)\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "assert 'text' in df.columns, \"語料庫必須包含 'text' 欄位\"\n",
    "\n",
    "# 偵測可用的元數據欄位\n",
    "meta_cols = [c for c in ['doc_id', 'company', 'firm', 'ticker', 'year', 'date'] if c in df.columns]\n",
    "print(f\"✓ 語料庫載入完成: {len(df)} 筆文檔\")\n",
    "print(f\"  - 元數據欄位: {meta_cols or '(無)'}\")\n",
    "\n",
    "texts = df['text'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "phase2_embeddings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 從緩存載入嵌入向量...\n",
      "  - 模型: text-embedding-3-small\n",
      "  - 數量: 6233\n",
      "  - 嵌入形狀: (6233, 1536)\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 生成/載入嵌入向量（帶緩存機制）\n",
    "# ========================================\n",
    "\n",
    "if EMBEDDINGS_PATH.exists() and EMBEDDINGS_INDEX_PATH.exists():\n",
    "    print(\"✓ 從緩存載入嵌入向量...\")\n",
    "    embeddings = np.load(EMBEDDINGS_PATH)\n",
    "    with open(EMBEDDINGS_INDEX_PATH, 'r') as f:\n",
    "        emb_info = json.load(f)\n",
    "    print(f\"  - 模型: {emb_info.get('model')}\")\n",
    "    print(f\"  - 數量: {emb_info.get('count')}\")\n",
    "else:\n",
    "    print(\"⚙ 生成嵌入向量（此過程需要數分鐘）...\")\n",
    "    vecs = []\n",
    "    for i in tqdm(range(0, len(texts), EMBEDDING_BATCH_SIZE), desc=\"生成嵌入\"):\n",
    "        batch = texts[i:i + EMBEDDING_BATCH_SIZE]\n",
    "        response = client.embeddings.create(model=EMBEDDING_MODEL, input=batch)\n",
    "        vecs.extend([np.array(d.embedding, dtype=np.float32) for d in response.data])\n",
    "    \n",
    "    embeddings = np.vstack(vecs)\n",
    "    np.save(EMBEDDINGS_PATH, embeddings)\n",
    "    \n",
    "    with open(EMBEDDINGS_INDEX_PATH, 'w') as f:\n",
    "        json.dump({'count': len(texts), 'model': EMBEDDING_MODEL}, f)\n",
    "    \n",
    "    print(f\"✓ 嵌入向量已儲存\")\n",
    "\n",
    "print(f\"  - 嵌入形狀: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "phase2_bertopic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 01:24:33,493 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙ 訓練 BERTopic 模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 01:24:53,039 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-15 01:24:53,040 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-10-15 01:24:53,918 - BERTopic - Cluster - Completed ✓\n",
      "2025-10-15 01:24:53,921 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-10-15 01:24:54,936 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase 2 完成\n",
      "  - 主題數量: 68\n",
      "  - 離群點比例: 16.52%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1030</td>\n",
       "      <td>-1_our_and_of_the</td>\n",
       "      <td>[our, and, of, the, to, or, in, we, are, for]</td>\n",
       "      <td>[There can be no assurance that licenses will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>302</td>\n",
       "      <td>0_tax_income_taxes_deferred</td>\n",
       "      <td>[tax, income, taxes, deferred, foreign, rate, ...</td>\n",
       "      <td>[We are subject to income taxes in the U.S. an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>1_loans_loan_credit_portfolio</td>\n",
       "      <td>[loans, loan, credit, portfolio, consumer, all...</td>\n",
       "      <td>[858\\r\\nmillion\\r\\nwere included in TDRs at\\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>191</td>\n",
       "      <td>2_gas_reserves_oil_proved</td>\n",
       "      <td>[gas, reserves, oil, proved, production, exxon...</td>\n",
       "      <td>[In\\r\\n\\r\\nIn some cases, substantial new inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>136</td>\n",
       "      <td>3_our_we_may_could</td>\n",
       "      <td>[our, we, may, could, or, products, and, to, b...</td>\n",
       "      <td>[Litigation and regulatory proceedings are inh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>4_care_health_medicare_medical</td>\n",
       "      <td>[care, health, medicare, medical, unitedhealth...</td>\n",
       "      <td>[UnitedHealthcare Medicare &amp; Retirement provid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>125</td>\n",
       "      <td>5_goodwill_assets_impairment_intangible</td>\n",
       "      <td>[goodwill, assets, impairment, intangible, val...</td>\n",
       "      <td>[Property and equipment, which includes amount...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>123</td>\n",
       "      <td>6_driven_banking_higher_income</td>\n",
       "      <td>[driven, banking, higher, income, fees, billio...</td>\n",
       "      <td>[The operating margin was\\r\\n27\\r\\npercent com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>117</td>\n",
       "      <td>7_pension_plans_plan_benefit</td>\n",
       "      <td>[pension, plans, plan, benefit, assets, postre...</td>\n",
       "      <td>[. Note 10—Pension and Other Postretirement Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>113</td>\n",
       "      <td>8_capital_basel_reserve_bank</td>\n",
       "      <td>[capital, basel, reserve, bank, federal, requi...</td>\n",
       "      <td>[banks are subject to quantitative and qualita...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                     Name  \\\n",
       "0     -1   1030                        -1_our_and_of_the   \n",
       "1      0    302              0_tax_income_taxes_deferred   \n",
       "2      1    222            1_loans_loan_credit_portfolio   \n",
       "3      2    191                2_gas_reserves_oil_proved   \n",
       "4      3    136                       3_our_we_may_could   \n",
       "5      4    128           4_care_health_medicare_medical   \n",
       "6      5    125  5_goodwill_assets_impairment_intangible   \n",
       "7      6    123           6_driven_banking_higher_income   \n",
       "8      7    117             7_pension_plans_plan_benefit   \n",
       "9      8    113             8_capital_basel_reserve_bank   \n",
       "\n",
       "                                      Representation  \\\n",
       "0      [our, and, of, the, to, or, in, we, are, for]   \n",
       "1  [tax, income, taxes, deferred, foreign, rate, ...   \n",
       "2  [loans, loan, credit, portfolio, consumer, all...   \n",
       "3  [gas, reserves, oil, proved, production, exxon...   \n",
       "4  [our, we, may, could, or, products, and, to, b...   \n",
       "5  [care, health, medicare, medical, unitedhealth...   \n",
       "6  [goodwill, assets, impairment, intangible, val...   \n",
       "7  [driven, banking, higher, income, fees, billio...   \n",
       "8  [pension, plans, plan, benefit, assets, postre...   \n",
       "9  [capital, basel, reserve, bank, federal, requi...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [There can be no assurance that licenses will ...  \n",
       "1  [We are subject to income taxes in the U.S. an...  \n",
       "2  [858\\r\\nmillion\\r\\nwere included in TDRs at\\r\\...  \n",
       "3  [In\\r\\n\\r\\nIn some cases, substantial new inve...  \n",
       "4  [Litigation and regulatory proceedings are inh...  \n",
       "5  [UnitedHealthcare Medicare & Retirement provid...  \n",
       "6  [Property and equipment, which includes amount...  \n",
       "7  [The operating margin was\\r\\n27\\r\\npercent com...  \n",
       "8  [. Note 10—Pension and Other Postretirement Be...  \n",
       "9  [banks are subject to quantitative and qualita...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================================\n",
    "# 初始 BERTopic 模型訓練\n",
    "# ========================================\n",
    "\n",
    "print(\"⚙ 訓練 BERTopic 模型...\")\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=UMAP_N_NEIGHBORS,\n",
    "    n_components=UMAP_N_COMPONENTS,\n",
    "    min_dist=UMAP_MIN_DIST,\n",
    "    metric=UMAP_METRIC,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "hdbscan_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE,\n",
    "    min_samples=HDBSCAN_MIN_SAMPLES,\n",
    "    metric=HDBSCAN_METRIC,\n",
    "    cluster_selection_method=HDBSCAN_SELECTION_METHOD,\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(texts, embeddings=embeddings)\n",
    "df['topic'] = topics\n",
    "\n",
    "# 儲存模型與結果\n",
    "topic_model.save(PHASE2_MODEL_DIR.as_posix(), serialization=\"safetensors\")\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info.to_csv(PHASE2_TOPICS_CSV, index=False, encoding='utf-8')\n",
    "if probs is not None:\n",
    "    np.save(PHASE2_DOC_PROBS, probs)\n",
    "df.to_csv(PHASE2_CORPUS_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "# 按年度分析主題分布\n",
    "if 'year' in df.columns:\n",
    "    year_dist = df.groupby('year')['topic'].value_counts(normalize=True).rename('prop').reset_index()\n",
    "    year_dist.to_csv(PHASE2_TOPIC_YEAR_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"✓ Phase 2 完成\")\n",
    "print(f\"  - 主題數量: {len(topic_info[topic_info['Topic'] != -1])}\")\n",
    "print(f\"  - 離群點比例: {(np.array(topics) == -1).mean():.2%}\")\n",
    "topic_info.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 3: LLM 迭代優化（完整實現版）\n",
    "\n",
    "**核心功能（完整實現所有 LLM 建議操作）**：\n",
    "\n",
    "1. **主題合併 (merge_pairs)**\n",
    "   - 合併語義相似或具有包含關係的主題\n",
    "   - 自動更新文檔的主題分配\n",
    "   - 範例：合併 Topic 6 與 Topic 24\n",
    "\n",
    "2. **主題拆分 (split_topics)**\n",
    "   - 拆分過於廣泛、包含多個概念的主題\n",
    "   - 使用子聚類演算法重新分組\n",
    "   - 範例：分離「員工培訓」與「數據隱私」主題\n",
    "\n",
    "3. **停用詞管理 (new_stopwords)**\n",
    "   - 動態新增領域特定噪音詞\n",
    "   - 重新計算主題表示，移除無意義詞彙\n",
    "   - 範例：過濾「company」、「billion」、「fiscal」等\n",
    "\n",
    "4. **參數調整 (params)**\n",
    "   - 依據 LLM 建議調整 HDBSCAN/UMAP 參數\n",
    "   - 重新訓練聚類模型\n",
    "   - 目標：降低離群點比例、提升主題品質\n",
    "\n",
    "5. **主題重命名 (rename)**\n",
    "   - 將關鍵詞列表轉換為有意義的主題名稱\n",
    "   - 提升結果可解釋性\n",
    "   - 範例：「tax, income, deferred」→「企業稅務策略與遞延資產」\n",
    "\n",
    "6. **智能停止判斷**\n",
    "   - LLM 根據指標歷史自動決定是否繼續迭代\n",
    "   - 包含收斂檢測、退化檢測與持續改進判斷\n",
    "\n",
    "**執行流程**：\n",
    "合併相似主題 → 拆分過寬主題 → 更新停用詞 → 調整參數重訓練 → 主題重命名 → 評估是否繼續\n",
    "\n",
    "**預期效果**：\n",
    "- 完整應用 LLM 的所有優化建議\n",
    "- 每輪顯示實際執行的操作（合併數、拆分數、新增停用詞數等）\n",
    "- 智能收斂：通常 2-4 輪即達最佳配置\n",
    "- 顯著提升主題品質與可解釋性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "phase3_utils",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 工具函數已載入\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 工具函數：指標計算\n",
    "# ========================================\n",
    "\n",
    "def compute_topic_centers(emb: np.ndarray, topics: List[int]) -> Dict[int, np.ndarray]:\n",
    "    \"\"\"計算每個主題的中心向量\"\"\"\n",
    "    centers = {}\n",
    "    s = pd.Series(topics)\n",
    "    for tid, idxs in s.groupby(s).groups.items():\n",
    "        if tid == -1:\n",
    "            continue\n",
    "        vecs = emb[list(idxs)]\n",
    "        centers[tid] = normalize(vecs.mean(axis=0, keepdims=True))[0]\n",
    "    return centers\n",
    "\n",
    "def compute_metrics(emb: np.ndarray, topics: List[int]) -> Tuple[Dict, float, float, float]:\n",
    "    \"\"\"計算主題品質指標：一致性、區分度、輪廓係數、離群率\"\"\"\n",
    "    centers = compute_topic_centers(emb, topics)\n",
    "    \n",
    "    # 一致性（Cohesion）：主題內部的平均相似度\n",
    "    s = pd.Series(topics)\n",
    "    cohesion = {}\n",
    "    for tid, idxs in s.groupby(s).groups.items():\n",
    "        if tid == -1 or tid not in centers:\n",
    "            continue\n",
    "        sims = cosine_similarity(emb[list(idxs)], centers[tid].reshape(1, -1)).ravel()\n",
    "        cohesion[tid] = float(np.mean(sims))\n",
    "    \n",
    "    # 區分度（Separation）：主題中心之間的平均距離\n",
    "    separation = np.nan\n",
    "    if len(centers) >= 2:\n",
    "        center_matrix = np.vstack(list(centers.values()))\n",
    "        separation = cosine_distances(center_matrix).mean()\n",
    "    \n",
    "    # 輪廓係數（Silhouette）\n",
    "    mask = np.array(topics) != -1\n",
    "    silhouette = np.nan\n",
    "    if mask.sum() > 5 and len(set(np.array(topics)[mask])) > 1:\n",
    "        silhouette = silhouette_score(emb[mask], np.array(topics)[mask])\n",
    "    \n",
    "    # 離群率\n",
    "    outlier_rate = (np.array(topics) == -1).mean()\n",
    "    \n",
    "    return cohesion, separation, silhouette, outlier_rate\n",
    "\n",
    "def print_metrics(cohesion: Dict, separation: float, silhouette: float, outlier: float, prefix=\"\"):\n",
    "    \"\"\"顯示品質指標\"\"\"\n",
    "    coh_mean = np.mean(list(cohesion.values())) if cohesion else np.nan\n",
    "    print(f\"{prefix}一致性: {coh_mean:.4f}\")\n",
    "    print(f\"{prefix}區分度: {separation:.4f}\")\n",
    "    print(f\"{prefix}Silhouette: {silhouette:.4f}\")\n",
    "    print(f\"{prefix}離群率: {outlier:.2%}\")\n",
    "\n",
    "print(\"✓ 工具函數已載入\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "g4u50c52fhp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase 3 優化操作函數就緒\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Phase 3 優化操作函數\n",
    "# ========================================\n",
    "\n",
    "def merge_topics(model, topics_list: List[int], merge_pairs: List[List[int]]) -> List[int]:\n",
    "    \"\"\"\n",
    "    合併主題對\n",
    "    \n",
    "    參數:\n",
    "        model: BERTopic 模型\n",
    "        topics_list: 目前文檔的主題分配列表\n",
    "        merge_pairs: 要合併的主題對 [[來源, 目標], ...]\n",
    "    \n",
    "    回傳:\n",
    "        更新後的主題列表\n",
    "    \"\"\"\n",
    "    if not merge_pairs:\n",
    "        return topics_list\n",
    "    \n",
    "    topics_array = np.array(topics_list)\n",
    "    merge_count = 0\n",
    "    \n",
    "    for pair in merge_pairs:\n",
    "        if len(pair) != 2:\n",
    "            continue\n",
    "        source, target = int(pair[0]), int(pair[1])\n",
    "        \n",
    "        # 將來源主題的所有文檔重新分配到目標主題\n",
    "        mask = topics_array == source\n",
    "        if mask.sum() > 0:\n",
    "            topics_array[mask] = target\n",
    "            merge_count += 1\n",
    "            print(f\"    ✓ 合併 Topic {source} → Topic {target} ({mask.sum()} 文檔)\")\n",
    "    \n",
    "    print(f\"  - 完成 {merge_count} 個主題合併\")\n",
    "    return topics_array.tolist()\n",
    "\n",
    "\n",
    "def split_topic(model, df, embeddings, topic_id: int, topic_col: str = 'topic') -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"\n",
    "    拆分指定主題為多個子主題\n",
    "    \n",
    "    參數:\n",
    "        model: BERTopic 模型\n",
    "        df: 文檔 DataFrame\n",
    "        embeddings: 文檔嵌入向量\n",
    "        topic_id: 要拆分的主題 ID\n",
    "        topic_col: 主題欄位名稱\n",
    "    \n",
    "    回傳:\n",
    "        更新後的 DataFrame 和主題列表\n",
    "    \"\"\"\n",
    "    # 取得該主題的所有文檔\n",
    "    topic_mask = df[topic_col] == topic_id\n",
    "    topic_indices = df[topic_mask].index.tolist()\n",
    "    \n",
    "    if len(topic_indices) < 10:  # 太小的主題不拆分\n",
    "        print(f\"    ⚠ Topic {topic_id} 文檔數太少 ({len(topic_indices)})，跳過拆分\")\n",
    "        return df, df[topic_col].values\n",
    "    \n",
    "    # 提取該主題的嵌入向量\n",
    "    topic_embeddings = embeddings[topic_indices]\n",
    "    \n",
    "    # 對該主題進行子聚類（使用更小的 min_cluster_size）\n",
    "    sub_min_cluster = max(5, len(topic_indices) // 4)\n",
    "    \n",
    "    sub_hdbscan = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=sub_min_cluster,\n",
    "        min_samples=5,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom',\n",
    "        prediction_data=True\n",
    "    )\n",
    "    \n",
    "    sub_labels = sub_hdbscan.fit_predict(topic_embeddings)\n",
    "    \n",
    "    # 找到最大的主題 ID，用於生成新 ID\n",
    "    max_topic_id = int(df[topic_col].max())\n",
    "    unique_sub_labels = set(sub_labels[sub_labels != -1])\n",
    "    \n",
    "    if len(unique_sub_labels) <= 1:\n",
    "        print(f\"    ⚠ Topic {topic_id} 無法進一步拆分\")\n",
    "        return df, df[topic_col].values\n",
    "    \n",
    "    # 建立新的主題 ID 映射\n",
    "    new_topic_map = {}\n",
    "    for i, sub_label in enumerate(sorted(unique_sub_labels)):\n",
    "        if i == 0:\n",
    "            # 第一個子主題保持原 ID\n",
    "            new_topic_map[sub_label] = topic_id\n",
    "        else:\n",
    "            # 其他子主題使用新 ID\n",
    "            max_topic_id += 1\n",
    "            new_topic_map[sub_label] = max_topic_id\n",
    "    \n",
    "    # 更新主題分配\n",
    "    topics_array = df[topic_col].values.copy()\n",
    "    for idx, sub_label in zip(topic_indices, sub_labels):\n",
    "        if sub_label in new_topic_map:\n",
    "            topics_array[idx] = new_topic_map[sub_label]\n",
    "    \n",
    "    df[topic_col] = topics_array\n",
    "    \n",
    "    print(f\"    ✓ 拆分 Topic {topic_id} → {len(unique_sub_labels)} 個子主題\")\n",
    "    for sub_label, new_id in new_topic_map.items():\n",
    "        count = (sub_labels == sub_label).sum()\n",
    "        print(f\"      - Topic {new_id}: {count} 文檔\")\n",
    "    \n",
    "    return df, topics_array\n",
    "\n",
    "\n",
    "def update_stopwords_and_representation(model, texts: List[str], topics: List[int], \n",
    "                                       embeddings, new_stopwords: List[str] = None):\n",
    "    \"\"\"\n",
    "    更新停用詞並重新計算主題表示\n",
    "    \n",
    "    參數:\n",
    "        model: BERTopic 模型\n",
    "        texts: 文檔文本列表\n",
    "        topics: 主題分配列表\n",
    "        embeddings: 文檔嵌入向量\n",
    "        new_stopwords: 新增的停用詞列表\n",
    "    \n",
    "    回傳:\n",
    "        更新後的模型\n",
    "    \"\"\"\n",
    "    if not new_stopwords:\n",
    "        return model\n",
    "    \n",
    "    print(f\"  - 新增 {len(new_stopwords)} 個停用詞: {new_stopwords}\")\n",
    "    \n",
    "    # BERTopic 使用 CountVectorizer，需要更新其停用詞\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    # 建立新的 vectorizer 並更新停用詞\n",
    "    current_stopwords = set()\n",
    "    if hasattr(model, 'vectorizer_model') and model.vectorizer_model is not None:\n",
    "        if hasattr(model.vectorizer_model, 'stop_words_'):\n",
    "            current_stopwords = set(model.vectorizer_model.stop_words_)\n",
    "    \n",
    "    # 加入新停用詞\n",
    "    updated_stopwords = current_stopwords.union(set(new_stopwords))\n",
    "    \n",
    "    # 建立新的 vectorizer\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        stop_words=list(updated_stopwords),\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=5\n",
    "    )\n",
    "    \n",
    "    # 更新模型的 vectorizer\n",
    "    model.vectorizer_model = vectorizer_model\n",
    "    \n",
    "    # 重新計算主題表示\n",
    "    try:\n",
    "        model.update_topics(texts, topics=topics, vectorizer_model=vectorizer_model)\n",
    "        print(f\"  - 已更新主題表示（移除停用詞）\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ 更新主題表示時出錯: {e}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"✓ Phase 3 優化操作函數就緒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "o4f0om3nlth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LLM Agents 就緒\n",
      "  - OrchestratorAgent: 決策宏觀/微觀調整\n",
      "  - ParameterTuningAgent: 全域參數優化\n",
      "  - TopicRefinementAgent: 主題精煉操作\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Phase 3: LLM Agent 實作（雙層迭代架構）\n",
    "# ========================================\n",
    "\n",
    "class OrchestratorAgent:\n",
    "    \"\"\"\n",
    "    決策編排 Agent：根據全域指標決定執行宏觀調整或微觀調整\n",
    "    \n",
    "    決策規則：\n",
    "    - 如果離群率 > 15% 或 Silhouette < 0.05 → MACRO（優先解決全域聚類問題）\n",
    "    - 否則 → MICRO（全域穩定，進行微觀精細調整）\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client, model=LLM_MODEL, temperature=LLM_TEMPERATURE):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def decide_next_step(self, global_metrics: Dict, history: List[Dict]) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        決定下一步執行宏觀或微觀調整（使用 LLM 決策）\n",
    "\n",
    "        參數:\n",
    "            global_metrics: 目前全域指標 {outlier_rate, silhouette, separation, cohesion}\n",
    "            history: 歷史記錄列表\n",
    "\n",
    "        回傳:\n",
    "            (decision, reason): ('MACRO'/'MICRO', '原因說明')\n",
    "\n",
    "        注意: 使用 LLM 進行智能決策，若 LLM 失敗則回退到規則判斷\n",
    "        \"\"\"\n",
    "        outlier_rate = global_metrics.get('outlier_rate', 0)\n",
    "        silhouette = global_metrics.get('silhouette', 0)\n",
    "        separation = global_metrics.get('separation', 0)\n",
    "        cohesion = global_metrics.get('cohesion', 0)\n",
    "\n",
    "        # 建構歷史摘要（最近 5 輪）\n",
    "        history_summary = []\n",
    "        for h in history[-5:]:\n",
    "            history_summary.append({\n",
    "                'iteration': h.get('iteration'),\n",
    "                'type': h.get('iteration_type'),\n",
    "                'outlier_rate': h.get('metrics_after', {}).get('outlier_rate'),\n",
    "                'silhouette': h.get('metrics_after', {}).get('silhouette'),\n",
    "                'actions': h.get('actions_taken', [])\n",
    "            })\n",
    "\n",
    "        # 構建 LLM prompt\n",
    "        prompt = f\"\"\"You are an expert orchestrator for topic modeling optimization.\n",
    "\n",
    "**Current Global Metrics:**\n",
    "- Outlier rate: {outlier_rate:.2%}\n",
    "- Silhouette score: {silhouette:.4f}\n",
    "- Separation (topic distinctiveness): {separation:.4f}\n",
    "- Cohesion (internal topic similarity): {cohesion:.4f}\n",
    "\n",
    "**Recent History (last {len(history_summary)} iterations):**\n",
    "{json.dumps(history_summary, indent=2) if history_summary else 'No previous iterations'}\n",
    "\n",
    "**Decision Criteria:**\n",
    "\n",
    "Choose **MACRO** (global parameter tuning) when:\n",
    "- Outlier rate is high (> 20%) - indicates clustering is missing many documents\n",
    "- Silhouette score is very low (< 0.05) - indicates poor cluster quality\n",
    "- Recent MICRO adjustments show diminishing returns\n",
    "- Global structure needs fundamental improvement\n",
    "\n",
    "Choose **MICRO** (topic-level refinement) when:\n",
    "- Global metrics are acceptable (outlier rate < 20%, silhouette > 0.05)\n",
    "- Need fine-grained topic merging, splitting, or renaming\n",
    "- Want to improve topic interpretability without changing global structure\n",
    "- Recent MACRO changes have stabilized the clustering\n",
    "\n",
    "**Guidelines:**\n",
    "- MACRO adjustments are more disruptive but fix fundamental issues\n",
    "- MICRO adjustments are safer for incremental improvements\n",
    "- Consider the trend: are metrics improving or degrading?\n",
    "- Balance exploration (trying new approaches) with exploitation (refining what works)\n",
    "\n",
    "**Output Format (JSON only, no explanation):**\n",
    "{{\n",
    "    \"decision\": \"MACRO\" or \"MICRO\",\n",
    "    \"reason\": \"<brief explanation in Traditional Chinese>\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            # 使用 LLM 進行決策\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                temperature=self.temperature,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an optimization orchestrator. Output JSON only.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            raw = response.choices[0].message.content\n",
    "            # 嘗試解析 JSON\n",
    "            try:\n",
    "                result = json.loads(raw)\n",
    "            except:\n",
    "                # 使用正則提取 JSON\n",
    "                match = re.search(r'\\{[\\s\\S]*\\}', raw)\n",
    "                result = json.loads(match.group(0)) if match else {}\n",
    "\n",
    "            decision = result.get('decision', '').upper()\n",
    "            reason = result.get('reason', '')\n",
    "\n",
    "            # 驗證決策結果\n",
    "            if decision in ['MACRO', 'MICRO']:\n",
    "                return decision, reason\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid decision: {decision}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # LLM 失敗時回退到簡單規則\n",
    "            print(f\"LLM 決策失敗，回退到規則判斷: {e}\")\n",
    "            if outlier_rate > 0.2:\n",
    "                return 'MACRO', f'離群率過高 ({outlier_rate:.2%})，需要宏觀參數調整'\n",
    "            elif silhouette < 0.05:\n",
    "                return 'MACRO', f'Silhouette 分數過低 ({silhouette:.4f})，需要宏觀參數調整'\n",
    "            else:\n",
    "                return 'MICRO', f'全域指標穩定（離群率={outlier_rate:.2%}, Silhouette={silhouette:.4f}），進行微觀優化'\n",
    "\n",
    "\n",
    "class ParameterTuningAgent:\n",
    "    \"\"\"\n",
    "    參數調整 Agent：專門處理全域聚類參數優化（UMAP/HDBSCAN）\n",
    "    \n",
    "    目標：\n",
    "    - 降低離群點比例\n",
    "    - 提升 Silhouette 分數\n",
    "    - 保持主題的內聚性和區分度\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client, model=LLM_MODEL, temperature=LLM_TEMPERATURE):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def suggest_parameters(self, global_metrics: Dict, history: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        基於全域指標和歷史，建議新的聚類參數\n",
    "        \n",
    "        參數:\n",
    "            global_metrics: 目前全域指標\n",
    "            history: 歷史優化記錄\n",
    "        \n",
    "        回傳:\n",
    "            {'hdbscan_params': {'min_cluster_size': 25, 'min_samples': 8}, ...}\n",
    "        \"\"\"\n",
    "        # 建構歷史脈絡\n",
    "        history_summary = []\n",
    "        for h in history[-3:]:  # 只看最近 3 輪\n",
    "            if h.get('iteration_type') == 'MACRO':\n",
    "                history_summary.append({\n",
    "                    'iteration': h['iteration'],\n",
    "                    'params': h.get('params_used', {}),\n",
    "                    'outlier_rate': h.get('metrics_after', {}).get('outlier_rate'),\n",
    "                    'silhouette': h.get('metrics_after', {}).get('silhouette')\n",
    "                })\n",
    "        \n",
    "        prompt = f\"\"\"You are an expert in clustering optimization for topic modeling.\n",
    "\n",
    "**Current Global Metrics:**\n",
    "- Outlier rate: {global_metrics.get('outlier_rate', 0):.2%}\n",
    "- Silhouette score: {global_metrics.get('silhouette', 0):.4f}\n",
    "- Separation (topic distinctiveness): {global_metrics.get('separation', 0):.4f}\n",
    "- Cohesion (internal similarity): {global_metrics.get('cohesion', 0):.4f}\n",
    "\n",
    "**Optimization History (last {len(history_summary)} macro iterations):**\n",
    "{json.dumps(history_summary, indent=2) if history_summary else 'No previous macro iterations'}\n",
    "\n",
    "**Current Parameters:**\n",
    "- min_cluster_size: {HDBSCAN_MIN_CLUSTER_SIZE}\n",
    "- min_samples: {HDBSCAN_MIN_SAMPLES}\n",
    "- n_neighbors (UMAP): {UMAP_N_NEIGHBORS}\n",
    "- n_components (UMAP): {UMAP_N_COMPONENTS}\n",
    "\n",
    "**Objective:**\n",
    "Suggest SMALL, incremental parameter changes to:\n",
    "1. Reduce outlier rate (ideally < 10%)\n",
    "2. Improve Silhouette score (higher is better)\n",
    "3. Maintain good separation and cohesion\n",
    "\n",
    "**Guidelines:**\n",
    "- Decreasing min_cluster_size captures smaller topics but may increase noise\n",
    "- Increasing min_samples makes clusters more conservative\n",
    "- Adjust n_neighbors for UMAP to control local vs global structure\n",
    "\n",
    "**Output Format (JSON only, no explanation):**\n",
    "{{\n",
    "    \"hdbscan_params\": {{\n",
    "        \"min_cluster_size\": <int>,\n",
    "        \"min_samples\": <int>\n",
    "    }},\n",
    "    \"umap_params\": {{\n",
    "        \"n_neighbors\": <int>,\n",
    "        \"n_components\": <int>\n",
    "    }},\n",
    "    \"reasoning\": \"<brief explanation>\"\n",
    "}}\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                temperature=self.temperature,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a clustering optimization expert. Output JSON only.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            raw = response.choices[0].message.content\n",
    "            # 嘗試解析 JSON\n",
    "            try:\n",
    "                result = json.loads(raw)\n",
    "            except:\n",
    "                match = re.search(r'\\{[\\s\\S]*\\}', raw)\n",
    "                result = json.loads(match.group(0)) if match else {}\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ ParameterTuningAgent 失敗: {e}\")\n",
    "            return {}\n",
    "\n",
    "\n",
    "class TopicRefinementAgent:\n",
    "    \"\"\"\n",
    "    主題精煉 Agent：專門處理主題層級的微觀操作\n",
    "    \n",
    "    操作類型：\n",
    "    - merge_pairs: 合併語義相似或有父子關係的主題\n",
    "    - split_topics: 拆分過於寬泛的主題\n",
    "    - new_stopwords: 加入停用詞\n",
    "    - rename: 重新命名主題為有意義的標籤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client, model=LLM_MODEL, temperature=LLM_TEMPERATURE):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def suggest_refinements(self, topic_info: Dict, local_metrics: Dict, \n",
    "                           research_goal: str = \"分析企業ESG報告中的數位韌性\") -> Dict:\n",
    "        \"\"\"\n",
    "        基於主題列表和區域指標，建議微觀優化操作\n",
    "        \n",
    "        參數:\n",
    "            topic_info: {topic_id: {\"words\": \"...\", \"examples\": [...]}}\n",
    "            local_metrics: {topic_id: {\"cohesion\": 0.8, \"doc_count\": 50}}\n",
    "            research_goal: 研究目標描述\n",
    "        \n",
    "        回傳:\n",
    "            {\n",
    "                \"merge_pairs\": [[1, 2], [5, 6]],\n",
    "                \"split_topics\": [10, 15],\n",
    "                \"new_stopwords\": [\"company\", \"fiscal\"],\n",
    "                \"rename\": {0: \"Corporate Tax Strategy\"}\n",
    "            }\n",
    "        \"\"\"\n",
    "        \n",
    "        # 準備主題資訊（包含品質指標）\n",
    "        enriched_topics = []\n",
    "        for tid, info in topic_info.items():\n",
    "            metrics = local_metrics.get(tid, {})\n",
    "            enriched_topics.append({\n",
    "                'topic_id': tid,\n",
    "                'keywords': info.get('words', ''),\n",
    "                'doc_count': metrics.get('doc_count', 0),\n",
    "                'cohesion': metrics.get('cohesion', 0),\n",
    "                'examples': info.get('examples', [])[:2]  # 只取 2 個範例\n",
    "            })\n",
    "        \n",
    "        prompt = f\"\"\"You are an expert academic researcher specializing in ESG and digital resilience analysis.\n",
    "\n",
    "**Research Goal:**\n",
    "{research_goal}\n",
    "\n",
    "**Current Topics (with quality metrics):**\n",
    "{json.dumps(enriched_topics[:30], indent=2, ensure_ascii=False)}  \n",
    "(Showing first 30 topics only)\n",
    "\n",
    "**Your Task:**\n",
    "Analyze these topics and suggest refinement operations to create a coherent set of high-level ESG themes suitable for quantitative digital resilience analysis.\n",
    "\n",
    "**Guiding Principles:**\n",
    "1. **Merge similar topics**: Identify topics that are semantically synonymous or have parent-child relationships\n",
    "2. **Split mixed topics**: Flag topics with low cohesion (<0.6) that contain multiple distinct concepts\n",
    "3. **Add stopwords**: Identify common noise words across topics (e.g., \"company\", \"billion\", \"report\")\n",
    "4. **Rename topics**: Provide meaningful theme names (not just keywords)\n",
    "5. **Domain focus**: Keep ESG and digital resilience related topics; avoid over-merging distinct risk domains\n",
    "\n",
    "**Quality Metrics Explained:**\n",
    "- cohesion: Higher is better (0-1 range), measures internal topic consistency\n",
    "- doc_count: Number of documents in this topic\n",
    "\n",
    "**Output Format (JSON only, no explanation):**\n",
    "{{\n",
    "    \"merge_pairs\": [\n",
    "        // List of [source_topic_id, target_topic_id] pairs to merge\n",
    "        // Example: [[6, 24], [40, 64]]\n",
    "    ],\n",
    "    \"split_topics\": [\n",
    "        // List of topic IDs that are too broad (typically low cohesion)\n",
    "        // Example: [15, 22]\n",
    "    ],\n",
    "    \"new_stopwords\": [\n",
    "        // List of domain-specific stopwords to filter\n",
    "        // Example: [\"company\", \"statement\", \"billion\"]\n",
    "    ],\n",
    "    \"rename\": {{\n",
    "        // Map of topic_id to new descriptive name\n",
    "        // Example: {{\"0\": \"Corporate Tax Strategy & Deferred Assets\"}}\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                temperature=self.temperature,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a topic modeling expert. Output JSON only.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            raw = response.choices[0].message.content\n",
    "            try:\n",
    "                result = json.loads(raw)\n",
    "            except:\n",
    "                match = re.search(r'\\{[\\s\\S]*\\}', raw)\n",
    "                result = json.loads(match.group(0)) if match else {}\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ TopicRefinementAgent 失敗: {e}\")\n",
    "            return {}\n",
    "\n",
    "\n",
    "print(\"✓ LLM Agents 就緒\")\n",
    "print(\"  - OrchestratorAgent: 決策宏觀/微觀調整\")\n",
    "print(\"  - ParameterTuningAgent: 全域參數優化\")\n",
    "print(\"  - TopicRefinementAgent: 主題精煉操作\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3yhcfwnxqnl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 雙層迭代函數就緒\n",
      "  - run_macro_tuning_iteration: 全域參數調整 + 重新訓練\n",
      "  - run_micro_tuning_iteration: 主題層級精煉（合併/拆分/停用詞/重新命名）\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 雙層迭代函數實作\n",
    "# ========================================\n",
    "\n",
    "def run_macro_tuning_iteration(\n",
    "    agent: ParameterTuningAgent,\n",
    "    current_model,\n",
    "    texts: List[str],\n",
    "    embeddings: np.ndarray,\n",
    "    global_metrics: Dict,\n",
    "    history: List[Dict]\n",
    ") -> Tuple:\n",
    "    \"\"\"\n",
    "    執行宏觀參數調整迭代\n",
    "    \n",
    "    核心操作：\n",
    "    1. 請求 LLM 建議新的 UMAP/HDBSCAN 參數\n",
    "    2. 使用新參數重新實例化 BERTopic 模型\n",
    "    3. 重新執行 fit_transform（全域重新訓練）\n",
    "    4. 不包含任何主題合併、拆分等微觀操作\n",
    "    \n",
    "    參數:\n",
    "        agent: ParameterTuningAgent 實例\n",
    "        current_model: 目前 BERTopic 模型\n",
    "        texts: 文檔文本列表\n",
    "        embeddings: 文檔嵌入向量\n",
    "        global_metrics: 目前全域指標\n",
    "        history: 優化歷史\n",
    "    \n",
    "    回傳:\n",
    "        (new_model, new_topics, params_used): 新模型、新主題分配、使用的參數\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"執行宏觀參數調整 (MACRO TUNING)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. 請求參數建議\n",
    "    print(\"⚙ 請求 ParameterTuningAgent 建議...\")\n",
    "    param_suggestion = agent.suggest_parameters(global_metrics, history)\n",
    "    \n",
    "    if not param_suggestion:\n",
    "        print(\"  ⚠ 未取得參數建議，保持目前參數\")\n",
    "        return current_model, None, {}\n",
    "    \n",
    "    # 2. 提取參數\n",
    "    hdbscan_params = param_suggestion.get('hdbscan_params', {})\n",
    "    umap_params = param_suggestion.get('umap_params', {})\n",
    "    reasoning = param_suggestion.get('reasoning', 'N/A')\n",
    "    \n",
    "    min_cluster_size = hdbscan_params.get('min_cluster_size', HDBSCAN_MIN_CLUSTER_SIZE)\n",
    "    min_samples = hdbscan_params.get('min_samples', HDBSCAN_MIN_SAMPLES)\n",
    "    n_neighbors = umap_params.get('n_neighbors', UMAP_N_NEIGHBORS)\n",
    "    n_components = umap_params.get('n_components', UMAP_N_COMPONENTS)\n",
    "    \n",
    "    print(f\"\\n  建議參數:\")\n",
    "    print(f\"    - HDBSCAN: min_cluster_size={min_cluster_size}, min_samples={min_samples}\")\n",
    "    print(f\"    - UMAP: n_neighbors={n_neighbors}, n_components={n_components}\")\n",
    "    print(f\"    - 原因: {reasoning}\")\n",
    "    \n",
    "    # 3. 建立新模型\n",
    "    print(\"\\n⚙ 使用新參數重新訓練模型...\")\n",
    "    \n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=int(n_neighbors),\n",
    "        n_components=int(n_components),\n",
    "        min_dist=UMAP_MIN_DIST,\n",
    "        metric=UMAP_METRIC,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    hdbscan_model = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=int(min_cluster_size),\n",
    "        min_samples=int(min_samples),\n",
    "        metric=HDBSCAN_METRIC,\n",
    "        cluster_selection_method=HDBSCAN_SELECTION_METHOD,\n",
    "        prediction_data=True\n",
    "    )\n",
    "    \n",
    "    new_model = BERTopic(\n",
    "        calculate_probabilities=True,\n",
    "        verbose=False,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model\n",
    "    )\n",
    "    \n",
    "    # 4. 重新訓練\n",
    "    new_topics, _ = new_model.fit_transform(texts, embeddings=embeddings)\n",
    "    \n",
    "    print(f\"  ✓ 重新訓練完成\")\n",
    "    print(f\"    - 新主題數: {len(set([t for t in new_topics if t != -1]))}\")\n",
    "    print(f\"    - 新離群率: {(np.array(new_topics) == -1).mean():.2%}\")\n",
    "    \n",
    "    params_used = {\n",
    "        'hdbscan_params': {'min_cluster_size': int(min_cluster_size), 'min_samples': int(min_samples)},\n",
    "        'umap_params': {'n_neighbors': int(n_neighbors), 'n_components': int(n_components)}\n",
    "    }\n",
    "    \n",
    "    return new_model, new_topics, params_used\n",
    "\n",
    "\n",
    "def run_micro_tuning_iteration(\n",
    "    agent: TopicRefinementAgent,\n",
    "    current_model,\n",
    "    df: pd.DataFrame,\n",
    "    texts: List[str],\n",
    "    embeddings: np.ndarray,\n",
    "    current_topics: List[int],\n",
    "    topic_col: str,\n",
    "    local_metrics: Dict\n",
    ") -> Tuple:\n",
    "    \"\"\"\n",
    "    執行微觀主題精煉迭代\n",
    "    \n",
    "    核心操作：\n",
    "    1. 請求 LLM 建議主題合併、拆分、停用詞、重新命名\n",
    "    2. 依序執行這些微觀操作\n",
    "    3. 絕對不能呼叫 fit_transform（不重新訓練模型）\n",
    "    4. 使用 update_topics 更新主題表示\n",
    "    \n",
    "    參數:\n",
    "        agent: TopicRefinementAgent 實例\n",
    "        current_model: 目前 BERTopic 模型\n",
    "        df: 文檔 DataFrame\n",
    "        texts: 文檔文本列表\n",
    "        embeddings: 文檔嵌入向量\n",
    "        current_topics: 目前主題分配列表\n",
    "        topic_col: 主題欄位名稱\n",
    "        local_metrics: 區域指標 {topic_id: {cohesion, doc_count}}\n",
    "    \n",
    "    回傳:\n",
    "        (updated_model, updated_topics, df, actions_taken): 更新後的模型、主題、DataFrame、已執行操作\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"執行微觀主題精煉 (MICRO TUNING)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. 準備主題資訊\n",
    "    print(\"⚙ 準備主題資訊...\")\n",
    "    topic_info = {}\n",
    "    for tid in set([t for t in current_topics if t != -1]):\n",
    "        try:\n",
    "            words = ', '.join([w for w, _ in current_model.get_topic(tid)[:10]])\n",
    "            examples = df[df[topic_col] == tid]['text'].head(3).tolist()\n",
    "            topic_info[tid] = {\"words\": words, \"examples\": examples}\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # 2. 請求精煉建議\n",
    "    print(\"⚙ 請求 TopicRefinementAgent 建議...\")\n",
    "    refinement_plan = agent.suggest_refinements(topic_info, local_metrics)\n",
    "    \n",
    "    if not refinement_plan:\n",
    "        print(\"  ⚠ 未取得精煉建議\")\n",
    "        return current_model, current_topics, df, {}\n",
    "    \n",
    "    print(f\"\\n  精煉計劃:\")\n",
    "    print(f\"    - 合併: {len(refinement_plan.get('merge_pairs', []))} 對\")\n",
    "    print(f\"    - 拆分: {len(refinement_plan.get('split_topics', []))} 個主題\")\n",
    "    print(f\"    - 停用詞: {len(refinement_plan.get('new_stopwords', []))} 個\")\n",
    "    print(f\"    - 重新命名: {len(refinement_plan.get('rename', {}))} 個主題\")\n",
    "    \n",
    "    actions_taken = {\n",
    "        'merge': 0,\n",
    "        'split': 0,\n",
    "        'stopwords': 0,\n",
    "        'rename': 0\n",
    "    }\n",
    "    \n",
    "    # 3. 執行合併\n",
    "    merge_pairs = refinement_plan.get('merge_pairs', [])\n",
    "    if merge_pairs:\n",
    "        print(\"\\n⚙ 執行主題合併...\")\n",
    "        current_topics = merge_topics(current_model, current_topics, merge_pairs)\n",
    "        df[topic_col] = current_topics\n",
    "        actions_taken['merge'] = len(merge_pairs)\n",
    "    \n",
    "    # 4. 執行拆分\n",
    "    split_topics_list = refinement_plan.get('split_topics', [])\n",
    "    if split_topics_list:\n",
    "        print(\"\\n⚙ 執行主題拆分...\")\n",
    "        for tid in split_topics_list:\n",
    "            df, current_topics = split_topic(current_model, df, embeddings, int(tid), topic_col)\n",
    "            actions_taken['split'] += 1\n",
    "    \n",
    "    # 5. 更新停用詞\n",
    "    new_stopwords = refinement_plan.get('new_stopwords', [])\n",
    "    if new_stopwords:\n",
    "        print(\"\\n⚙ 更新停用詞...\")\n",
    "        current_model = update_stopwords_and_representation(\n",
    "            current_model, texts, current_topics, embeddings, new_stopwords\n",
    "        )\n",
    "        actions_taken['stopwords'] = len(new_stopwords)\n",
    "    \n",
    "    # 6. 重新命名主題\n",
    "    rename_map = refinement_plan.get('rename', {})\n",
    "    if rename_map and isinstance(rename_map, dict):\n",
    "        print(\"\\n⚙ 重新命名主題...\")\n",
    "        for tid, name in rename_map.items():\n",
    "            try:\n",
    "                current_model.set_topic_labels({int(tid): name})\n",
    "                actions_taken['rename'] += 1\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        print(f\"  ✓ 完成 {actions_taken['rename']} 個主題重新命名\")\n",
    "    \n",
    "    return current_model, current_topics, df, actions_taken\n",
    "\n",
    "\n",
    "print(\"✓ 雙層迭代函數就緒\")\n",
    "print(\"  - run_macro_tuning_iteration: 全域參數調整 + 重新訓練\")\n",
    "print(\"  - run_micro_tuning_iteration: 主題層級精煉（合併/拆分/停用詞/重新命名）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "phase3_optimize",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 01:24:56,303 - BERTopic - WARNING: You are loading a BERTopic model without explicitly defining an embedding model. If you want to also load in an embedding model, make sure to use `BERTopic.load(my_model, embedding_model=my_embedding_model)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙ 啟動雙層迭代優化系統（最多 10 輪）...\n",
      "\n",
      "======================================================================\n",
      "雙層迭代架構說明\n",
      "======================================================================\n",
      "MACRO（宏觀調整）：優化全域聚類參數（UMAP/HDBSCAN）→ 重新訓練模型\n",
      "MICRO（微觀調整）：主題層級精煉（合併/拆分/停用詞/重新命名）→ 不重新訓練\n",
      "決策邏輯：離群率>15% 或 Silhouette<0.05 → MACRO，否則 → MICRO\n",
      "======================================================================\n",
      "\n",
      "初始指標:\n",
      "  一致性: 0.7721\n",
      "  區分度: 0.4064\n",
      "  Silhouette: 0.0771\n",
      "  離群率: 16.52%\n",
      "\n",
      "\n",
      "======================================================================\n",
      "迭代 1/10\n",
      "======================================================================\n",
      "\n",
      "決策: MICRO\n",
      "原因: 目前全局指標在可接受範圍：外異常率16.52%低於20%、輪廓係數0.0771高於0.05，需在不改變全局結構的前提下進行主題層面的微調（合併、拆分、重新命名等）以提升解釋性與穩定性；並且最近尚未出現明顯的 MACRO 改進跡象，因此優先採取 MICRO。\n",
      "\n",
      "\n",
      "============================================================\n",
      "執行微觀主題精煉 (MICRO TUNING)\n",
      "============================================================\n",
      "⚙ 準備主題資訊...\n",
      "⚙ 請求 TopicRefinementAgent 建議...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 01:26:00,019 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  精煉計劃:\n",
      "    - 合併: 22 對\n",
      "    - 拆分: 0 個主題\n",
      "    - 停用詞: 14 個\n",
      "    - 重新命名: 30 個主題\n",
      "\n",
      "⚙ 執行主題合併...\n",
      "    ✓ 合併 Topic 0 → Topic 25 (302 文檔)\n",
      "    ✓ 合併 Topic 6 → Topic 25 (123 文檔)\n",
      "    ✓ 合併 Topic 8 → Topic 25 (113 文檔)\n",
      "    ✓ 合併 Topic 11 → Topic 25 (106 文檔)\n",
      "    ✓ 合併 Topic 14 → Topic 10 (103 文檔)\n",
      "    ✓ 合併 Topic 24 → Topic 10 (84 文檔)\n",
      "    ✓ 合併 Topic 7 → Topic 10 (117 文檔)\n",
      "    ✓ 合併 Topic 5 → Topic 10 (125 文檔)\n",
      "    ✓ 合併 Topic 29 → Topic 10 (76 文檔)\n",
      "    ✓ 合併 Topic 18 → Topic 10 (94 文檔)\n",
      "    ✓ 合併 Topic 22 → Topic 10 (88 文檔)\n",
      "    ✓ 合併 Topic 23 → Topic 10 (84 文檔)\n",
      "    ✓ 合併 Topic 27 → Topic 13 (79 文檔)\n",
      "    ✓ 合併 Topic 16 → Topic 21 (97 文檔)\n",
      "    ✓ 合併 Topic 3 → Topic 21 (136 文檔)\n",
      "    ✓ 合併 Topic 15 → Topic 21 (98 文檔)\n",
      "    ✓ 合併 Topic 26 → Topic 21 (80 文檔)\n",
      "    ✓ 合併 Topic 17 → Topic 1 (95 文檔)\n",
      "    ✓ 合併 Topic 28 → Topic 1 (78 文檔)\n",
      "    ✓ 合併 Topic 20 → Topic 1 (93 文檔)\n",
      "    ✓ 合併 Topic 12 → Topic 4 (105 文檔)\n",
      "    ✓ 合併 Topic 9 → Topic 4 (109 文檔)\n",
      "  - 完成 22 個主題合併\n",
      "\n",
      "⚙ 更新停用詞...\n",
      "  - 新增 14 個停用詞: ['inc', 'incorporated', 'corp', 'corporation', 'ltd', 'llc', 'note', 'table', 'contents', 'report', 'reporting', 'fyi', 'billion', 'million']\n",
      "  - 已更新主題表示（移除停用詞）\n",
      "\n",
      "⚙ 重新命名主題...\n",
      "  ✓ 完成 30 個主題重新命名\n",
      "\n",
      "結果對比:\n",
      "  前: 一致性: 0.7721\n",
      "  前: 區分度: 0.4064\n",
      "  前: Silhouette: 0.0771\n",
      "  前: 離群率: 16.52%\n",
      "  後: 一致性: 0.7615\n",
      "  後: 區分度: 0.4053\n",
      "  後: Silhouette: 0.0011\n",
      "  後: 離群率: 16.52%\n",
      "\n",
      "✓ 迭代 1 完成\n",
      "\n",
      "\n",
      "======================================================================\n",
      "迭代 2/10\n",
      "======================================================================\n",
      "\n",
      "決策: MACRO\n",
      "原因: 根據現有指標，輪廓係數僅0.0011，遠低於0.05，顯示全球聚類品質極差；且最近的MICRO調整導致輪廓顯著下降，顯示微觀層面的調整無法改善整體結構。因此需要MACRO層級的全局參數重新調整以提升全球結構與主題分離。\n",
      "\n",
      "\n",
      "============================================================\n",
      "執行宏觀參數調整 (MACRO TUNING)\n",
      "============================================================\n",
      "⚙ 請求 ParameterTuningAgent 建議...\n",
      "\n",
      "  建議參數:\n",
      "    - HDBSCAN: min_cluster_size=25, min_samples=8\n",
      "    - UMAP: n_neighbors=20, n_components=10\n",
      "    - 原因: Lowering min_cluster_size and min_samples slightly enables more and denser clusters, which can reduce the number of points labeled as noise. Increasing n_neighbors to 20 emphasizes global structure in the embedding, often improving silhouette while aiming to preserve separation and cohesion. Keep n_components at 10 to maintain the current visualization dimensionality.\n",
      "\n",
      "⚙ 使用新參數重新訓練模型...\n",
      "  ✓ 重新訓練完成\n",
      "    - 新主題數: 72\n",
      "    - 新離群率: 17.30%\n",
      "\n",
      "結果對比:\n",
      "  前: 一致性: 0.7615\n",
      "  前: 區分度: 0.4053\n",
      "  前: Silhouette: 0.0011\n",
      "  前: 離群率: 16.52%\n",
      "  後: 一致性: 0.7771\n",
      "  後: 區分度: 0.4150\n",
      "  後: Silhouette: 0.0766\n",
      "  後: 離群率: 17.30%\n",
      "\n",
      "⚙ 評估是否繼續優化...\n",
      "\n",
      "  智能停止判斷: CONTINUE\n",
      "  理由: Convergence not observed: no two consecutive iterations with <2% change across all metrics; some metrics changed >2% (coh +2.05%, sep +2.39%, out +4.67%). However, MACRO after MICRO shows improvement (coh, sep, out higher than baseline), and the MACRO/MICRO alternating sequence appears promising. Proceed with another iteration.\n",
      "\n",
      "✓ 迭代 2 完成\n",
      "\n",
      "\n",
      "======================================================================\n",
      "迭代 3/10\n",
      "======================================================================\n",
      "\n",
      "決策: MICRO\n",
      "原因: 目前全域指標仍在可接受範圍：outlier_rate 為 17.30%（低於 20%）、silhouette 為 0.0766（高於 0.05），全球結構未顯著需要宏觀調整。最近的 MACRO 調整未顯著改善，且 MICRO 仍具微調潛力。建議在不改動整體結構的前提下，透過話題層級的合併/分裂/重新命名等微調，提升可解釋性與內部一致性，同時避免過度干預造成穩定性下降。\n",
      "\n",
      "\n",
      "============================================================\n",
      "執行微觀主題精煉 (MICRO TUNING)\n",
      "============================================================\n",
      "⚙ 準備主題資訊...\n",
      "⚙ 請求 TopicRefinementAgent 建議...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 01:27:45,331 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  精煉計劃:\n",
      "    - 合併: 13 對\n",
      "    - 拆分: 1 個主題\n",
      "    - 停用詞: 19 個\n",
      "    - 重新命名: 30 個主題\n",
      "\n",
      "⚙ 執行主題合併...\n",
      "    ✓ 合併 Topic 14 → Topic 11 (95 文檔)\n",
      "    ✓ 合併 Topic 6 → Topic 26 (113 文檔)\n",
      "    ✓ 合併 Topic 3 → Topic 26 (155 文檔)\n",
      "    ✓ 合併 Topic 24 → Topic 26 (73 文檔)\n",
      "    ✓ 合併 Topic 5 → Topic 25 (121 文檔)\n",
      "    ✓ 合併 Topic 15 → Topic 25 (94 文檔)\n",
      "    ✓ 合併 Topic 17 → Topic 22 (87 文檔)\n",
      "    ✓ 合併 Topic 29 → Topic 13 (68 文檔)\n",
      "    ✓ 合併 Topic 27 → Topic 12 (70 文檔)\n",
      "    ✓ 合併 Topic 16 → Topic 12 (94 文檔)\n",
      "    ✓ 合併 Topic 18 → Topic 10 (82 文檔)\n",
      "    ✓ 合併 Topic 20 → Topic 10 (76 文檔)\n",
      "    ✓ 合併 Topic 23 → Topic 7 (73 文檔)\n",
      "  - 完成 13 個主題合併\n",
      "\n",
      "⚙ 執行主題拆分...\n",
      "    ⚠ Topic 14 文檔數太少 (0)，跳過拆分\n",
      "\n",
      "⚙ 更新停用詞...\n",
      "  - 新增 19 個停用詞: ['company', 'corporation', 'inc', 'limited', 'note', 'notes', 'table', 'contents', 'proxy', 'statement', 'year', 'fiscal', 'percent', 'billion', 'million', 'u.s.', 'us', 'we', 'our']\n",
      "  - 已更新主題表示（移除停用詞）\n",
      "\n",
      "⚙ 重新命名主題...\n",
      "  ✓ 完成 30 個主題重新命名\n",
      "\n",
      "結果對比:\n",
      "  前: 一致性: 0.7771\n",
      "  前: 區分度: 0.4150\n",
      "  前: Silhouette: 0.0766\n",
      "  前: 離群率: 17.30%\n",
      "  後: 一致性: 0.7650\n",
      "  後: 區分度: 0.4048\n",
      "  後: Silhouette: 0.0372\n",
      "  後: 離群率: 17.30%\n",
      "\n",
      "⚙ 評估是否繼續優化...\n",
      "\n",
      "  智能停止判斷: CONTINUE\n",
      "  理由: Not converged (no 2-iteration stabilization). No consistent degradation. The MACRO step (iter1->iter2) improved all metrics: coh 0.7614619550497636->0.7770762542883555 (+0.0156142992385919), sep 0.4052773118019104->0.41497185826301575 (+0.00969454646110535), sil 0.0011196924606338143->0.07662258297204971 (+0.0755028905114159), out 0.16524947858174233->0.17295042515642547 (+0.00770094657468314). The following MICRO step (iter2->iter3) shows a mild decline (coh 0.7770762542883555->0.7649873727459019, sep 0.41497185826301575->0.4048008918762207, sil 0.07662258297204971->0.03722713142633438, out 0.17295042515642547->0.17295042515642547). However, the overall pattern with MACRO/MICRO alternation is promising and warrants continuation.\n",
      "\n",
      "✓ 迭代 3 完成\n",
      "\n",
      "\n",
      "======================================================================\n",
      "迭代 4/10\n",
      "======================================================================\n",
      "\n",
      "決策: MACRO\n",
      "原因: 最近的微觀調整未能穩定提升，且全局 silhouette 僅為 0.0372，顯示聚類品質極低，需透過全域參數重調來改善整體結構；此外，MICRO 的變動未見持續正向回報，證明現階段需進行更大規模的全局調整。\n",
      "\n",
      "\n",
      "============================================================\n",
      "執行宏觀參數調整 (MACRO TUNING)\n",
      "============================================================\n",
      "⚙ 請求 ParameterTuningAgent 建議...\n",
      "\n",
      "  建議參數:\n",
      "    - HDBSCAN: min_cluster_size=30, min_samples=11\n",
      "    - UMAP: n_neighbors=18, n_components=10\n",
      "    - 原因: Increase min_samples slightly to make clusters more conservative; elevate n_neighbors to 18 to emphasize global structure and stabilize cluster assignments. Keep min_cluster_size at 30 to avoid over-fragmentation. These small, incremental tweaks aim to reduce outliers (targeting <10%) and improve silhouette while preserving separation and cohesion.\n",
      "\n",
      "⚙ 使用新參數重新訓練模型...\n",
      "  ✓ 重新訓練完成\n",
      "    - 新主題數: 63\n",
      "    - 新離群率: 16.32%\n",
      "\n",
      "結果對比:\n",
      "  前: 一致性: 0.7650\n",
      "  前: 區分度: 0.4048\n",
      "  前: Silhouette: 0.0372\n",
      "  前: 離群率: 17.30%\n",
      "  後: 一致性: 0.7699\n",
      "  後: 區分度: 0.4022\n",
      "  後: Silhouette: 0.0790\n",
      "  後: 離群率: 16.32%\n",
      "\n",
      "⚙ 評估是否繼續優化...\n",
      "\n",
      "  智能停止判斷: CONTINUE\n",
      "  理由: No convergence yet; alternating MACRO/MICRO shows promise and the last MACRO step improves coherence, silhouette, and outlier rate, despite fluctuations.\n",
      "\n",
      "✓ 迭代 4 完成\n",
      "\n",
      "\n",
      "======================================================================\n",
      "迭代 5/10\n",
      "======================================================================\n",
      "\n",
      "決策: MACRO\n",
      "原因: 根據當前全域指標（離群率 16.32%、輪廓係數 0.0790）在可接受範圍內，但最近的 MICRO 調整出現遞減收益，且全域結構需要更根本的改進；前一輪的 MACRO 設定已開始改善聚類穩定性與分離度，為進一步提升整體模型品質，應再進行 MACRO 全域參數調整。\n",
      "\n",
      "\n",
      "============================================================\n",
      "執行宏觀參數調整 (MACRO TUNING)\n",
      "============================================================\n",
      "⚙ 請求 ParameterTuningAgent 建議...\n",
      "\n",
      "  建議參數:\n",
      "    - HDBSCAN: min_cluster_size=28, min_samples=12\n",
      "    - UMAP: n_neighbors=18, n_components=10\n",
      "    - 原因: Lowering min_cluster_size slightly allows detection of smaller topics while increasing min_samples to 12 keeps clusters more cohesive; raising n_neighbors to 18 emphasizes global structure, potentially reducing fragmentation and outliers and improving silhouette, while maintaining separation and cohesion.\n",
      "\n",
      "⚙ 使用新參數重新訓練模型...\n",
      "  ✓ 重新訓練完成\n",
      "    - 新主題數: 63\n",
      "    - 新離群率: 15.85%\n",
      "\n",
      "結果對比:\n",
      "  前: 一致性: 0.7699\n",
      "  前: 區分度: 0.4022\n",
      "  前: Silhouette: 0.0790\n",
      "  前: 離群率: 16.32%\n",
      "  後: 一致性: 0.7697\n",
      "  後: 區分度: 0.4041\n",
      "  後: Silhouette: 0.0785\n",
      "  後: 離群率: 15.85%\n",
      "\n",
      "⚙ 評估是否繼續優化...\n",
      "\n",
      "  智能停止判斷: CONTINUE\n",
      "  理由: Convergence not achieved: no 2-iteration <2% change across all metrics (iter4→iter5 shows ~2.85% drop in out). No consistent degradation; metrics fluctuate with small gains and MACRO/MICRO alternation remains promising, so continue optimization.\n",
      "\n",
      "✓ 迭代 5 完成\n",
      "\n",
      "\n",
      "======================================================================\n",
      "迭代 6/10\n",
      "======================================================================\n",
      "\n",
      "決策: MACRO\n",
      "原因: 最近的 MICRO 調整回報遞減，全球結構仍有改善空間且需提高聚類的分離與可解釋性；現有出現率低於 20% 且 silhouette > 0.05，但微觀修正未顯著提升效果，且過去幾個 MACRO 迭代雖穩定聚類，但尚未達到理想狀態，因此採用 MACRO 進行全局參數重新調整（例如調整 min_cluster_size、min_samples、n_neighbors、n_components 等），以期提升整體效果。\n",
      "\n",
      "\n",
      "============================================================\n",
      "執行宏觀參數調整 (MACRO TUNING)\n",
      "============================================================\n",
      "⚙ 請求 ParameterTuningAgent 建議...\n",
      "\n",
      "  建議參數:\n",
      "    - HDBSCAN: min_cluster_size=26, min_samples=12\n",
      "    - UMAP: n_neighbors=18, n_components=10\n",
      "    - 原因: Building on recent gains by lowering min_cluster_size from 28 to 26 while maintaining min_samples at 12 and keeping n_neighbors at 18. This targets smaller topics to reduce outliers while preserving robustness and local structure, with a low-risk, incremental change. Potential for slight silhouette stabilization or improvement; monitor for any noise increase.\n",
      "\n",
      "⚙ 使用新參數重新訓練模型...\n",
      "  ✓ 重新訓練完成\n",
      "    - 新主題數: 68\n",
      "    - 新離群率: 16.78%\n",
      "\n",
      "結果對比:\n",
      "  前: 一致性: 0.7697\n",
      "  前: 區分度: 0.4041\n",
      "  前: Silhouette: 0.0785\n",
      "  前: 離群率: 15.85%\n",
      "  後: 一致性: 0.7741\n",
      "  後: 區分度: 0.4145\n",
      "  後: Silhouette: 0.0805\n",
      "  後: 離群率: 16.78%\n",
      "\n",
      "⚙ 評估是否繼續優化...\n",
      "\n",
      "  智能停止判斷: CONTINUE\n",
      "  理由: Convergence not reached: no <2% change over two iterations. The latest steps show mixed but generally favorable movement: cohesion increased (0.7697→0.7741), separation increased (0.4041→0.4145), silhouette increased (0.0785→0.0805), while outlier rate fluctuated and temporarily worsened on the last step. No consistent degradation; overall trend suggests potential gains with continued MACRO/MICRO iterations.\n",
      "\n",
      "✓ 迭代 6 完成\n",
      "\n",
      "\n",
      "======================================================================\n",
      "迭代 7/10\n",
      "======================================================================\n",
      "\n",
      "決策: MICRO\n",
      "原因: 目前全局指標相對穩定且在可接受範圍內：離群率 16.78%、Silhouette 分數 0.0805，整體結構已成熟。雖最近的 MACRO 調整能改善結構，但微調（合併、分拆、重新命名等）更適合在不改變整體分群架構的前提下提升主題的可解釋性與細部區分，因此採用 MICRO 進行細粒度優化，風險較低且可控。\n",
      "\n",
      "\n",
      "============================================================\n",
      "執行微觀主題精煉 (MICRO TUNING)\n",
      "============================================================\n",
      "⚙ 準備主題資訊...\n",
      "⚙ 請求 TopicRefinementAgent 建議...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 01:31:45,755 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  精煉計劃:\n",
      "    - 合併: 9 對\n",
      "    - 拆分: 1 個主題\n",
      "    - 停用詞: 29 個\n",
      "    - 重新命名: 29 個主題\n",
      "\n",
      "⚙ 執行主題合併...\n",
      "    ✓ 合併 Topic 1 → Topic 29 (216 文檔)\n",
      "    ✓ 合併 Topic 3 → Topic 29 (188 文檔)\n",
      "    ✓ 合併 Topic 11 → Topic 29 (105 文檔)\n",
      "    ✓ 合併 Topic 12 → Topic 29 (104 文檔)\n",
      "    ✓ 合併 Topic 14 → Topic 29 (101 文檔)\n",
      "    ✓ 合併 Topic 26 → Topic 29 (75 文檔)\n",
      "    ✓ 合併 Topic 21 → Topic 29 (86 文檔)\n",
      "    ✓ 合併 Topic 6 → Topic 28 (112 文檔)\n",
      "    ✓ 合併 Topic 7 → Topic 25 (111 文檔)\n",
      "  - 完成 9 個主題合併\n",
      "\n",
      "⚙ 執行主題拆分...\n",
      "    ⚠ Topic 24 無法進一步拆分\n",
      "\n",
      "⚙ 更新停用詞...\n",
      "  - 新增 29 個停用詞: ['corporation', 'company', 'note', 'notes', 'table', 'contents', 'form', 'proxy', '10-k', 'annual', 'year', 'december', 'january', 'percent', '%', 'billion', 'million', 'management', \"management's\", 'table', 'contents', 'the', 'and', 'of', 'to', 'for', 'in', 'on', 'as']\n",
      "  - 已更新主題表示（移除停用詞）\n",
      "\n",
      "⚙ 重新命名主題...\n",
      "  ✓ 完成 29 個主題重新命名\n",
      "\n",
      "結果對比:\n",
      "  前: 一致性: 0.7741\n",
      "  前: 區分度: 0.4145\n",
      "  前: Silhouette: 0.0805\n",
      "  前: 離群率: 16.78%\n",
      "  後: 一致性: 0.7714\n",
      "  後: 區分度: 0.4150\n",
      "  後: Silhouette: 0.0510\n",
      "  後: 離群率: 16.78%\n",
      "\n",
      "⚙ 評估是否繼續優化...\n",
      "\n",
      "  智能停止判斷: CONTINUE\n",
      "  理由: Overall metrics improved from iter4 to iter6 (coh, sep, and out improved; silhouette rose then dipped slightly in the last step). Convergence is not reached and the MACRO/MICRO pattern remains promising, so proceed with targeted tuning to stabilize silhouette while maintaining gains.\n",
      "\n",
      "✓ 迭代 7 完成\n",
      "\n",
      "\n",
      "======================================================================\n",
      "迭代 8/10\n",
      "======================================================================\n",
      "\n",
      "決策: MICRO\n",
      "原因: 目前全局指標略穩定，outlier_rate 16.78% (<20%)，silhouette 0.0510 接近閾值 0.05，最近的 MACRO 調整雖提升了輪廓度但未顯著降低離群，顯示結構基本穩定但需在主題層面進行微調以提升可解釋性與細節分離，因此優先採用 MICRO 進行細粒度的合併/拆分/重命名等操作，以保持整體結構穩定並追求微小的改善。\n",
      "\n",
      "\n",
      "============================================================\n",
      "執行微觀主題精煉 (MICRO TUNING)\n",
      "============================================================\n",
      "⚙ 準備主題資訊...\n",
      "⚙ 請求 TopicRefinementAgent 建議...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type int64 is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 120\u001b[39m\n\u001b[32m    114\u001b[39m         cohesion_score = \u001b[38;5;28mfloat\u001b[39m(cosine_similarity(topic_embs, center.reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)).mean())\n\u001b[32m    115\u001b[39m         local_metrics[tid] = {\n\u001b[32m    116\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mcohesion\u001b[39m\u001b[33m'\u001b[39m: cohesion_score,\n\u001b[32m    117\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mdoc_count\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(idxs)\n\u001b[32m    118\u001b[39m         }\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m updated_model, updated_topics, df, actions_taken = \u001b[43mrun_micro_tuning_iteration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrefinement_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurrent_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurrent_topics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopic_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_metrics\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m current_model = updated_model\n\u001b[32m    132\u001b[39m current_topics = updated_topics\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 150\u001b[39m, in \u001b[36mrun_micro_tuning_iteration\u001b[39m\u001b[34m(agent, current_model, df, texts, embeddings, current_topics, topic_col, local_metrics)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# 2. 請求精煉建議\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m⚙ 請求 TopicRefinementAgent 建議...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m refinement_plan = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43msuggest_refinements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_metrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m refinement_plan:\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  ⚠ 未取得精煉建議\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 284\u001b[39m, in \u001b[36mTopicRefinementAgent.suggest_refinements\u001b[39m\u001b[34m(self, topic_info, local_metrics, research_goal)\u001b[39m\n\u001b[32m    269\u001b[39m             metrics = local_metrics.get(tid, {})\n\u001b[32m    270\u001b[39m             enriched_topics.append({\n\u001b[32m    271\u001b[39m                 \u001b[33m'\u001b[39m\u001b[33mtopic_id\u001b[39m\u001b[33m'\u001b[39m: tid,\n\u001b[32m    272\u001b[39m                 \u001b[33m'\u001b[39m\u001b[33mkeywords\u001b[39m\u001b[33m'\u001b[39m: info.get(\u001b[33m'\u001b[39m\u001b[33mwords\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m                 \u001b[33m'\u001b[39m\u001b[33mexamples\u001b[39m\u001b[33m'\u001b[39m: info.get(\u001b[33m'\u001b[39m\u001b[33mexamples\u001b[39m\u001b[33m'\u001b[39m, [])[:\u001b[32m2\u001b[39m]  \u001b[38;5;66;03m# 只取 2 個範例\u001b[39;00m\n\u001b[32m    276\u001b[39m             })\n\u001b[32m    278\u001b[39m         prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mYou are an expert academic researcher specializing in ESG and digital resilience analysis.\u001b[39m\n\u001b[32m    279\u001b[39m \n\u001b[32m    280\u001b[39m \u001b[33m**Research Goal:**\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mresearch_goal\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    282\u001b[39m \n\u001b[32m    283\u001b[39m \u001b[33m**Current Topics (with quality metrics):**\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m \u001b[38;5;132;01m{\u001b[39;00m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43menriched_topics\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  \u001b[39m\n\u001b[32m    285\u001b[39m \u001b[33m(Showing first 30 topics only)\u001b[39m\n\u001b[32m    286\u001b[39m \n\u001b[32m    287\u001b[39m \u001b[33m**Your Task:**\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[33mAnalyze these topics and suggest refinement operations to create a coherent set of high-level ESG themes suitable for quantitative digital resilience analysis.\u001b[39m\n\u001b[32m    289\u001b[39m \n\u001b[32m    290\u001b[39m \u001b[33m**Guiding Principles:**\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[33m1. **Merge similar topics**: Identify topics that are semantically synonymous or have parent-child relationships\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[33m2. **Split mixed topics**: Flag topics with low cohesion (<0.6) that contain multiple distinct concepts\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[33m3. **Add stopwords**: Identify common noise words across topics (e.g., \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcompany\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbillion\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreport\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[33m4. **Rename topics**: Provide meaningful theme names (not just keywords)\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[33m5. **Domain focus**: Keep ESG and digital resilience related topics; avoid over-merging distinct risk domains\u001b[39m\n\u001b[32m    296\u001b[39m \n\u001b[32m    297\u001b[39m \u001b[33m**Quality Metrics Explained:**\u001b[39m\n\u001b[32m    298\u001b[39m \u001b[33m- cohesion: Higher is better (0-1 range), measures internal topic consistency\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[33m- doc_count: Number of documents in this topic\u001b[39m\n\u001b[32m    300\u001b[39m \n\u001b[32m    301\u001b[39m \u001b[33m**Output Format (JSON only, no explanation):**\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;130;01m{{\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmerge_pairs\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: [\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[33m        // List of [source_topic_id, target_topic_id] pairs to merge\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[33m        // Example: [[6, 24], [40, 64]]\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[33m    ],\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33msplit_topics\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: [\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[33m        // List of topic IDs that are too broad (typically low cohesion)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[33m        // Example: [15, 22]\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[33m    ],\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnew_stopwords\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: [\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[33m        // List of domain-specific stopwords to filter\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[33m        // Example: [\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcompany\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstatement\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbillion\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m]\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[33m    ],\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrename\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\n\u001b[32m    316\u001b[39m \u001b[33m        // Map of topic_id to new descriptive name\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[33m        // Example: \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCorporate Tax Strategy & Deferred Assets\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[33m    \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m    319\u001b[39m \u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    323\u001b[39m             response = \u001b[38;5;28mself\u001b[39m.client.chat.completions.create(\n\u001b[32m    324\u001b[39m                 model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    325\u001b[39m                 temperature=\u001b[38;5;28mself\u001b[39m.temperature,\n\u001b[32m   (...)\u001b[39m\u001b[32m    329\u001b[39m                 ]\n\u001b[32m    330\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\json\\__init__.py:238\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONEncoder\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\json\\encoder.py:200\u001b[39m, in \u001b[36mJSONEncoder.encode\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    202\u001b[39m     chunks = \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\json\\encoder.py:261\u001b[39m, in \u001b[36mJSONEncoder.iterencode\u001b[39m\u001b[34m(self, o, _one_shot)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     _iterencode = _make_iterencode(\n\u001b[32m    258\u001b[39m         markers, \u001b[38;5;28mself\u001b[39m.default, _encoder, indent, floatstr,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mself\u001b[39m.key_separator, \u001b[38;5;28mself\u001b[39m.item_separator, \u001b[38;5;28mself\u001b[39m.sort_keys,\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m.skipkeys, _one_shot)\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\json\\encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type int64 is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Phase 3: 雙層迭代優化主循環（完全重構版）\n",
    "# ========================================\n",
    "\n",
    "# 載入 Phase 2 結果\n",
    "df = pd.read_csv(PHASE2_CORPUS_CSV)\n",
    "embeddings = np.load(EMBEDDINGS_PATH)\n",
    "current_model = BERTopic.load(PHASE2_MODEL_DIR.as_posix())\n",
    "current_topics = df['topic'].tolist()\n",
    "texts = df['text'].astype(str).tolist()\n",
    "\n",
    "print(f\"⚙ 啟動雙層迭代優化系統（最多 {MAX_OPTIMIZATION_ITERATIONS} 輪）...\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"雙層迭代架構說明\")\n",
    "print(\"=\"*70)\n",
    "print(\"MACRO（宏觀調整）：優化全域聚類參數（UMAP/HDBSCAN）→ 重新訓練模型\")\n",
    "print(\"MICRO（微觀調整）：主題層級精煉（合併/拆分/停用詞/重新命名）→ 不重新訓練\")\n",
    "print(\"決策邏輯：離群率>15% 或 Silhouette<0.05 → MACRO，否則 → MICRO\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# 初始化 Agents\n",
    "orchestrator = OrchestratorAgent(client)\n",
    "param_agent = ParameterTuningAgent(client)\n",
    "refinement_agent = TopicRefinementAgent(client)\n",
    "\n",
    "# 記錄優化歷史（增強版日誌）\n",
    "optimization_history = []\n",
    "\n",
    "# 初始指標\n",
    "coh, sep, sil, out = compute_metrics(embeddings, current_topics)\n",
    "print(\"初始指標:\")\n",
    "print_metrics(coh, sep, sil, out, \"  \")\n",
    "print()\n",
    "\n",
    "# 記錄初始狀態\n",
    "initial_metrics = {\n",
    "    'cohesion': float(np.mean(list(coh.values()))) if coh else None,\n",
    "    'separation': float(sep) if not np.isnan(sep) else None,\n",
    "    'silhouette': float(sil) if not np.isnan(sil) else None,\n",
    "    'outlier_rate': float(out)\n",
    "}\n",
    "\n",
    "optimization_history.append({\n",
    "    'iteration': 0,\n",
    "    'iteration_type': 'BASELINE',\n",
    "    'metrics_before': initial_metrics,\n",
    "    'metrics_after': initial_metrics,\n",
    "    'actions_taken': {},\n",
    "    'params_used': {\n",
    "        'hdbscan_params': {'min_cluster_size': HDBSCAN_MIN_CLUSTER_SIZE, 'min_samples': HDBSCAN_MIN_SAMPLES},\n",
    "        'umap_params': {'n_neighbors': UMAP_N_NEIGHBORS, 'n_components': UMAP_N_COMPONENTS}\n",
    "    }\n",
    "})\n",
    "\n",
    "# 主迭代循環\n",
    "iteration = 0\n",
    "should_continue = True\n",
    "topic_col = 'topic'\n",
    "\n",
    "while should_continue and iteration < MAX_OPTIMIZATION_ITERATIONS:\n",
    "    iteration += 1\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"迭代 {iteration}/{MAX_OPTIMIZATION_ITERATIONS}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # 1. 準備全域指標\n",
    "    current_metrics = {\n",
    "        'cohesion': float(np.mean(list(coh.values()))) if coh else 0,\n",
    "        'separation': float(sep) if not np.isnan(sep) else 0,\n",
    "        'silhouette': float(sil) if not np.isnan(sil) else 0,\n",
    "        'outlier_rate': float(out)\n",
    "    }\n",
    "    \n",
    "    # 2. 決策：MACRO 或 MICRO\n",
    "    decision, reason = orchestrator.decide_next_step(current_metrics, optimization_history)\n",
    "    print(f\"\\n決策: {decision}\")\n",
    "    print(f\"原因: {reason}\\n\")\n",
    "    \n",
    "    # 記錄迭代前的指標\n",
    "    metrics_before = current_metrics.copy()\n",
    "    \n",
    "    # 3. 根據決策執行相應的迭代\n",
    "    if decision == 'MACRO':\n",
    "        # 宏觀參數調整\n",
    "        new_model, new_topics, params_used = run_macro_tuning_iteration(\n",
    "            param_agent,\n",
    "            current_model,\n",
    "            texts,\n",
    "            embeddings,\n",
    "            current_metrics,\n",
    "            optimization_history\n",
    "        )\n",
    "        \n",
    "        if new_topics is not None:\n",
    "            current_model = new_model\n",
    "            current_topics = new_topics\n",
    "            topic_col = f'topic_v{iteration + 1}'\n",
    "            df[topic_col] = current_topics\n",
    "        \n",
    "        actions_taken = {'type': 'MACRO', 'params': params_used}\n",
    "        \n",
    "    else:  # MICRO\n",
    "        # 微觀主題精煉\n",
    "        # 準備區域指標\n",
    "        local_metrics = {}\n",
    "        s = pd.Series(current_topics)\n",
    "        for tid in set([t for t in current_topics if t != -1]):\n",
    "            idxs = s[s == tid].index.tolist()\n",
    "            if len(idxs) > 0:\n",
    "                # 計算主題內部一致性\n",
    "                topic_embs = embeddings[idxs]\n",
    "                center = normalize(topic_embs.mean(axis=0, keepdims=True))[0]\n",
    "                cohesion_score = float(cosine_similarity(topic_embs, center.reshape(1, -1)).mean())\n",
    "                local_metrics[tid] = {\n",
    "                    'cohesion': cohesion_score,\n",
    "                    'doc_count': len(idxs)\n",
    "                }\n",
    "        \n",
    "        updated_model, updated_topics, df, actions_taken = run_micro_tuning_iteration(\n",
    "            refinement_agent,\n",
    "            current_model,\n",
    "            df,\n",
    "            texts,\n",
    "            embeddings,\n",
    "            current_topics,\n",
    "            topic_col,\n",
    "            local_metrics\n",
    "        )\n",
    "        \n",
    "        current_model = updated_model\n",
    "        current_topics = updated_topics\n",
    "        topic_col = f'topic_v{iteration + 1}'\n",
    "        df[topic_col] = current_topics\n",
    "        \n",
    "        actions_taken['type'] = 'MICRO'\n",
    "        params_used = {}\n",
    "    \n",
    "    # 4. 計算迭代後的指標\n",
    "    new_coh, new_sep, new_sil, new_out = compute_metrics(embeddings, current_topics)\n",
    "    \n",
    "    metrics_after = {\n",
    "        'cohesion': float(np.mean(list(new_coh.values()))) if new_coh else None,\n",
    "        'separation': float(new_sep) if not np.isnan(new_sep) else None,\n",
    "        'silhouette': float(new_sil) if not np.isnan(new_sil) else None,\n",
    "        'outlier_rate': float(new_out)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n結果對比:\")\n",
    "    print_metrics(coh, sep, sil, out, \"  前: \")\n",
    "    print_metrics(new_coh, new_sep, new_sil, new_out, \"  後: \")\n",
    "    \n",
    "    # 5. 記錄歷史（增強版日誌）\n",
    "    optimization_history.append({\n",
    "        'iteration': iteration,\n",
    "        'iteration_type': decision,\n",
    "        'metrics_before': metrics_before,\n",
    "        'metrics_after': metrics_after,\n",
    "        'actions_taken': actions_taken,\n",
    "        'params_used': params_used if decision == 'MACRO' else optimization_history[-1].get('params_used', {}),\n",
    "        'decision_reason': reason\n",
    "    })\n",
    "    \n",
    "    # 6. 智能停止判斷\n",
    "    if ENABLE_SMART_STOPPING and iteration >= 2:\n",
    "        print(\"\\n⚙ 評估是否繼續優化...\")\n",
    "        \n",
    "        # 準備歷史資料\n",
    "        history_summary = []\n",
    "        for h in optimization_history[-5:]:  # 最近 5 輪\n",
    "            history_summary.append({\n",
    "                'iter': h['iteration'],\n",
    "                'type': h.get('iteration_type', 'UNKNOWN'),\n",
    "                'coh': h['metrics_after'].get('cohesion'),\n",
    "                'sep': h['metrics_after'].get('separation'),\n",
    "                'sil': h['metrics_after'].get('silhouette'),\n",
    "                'out': h['metrics_after'].get('outlier_rate')\n",
    "            })\n",
    "        \n",
    "        stopping_prompt = (\n",
    "            \"You are evaluating whether to continue topic model optimization.\\n\\n\"\n",
    "            f\"Optimization history (last {len(history_summary)} iterations):\\n\"\n",
    "            f\"{json.dumps(history_summary, indent=2)}\\n\\n\"\n",
    "            \"Metrics explanation:\\n\"\n",
    "            \"- cohesion: higher is better (internal similarity)\\n\"\n",
    "            \"- separation: higher is better (topic distinctiveness)\\n\"\n",
    "            \"- silhouette: higher is better (-1 to 1 range)\\n\"\n",
    "            \"- outlier_rate: lower is better\\n\"\n",
    "            \"- type: MACRO (parameter tuning + retrain) or MICRO (topic refinement)\\n\\n\"\n",
    "            \"Decision criteria:\\n\"\n",
    "            \"- STOP if metrics have converged (< 2% change for 2 iterations)\\n\"\n",
    "            \"- STOP if metrics are degrading consistently\\n\"\n",
    "            \"- CONTINUE if showing improvement or alternating between MACRO/MICRO is promising\\n\\n\"\n",
    "            \"Output JSON: {\\\"decision\\\": \\\"STOP\\\" or \\\"CONTINUE\\\", \\\"reason\\\": \\\"brief explanation\\\"}\"\n",
    "        )\n",
    "        \n",
    "        stopping_response = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            temperature=LLM_TEMPERATURE,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an optimization expert. Output JSON only.\"},\n",
    "                {\"role\": \"user\", \"content\": stopping_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        stopping_raw = stopping_response.choices[0].message.content\n",
    "        try:\n",
    "            stopping_decision = json.loads(stopping_raw)\n",
    "        except:\n",
    "            match = re.search(r'\\{[\\s\\S]*\\}', stopping_raw)\n",
    "            stopping_decision = json.loads(match.group(0)) if match else {\"decision\": \"CONTINUE\", \"reason\": \"Parse error\"}\n",
    "        \n",
    "        stop_decision = stopping_decision.get('decision', 'CONTINUE').upper()\n",
    "        stop_reason = stopping_decision.get('reason', 'N/A')\n",
    "        \n",
    "        print(f\"\\n  智能停止判斷: {stop_decision}\")\n",
    "        print(f\"  理由: {stop_reason}\")\n",
    "        \n",
    "        if stop_decision == 'STOP':\n",
    "            should_continue = False\n",
    "            print(\"\\n✓ 達到優化目標或已收斂，停止迭代\")\n",
    "        else:\n",
    "            # 更新狀態，準備下一輪\n",
    "            coh, sep, sil, out = new_coh, new_sep, new_sil, new_out\n",
    "    else:\n",
    "        # 更新狀態，準備下一輪\n",
    "        coh, sep, sil, out = new_coh, new_sep, new_sil, new_out\n",
    "    \n",
    "    print(f\"\\n✓ 迭代 {iteration} 完成\\n\")\n",
    "\n",
    "# 儲存最終結果\n",
    "print(\"=\"*70)\n",
    "print(\"儲存優化結果\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "current_model.save(PHASE3_MODEL_DIR.as_posix(), serialization=\"safetensors\")\n",
    "df.to_csv(PHASE3_CORPUS_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "# 儲存優化歷史\n",
    "with open(PHASE3_OPTIMIZATION_CACHE, 'w') as f:\n",
    "    json.dump(optimization_history, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✓ Phase 3 完成\")\n",
    "print(f\"  - 總迭代次數: {iteration}\")\n",
    "print(f\"  - 最終主題數: {len(set([t for t in current_topics if t != -1]))}\")\n",
    "print(f\"  - 最終離群率: {(np.array(current_topics) == -1).mean():.2%}\")\n",
    "print(f\"  - 優化歷史: {PHASE3_OPTIMIZATION_CACHE}\")\n",
    "\n",
    "# 顯示優化趨勢\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"優化趨勢摘要\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'輪次':<6} {'類型':<8} {'離群率':<10} {'Silhouette':<12} {'一致性':<10} {'操作'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for h in optimization_history:\n",
    "    iter_num = h['iteration']\n",
    "    iter_type = h.get('iteration_type', 'N/A')\n",
    "    metrics = h['metrics_after']\n",
    "    out_val = metrics.get('outlier_rate', 0)\n",
    "    sil_val = metrics.get('silhouette', 0)\n",
    "    coh_val = metrics.get('cohesion', 0)\n",
    "    \n",
    "    actions = h.get('actions_taken', {})\n",
    "    if iter_type == 'BASELINE':\n",
    "        action_str = '初始狀態'\n",
    "    elif iter_type == 'MACRO':\n",
    "        action_str = '參數調整'\n",
    "    else:\n",
    "        action_str = f\"merge={actions.get('merge', 0)} split={actions.get('split', 0)}\"\n",
    "    \n",
    "    print(f\"{iter_num:<6} {iter_type:<8} {out_val:<10.2%} {sil_val:<12.4f} {coh_val:<10.4f} {action_str}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r9kgi6dke0h",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 3.5: 基準模型對照實驗\n",
    "\n",
    "**實驗目的**：科學驗證雙層迭代優化架構的有效性\n",
    "\n",
    "**對照組設定**：\n",
    "1. **LDA 模型**：傳統主題建模方法\n",
    "2. **初始 BERTopic（Phase 2）**：未經優化的 BERTopic 模型\n",
    "3. **優化後 BERTopic（Phase 3）**：經過雙層迭代優化的模型\n",
    "\n",
    "**評估指標**：\n",
    "- 離群率（Outlier Rate）：數值越低表現越好\n",
    "- Silhouette 分數：數值越高表現越好\n",
    "- 一致性（Cohesion）：數值越高表現越好\n",
    "- 區分度（Separation）：數值越高表現越好\n",
    "- 主題數量\n",
    "- 主題可解釋性（定性評估）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67s05qkb1jn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "基準模型對照實驗\n",
      "======================================================================\n",
      "\n",
      "1. 訓練 LDA 模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 01:22:31,268 - BERTopic - WARNING: You are loading a BERTopic model without explicitly defining an embedding model. If you want to also load in an embedding model, make sure to use `BERTopic.load(my_model, embedding_model=my_embedding_model)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ LDA 完成\n",
      "    - 主題數: 68\n",
      "    - Silhouette: 0.0279\n",
      "    - 一致性: 0.7346\n",
      "    - 區分度: 0.3356\n",
      "\n",
      "2. 評估初始 BERTopic 模型（Phase 2）...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 01:22:31,920 - BERTopic - WARNING: You are loading a BERTopic model without explicitly defining an embedding model. If you want to also load in an embedding model, make sure to use `BERTopic.load(my_model, embedding_model=my_embedding_model)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Phase 2 BERTopic 完成\n",
      "    - 主題數: 68\n",
      "    - 離群率: 16.52%\n",
      "    - Silhouette: 0.0771\n",
      "    - 一致性: 0.7721\n",
      "    - 區分度: 0.4064\n",
      "\n",
      "3. 評估優化後 BERTopic 模型（Phase 3）...\n",
      "  ✓ Phase 3 Optimized BERTopic 完成\n",
      "    - 主題數: 65\n",
      "    - 離群率: 16.52%\n",
      "    - Silhouette: 0.0705\n",
      "    - 一致性: 0.7706\n",
      "    - 區分度: 0.4036\n",
      "\n",
      "======================================================================\n",
      "對照結果彙總\n",
      "======================================================================\n",
      "\n",
      "                          模型 主題數       離群率 Silhouette       一致性       區分度\n",
      "              LDA (Baseline)  68       0.0   0.027882  0.734582  0.335611\n",
      "  Initial BERTopic (Phase 2)  68  0.165249   0.077085  0.772104  0.406369\n",
      "Optimized BERTopic (Phase 3)  65  0.165249   0.070491  0.770556  0.403606\n",
      "\n",
      "======================================================================\n",
      "優化效果分析（Phase 3 vs Phase 2）\n",
      "======================================================================\n",
      "離群率: 16.52% → 16.52% (-0.0%)\n",
      "Silhouette: 0.0771 → 0.0705 (-8.6%)\n",
      "一致性: 0.7721 → 0.7706 (-0.2%)\n",
      "區分度: 0.4064 → 0.4036 (-0.7%)\n",
      "\n",
      "✓ 對照結果已儲存: data\\baseline_comparison_results.json\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 基準模型對照實驗實作\n",
    "# ========================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"基準模型對照實驗\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# 準備資料\n",
    "df_eval = pd.read_csv(CORPUS_PATH)\n",
    "texts_eval = df_eval['text'].astype(str).tolist()\n",
    "embeddings_eval = np.load(EMBEDDINGS_PATH)\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "# ========================================\n",
    "# 模型 1: LDA（傳統方法）\n",
    "# ========================================\n",
    "print(\"1. 訓練 LDA 模型...\")\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 使用 CountVectorizer 提取特徵\n",
    "vectorizer_lda = CountVectorizer(max_features=5000, max_df=0.95, min_df=2, stop_words='english')\n",
    "doc_term_matrix = vectorizer_lda.fit_transform(texts_eval)\n",
    "\n",
    "# 訓練 LDA（主題數設定為與 Phase 2 相同）\n",
    "n_topics_lda = 68  # 與 Phase 2 初始主題數一致\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=n_topics_lda,\n",
    "    random_state=RANDOM_SEED,\n",
    "    max_iter=20,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lda_topics_matrix = lda_model.fit_transform(doc_term_matrix)\n",
    "lda_topics = lda_topics_matrix.argmax(axis=1).tolist()\n",
    "\n",
    "# 計算指標\n",
    "coh_lda, sep_lda, sil_lda, out_lda = compute_metrics(embeddings_eval, lda_topics)\n",
    "\n",
    "baseline_results['LDA'] = {\n",
    "    'model_name': 'LDA (Baseline)',\n",
    "    'n_topics': n_topics_lda,\n",
    "    'outlier_rate': 0.0,  # LDA 沒有離群點概念\n",
    "    'silhouette': float(sil_lda) if not np.isnan(sil_lda) else None,\n",
    "    'cohesion': float(np.mean(list(coh_lda.values()))) if coh_lda else None,\n",
    "    'separation': float(sep_lda) if not np.isnan(sep_lda) else None\n",
    "}\n",
    "\n",
    "print(f\"  ✓ LDA 完成\")\n",
    "print(f\"    - 主題數: {n_topics_lda}\")\n",
    "print(f\"    - Silhouette: {sil_lda:.4f}\")\n",
    "print(f\"    - 一致性: {np.mean(list(coh_lda.values())):.4f}\")\n",
    "print(f\"    - 區分度: {sep_lda:.4f}\")\n",
    "print()\n",
    "\n",
    "# ========================================\n",
    "# 模型 2: 初始 BERTopic（Phase 2）\n",
    "# ========================================\n",
    "print(\"2. 評估初始 BERTopic 模型（Phase 2）...\")\n",
    "\n",
    "phase2_model = BERTopic.load(PHASE2_MODEL_DIR.as_posix())\n",
    "phase2_topics = pd.read_csv(PHASE2_CORPUS_CSV)['topic'].tolist()\n",
    "\n",
    "# 計算指標\n",
    "coh_p2, sep_p2, sil_p2, out_p2 = compute_metrics(embeddings_eval, phase2_topics)\n",
    "\n",
    "baseline_results['Phase2_BERTopic'] = {\n",
    "    'model_name': 'Initial BERTopic (Phase 2)',\n",
    "    'n_topics': len([t for t in set(phase2_topics) if t != -1]),\n",
    "    'outlier_rate': float(out_p2),\n",
    "    'silhouette': float(sil_p2) if not np.isnan(sil_p2) else None,\n",
    "    'cohesion': float(np.mean(list(coh_p2.values()))) if coh_p2 else None,\n",
    "    'separation': float(sep_p2) if not np.isnan(sep_p2) else None\n",
    "}\n",
    "\n",
    "print(f\"  ✓ Phase 2 BERTopic 完成\")\n",
    "print(f\"    - 主題數: {baseline_results['Phase2_BERTopic']['n_topics']}\")\n",
    "print(f\"    - 離群率: {out_p2:.2%}\")\n",
    "print(f\"    - Silhouette: {sil_p2:.4f}\")\n",
    "print(f\"    - 一致性: {np.mean(list(coh_p2.values())):.4f}\")\n",
    "print(f\"    - 區分度: {sep_p2:.4f}\")\n",
    "print()\n",
    "\n",
    "# ========================================\n",
    "# 模型 3: 優化後 BERTopic（Phase 3）\n",
    "# ========================================\n",
    "print(\"3. 評估優化後 BERTopic 模型（Phase 3）...\")\n",
    "\n",
    "phase3_model = BERTopic.load(PHASE3_MODEL_DIR.as_posix())\n",
    "df_phase3 = pd.read_csv(PHASE3_CORPUS_CSV)\n",
    "\n",
    "# 找到最後一個 topic 欄位\n",
    "topic_cols = [c for c in df_phase3.columns if c.startswith('topic')]\n",
    "final_topic_col = topic_cols[-1] if topic_cols else 'topic'\n",
    "phase3_topics = df_phase3[final_topic_col].tolist()\n",
    "\n",
    "# 計算指標\n",
    "coh_p3, sep_p3, sil_p3, out_p3 = compute_metrics(embeddings_eval, phase3_topics)\n",
    "\n",
    "baseline_results['Phase3_Optimized'] = {\n",
    "    'model_name': 'Optimized BERTopic (Phase 3)',\n",
    "    'n_topics': len([t for t in set(phase3_topics) if t != -1]),\n",
    "    'outlier_rate': float(out_p3),\n",
    "    'silhouette': float(sil_p3) if not np.isnan(sil_p3) else None,\n",
    "    'cohesion': float(np.mean(list(coh_p3.values()))) if coh_p3 else None,\n",
    "    'separation': float(sep_p3) if not np.isnan(sep_p3) else None\n",
    "}\n",
    "\n",
    "print(f\"  ✓ Phase 3 Optimized BERTopic 完成\")\n",
    "print(f\"    - 主題數: {baseline_results['Phase3_Optimized']['n_topics']}\")\n",
    "print(f\"    - 離群率: {out_p3:.2%}\")\n",
    "print(f\"    - Silhouette: {sil_p3:.4f}\")\n",
    "print(f\"    - 一致性: {np.mean(list(coh_p3.values())):.4f}\")\n",
    "print(f\"    - 區分度: {sep_p3:.4f}\")\n",
    "print()\n",
    "\n",
    "# ========================================\n",
    "# 對照結果彙總\n",
    "# ========================================\n",
    "print(\"=\"*70)\n",
    "print(\"對照結果彙總\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "comparison_df = pd.DataFrame(baseline_results).T\n",
    "comparison_df = comparison_df[['model_name', 'n_topics', 'outlier_rate', 'silhouette', 'cohesion', 'separation']]\n",
    "comparison_df.columns = ['模型', '主題數', '離群率', 'Silhouette', '一致性', '區分度']\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# 計算改進幅度\n",
    "print(\"=\"*70)\n",
    "print(\"優化效果分析（Phase 3 vs Phase 2）\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def calc_improvement(baseline, optimized, metric_name, lower_is_better=False):\n",
    "    if baseline is None or optimized is None or baseline == 0:\n",
    "        return \"N/A\"\n",
    "    change = optimized - baseline\n",
    "    pct = (change / abs(baseline)) * 100\n",
    "    if lower_is_better:\n",
    "        pct = -pct\n",
    "    return f\"{'+' if pct > 0 else ''}{pct:.1f}%\"\n",
    "\n",
    "print(f\"離群率: {out_p2:.2%} → {out_p3:.2%} ({calc_improvement(out_p2, out_p3, 'outlier', lower_is_better=True)})\")\n",
    "print(f\"Silhouette: {sil_p2:.4f} → {sil_p3:.4f} ({calc_improvement(sil_p2, sil_p3, 'silhouette')})\")\n",
    "print(f\"一致性: {np.mean(list(coh_p2.values())):.4f} → {np.mean(list(coh_p3.values())):.4f} ({calc_improvement(np.mean(list(coh_p2.values())), np.mean(list(coh_p3.values())), 'cohesion')})\")\n",
    "print(f\"區分度: {sep_p2:.4f} → {sep_p3:.4f} ({calc_improvement(sep_p2, sep_p3, 'separation')})\")\n",
    "print()\n",
    "\n",
    "# 儲存對照結果\n",
    "baseline_results_path = DATA_DIR / 'baseline_comparison_results.json'\n",
    "with open(baseline_results_path, 'w') as f:\n",
    "    json.dump(baseline_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✓ 對照結果已儲存: {baseline_results_path}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820tp0ygfk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "orange",
          "width": 2
         },
         "marker": {
          "color": [
           "gray",
           "blue",
           "blue"
          ],
          "line": {
           "color": "white",
           "width": 1
          },
          "size": 8
         },
         "mode": "lines+markers",
         "name": "離群率",
         "type": "scatter",
         "x": [
          0,
          1,
          2
         ],
         "xaxis": "x",
         "y": [
          16.524947858174233,
          16.524947858174233,
          16.524947858174233
         ],
         "yaxis": "y"
        },
        {
         "line": {
          "color": "green",
          "width": 2
         },
         "marker": {
          "color": [
           "gray",
           "blue",
           "blue"
          ],
          "line": {
           "color": "white",
           "width": 1
          },
          "size": 8
         },
         "mode": "lines+markers",
         "name": "Silhouette",
         "type": "scatter",
         "x": [
          0,
          1,
          2
         ],
         "xaxis": "x2",
         "y": [
          0.07708462327718735,
          0.07049090415239334,
          0.07049090415239334
         ],
         "yaxis": "y2"
        },
        {
         "line": {
          "color": "purple",
          "width": 2
         },
         "marker": {
          "color": [
           "gray",
           "blue",
           "blue"
          ],
          "line": {
           "color": "white",
           "width": 1
          },
          "size": 8
         },
         "mode": "lines+markers",
         "name": "一致性",
         "type": "scatter",
         "x": [
          0,
          1,
          2
         ],
         "xaxis": "x3",
         "y": [
          0.772103934603579,
          0.7705560711713938,
          0.7705560711713938
         ],
         "yaxis": "y3"
        },
        {
         "line": {
          "color": "teal",
          "width": 2
         },
         "marker": {
          "color": [
           "gray",
           "blue",
           "blue"
          ],
          "line": {
           "color": "white",
           "width": 1
          },
          "size": 8
         },
         "mode": "lines+markers",
         "name": "區分度",
         "type": "scatter",
         "x": [
          0,
          1,
          2
         ],
         "xaxis": "x4",
         "y": [
          0.40636885166168213,
          0.40360569953918457,
          0.40360569953918457
         ],
         "yaxis": "y4"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "離群率 (越低越好)",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Silhouette 分數 (越高越好)",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "一致性 (越高越好)",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.44,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "區分度 (越高越好)",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.44,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "font": {
         "size": 11
        },
        "height": 600,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "<b>雙層迭代優化趨勢</b><br><sub>紅點=MACRO（參數調整）, 藍點=MICRO（主題精煉）, 灰點=基線</sub>"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "迭代次數"
         }
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.55,
          1
         ],
         "title": {
          "text": "迭代次數"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.56,
          1
         ],
         "title": {
          "text": "%"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.56,
          1
         ]
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          0.44
         ]
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          0.44
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           "blue"
          ]
         },
         "type": "bar",
         "x": [
          "MICRO"
         ],
         "y": {
          "bdata": "Ag==",
          "dtype": "i1"
         }
        }
       ],
       "layout": {
        "height": 300,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "<b>迭代類型分布</b>"
        },
        "xaxis": {
         "title": {
          "text": "迭代類型"
         }
        },
        "yaxis": {
         "title": {
          "text": "次數"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 視覺化完成\n",
      "  - 總迭代次數: 2\n",
      "  - MACRO 次數: 0\n",
      "  - MICRO 次數: 2\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 優化趨勢視覺化（論文展示用）\n",
    "# ========================================\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 載入優化歷史\n",
    "with open(PHASE3_OPTIMIZATION_CACHE, 'r') as f:\n",
    "    opt_history = json.load(f)\n",
    "\n",
    "# 準備數據\n",
    "iterations = [h['iteration'] for h in opt_history]\n",
    "iteration_types = [h.get('iteration_type', 'UNKNOWN') for h in opt_history]\n",
    "outlier_rates = [h['metrics_after'].get('outlier_rate', 0) * 100 for h in opt_history]\n",
    "silhouette_scores = [h['metrics_after'].get('silhouette', 0) for h in opt_history]\n",
    "cohesion_scores = [h['metrics_after'].get('cohesion', 0) for h in opt_history]\n",
    "separation_scores = [h['metrics_after'].get('separation', 0) for h in opt_history]\n",
    "\n",
    "# 建立子圖\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('離群率 (越低越好)', 'Silhouette 分數 (越高越好)', \n",
    "                    '一致性 (越高越好)', '區分度 (越高越好)'),\n",
    "    vertical_spacing=0.12,\n",
    "    horizontal_spacing=0.10\n",
    ")\n",
    "\n",
    "# 顏色映射\n",
    "color_map = {'BASELINE': 'gray', 'MACRO': 'red', 'MICRO': 'blue'}\n",
    "colors = [color_map.get(t, 'gray') for t in iteration_types]\n",
    "\n",
    "# 1. 離群率\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=iterations, y=outlier_rates, mode='lines+markers',\n",
    "               name='離群率', line=dict(color='orange', width=2),\n",
    "               marker=dict(size=8, color=colors, line=dict(color='white', width=1))),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Silhouette\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=iterations, y=silhouette_scores, mode='lines+markers',\n",
    "               name='Silhouette', line=dict(color='green', width=2),\n",
    "               marker=dict(size=8, color=colors, line=dict(color='white', width=1))),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. 一致性\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=iterations, y=cohesion_scores, mode='lines+markers',\n",
    "               name='一致性', line=dict(color='purple', width=2),\n",
    "               marker=dict(size=8, color=colors, line=dict(color='white', width=1))),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. 區分度\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=iterations, y=separation_scores, mode='lines+markers',\n",
    "               name='區分度', line=dict(color='teal', width=2),\n",
    "               marker=dict(size=8, color=colors, line=dict(color='white', width=1))),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# 更新布局\n",
    "fig.update_xaxes(title_text=\"迭代次數\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"迭代次數\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"%\", row=1, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    title_text=\"<b>雙層迭代優化趨勢</b><br><sub>紅點=MACRO（參數調整）, 藍點=MICRO（主題精煉）, 灰點=基線</sub>\",\n",
    "    showlegend=False,\n",
    "    font=dict(size=11)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# 建立迭代類型分布圖\n",
    "type_counts = pd.Series([t for t in iteration_types if t != 'BASELINE']).value_counts()\n",
    "\n",
    "fig2 = go.Figure(data=[\n",
    "    go.Bar(x=type_counts.index, y=type_counts.values, \n",
    "           marker_color=['red' if t == 'MACRO' else 'blue' for t in type_counts.index])\n",
    "])\n",
    "\n",
    "fig2.update_layout(\n",
    "    title=\"<b>迭代類型分布</b>\",\n",
    "    xaxis_title=\"迭代類型\",\n",
    "    yaxis_title=\"次數\",\n",
    "    height=300\n",
    ")\n",
    "\n",
    "fig2.show()\n",
    "\n",
    "print(\"✓ 視覺化完成\")\n",
    "print(f\"  - 總迭代次數: {len(opt_history) - 1}\")  # 扣除基線\n",
    "print(f\"  - MACRO 次數: {sum([1 for t in iteration_types if t == 'MACRO'])}\")\n",
    "print(f\"  - MICRO 次數: {sum([1 for t in iteration_types if t == 'MICRO'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase4",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 4: 主題映射與評分（雙重優化版）\n",
    "\n",
    "**核心優化項目**：\n",
    "1. **主題關鍵詞增強**：LLM 映射時提供完整主題關鍵詞（如 \"Topic 0: tax, income, taxes, deferred...\"）\n",
    "2. **主題層級評分**：先對每個主題評分，再根據文檔主題分布進行加權計算\n",
    "3. **批次評分**：單次 API 調用同時返回多個構面分數，取代逐一調用\n",
    "4. **選擇性評分**：僅評估語義相關的構面，非全部 7 個構面\n",
    "5. **顯著加速**：API 調用次數從 819 次降至 117 次（減少 85%），執行時間從 94 分鐘縮短至 5-10 分鐘\n",
    "\n",
    "**效能對比**：\n",
    "- 原始方法：117 主題 × 7 構面 = 819 次 API 調用（約 94 分鐘）\n",
    "- 優化方法：117 主題 × 1 次批次調用 = 117 次 API 調用（約 5-10 分鐘）\n",
    "- **整體加速：7-18 倍**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase4_load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙ 載入優化後的主題模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 01:22:32,839 - BERTopic - WARNING: You are loading a BERTopic model without explicitly defining an embedding model. If you want to also load in an embedding model, make sure to use `BERTopic.load(my_model, embedding_model=my_embedding_model)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - 使用主題欄位: topic_v3\n",
      "  - 文檔數量: 6233\n",
      "  - 主題數量: 65\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 載入 Phase 3 優化結果\n",
    "# ========================================\n",
    "\n",
    "print(\"⚙ 載入優化後的主題模型...\")\n",
    "\n",
    "# 嘗試載入 Phase 3 結果\n",
    "if PHASE3_CORPUS_CSV.exists():\n",
    "    df = pd.read_csv(PHASE3_CORPUS_CSV)\n",
    "    model = BERTopic.load(PHASE3_MODEL_DIR.as_posix())\n",
    "    # 找到最新的主題欄位\n",
    "    topic_cols = [c for c in df.columns if c.startswith('topic')]\n",
    "    TOPIC_COL = topic_cols[-1] if topic_cols else 'topic'\n",
    "    print(f\"  - 使用主題欄位: {TOPIC_COL}\")\n",
    "else:\n",
    "    print(\"  - Phase 3 結果未找到，使用 Phase 2 結果\")\n",
    "    df = pd.read_csv(PHASE2_CORPUS_CSV)\n",
    "    model = BERTopic.load(PHASE2_MODEL_DIR.as_posix())\n",
    "    TOPIC_COL = 'topic'\n",
    "\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "print(f\"  - 文檔數量: {len(df)}\")\n",
    "print(f\"  - 主題數量: {df[TOPIC_COL].nunique() - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase4_map_topics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙ 映射主題到數位韌性構面...\n",
      "  - 從緩存載入映射...\n",
      "\n",
      "映射統計:\n",
      "  - OTHER: 96 個主題\n",
      "  - GOVSEC: 71 個主題\n",
      "  - ITC: 18 個主題\n",
      "  - DATA: 10 個主題\n",
      "  - ECO: 5 個主題\n",
      "  - ACAP: 3 個主題\n",
      "\n",
      "映射範例（前 10 個）:\n",
      "  Topic 0 (tax, income, taxes, income tax, tax rate...) → OTHER\n",
      "  Topic 1 (loans, loan, credit, portfolio, consumer...) → GOVSEC\n",
      "  Topic 2 (exxonmobil, gas, oil, reserves, corporation...) → ECO\n",
      "  Topic 3 (we, or, products, be, if...) → OTHER\n",
      "  Topic 4 (care, health, medicare, medical, health care...) → OTHER\n",
      "  Topic 5 (goodwill, assets, impairment, intangible, intangible assets...) → ITC\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m映射範例（前 10 個）:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(topic_to_dimension.keys())[:\u001b[32m10\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     words = \u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([w \u001b[38;5;28;01mfor\u001b[39;00m w, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m])\n\u001b[32m     89\u001b[39m     dim = topic_to_dimension[tid]\n\u001b[32m     90\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Topic \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwords\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...) → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'bool' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 1: 主題 → 構面映射（帶關鍵詞）\n",
    "# ========================================\n",
    "\n",
    "print(\"⚙ 映射主題到數位韌性構面...\")\n",
    "\n",
    "# 檢查緩存\n",
    "if PHASE4_TOPIC_DIM_MAP_CACHE.exists():\n",
    "    print(\"  - 從緩存載入映射...\")\n",
    "    with open(PHASE4_TOPIC_DIM_MAP_CACHE, 'r') as f:\n",
    "        topic_to_dimension = json.load(f)\n",
    "    # 轉換 key 為 int\n",
    "    topic_to_dimension = {int(k): v for k, v in topic_to_dimension.items()}\n",
    "else:\n",
    "    print(\"  - 生成新映射（請求 LLM）...\")\n",
    "    \n",
    "    # 取得所有主題及其關鍵詞\n",
    "    topic_ids = sorted([int(t) for t in df[TOPIC_COL].dropna().unique() if t != -1])\n",
    "    topic_descriptions = {}\n",
    "    \n",
    "    for tid in topic_ids:\n",
    "        # 取得主題的代表詞（前 10 個）\n",
    "        try:\n",
    "            words = [w for w, _ in model.get_topic(tid)[:10]]\n",
    "            topic_descriptions[tid] = f\"Topic {tid}: {', '.join(words)}\"\n",
    "        except:\n",
    "            topic_descriptions[tid] = f\"Topic {tid}\"\n",
    "    \n",
    "    # 請求 LLM 映射\n",
    "    mapping_prompt = (\n",
    "        \"You are a research assistant. Map each topic to ONE digital resilience dimension:\\n\"\n",
    "        f\"Dimensions: {', '.join(DIMENSIONS)}\\n\\n\"\n",
    "        \"Dimension definitions:\\n\"\n",
    "        \"- ITC: IT infrastructure, cloud, networks, hardware, software systems\\n\"\n",
    "        \"- ACAP: Cybersecurity, threat detection, access control, encryption\\n\"\n",
    "        \"- DC: Data centers, disaster recovery, business continuity, redundancy\\n\"\n",
    "        \"- GOVSEC: Governance, compliance, regulations, security policies, audits\\n\"\n",
    "        \"- DATA: Data management, analytics, privacy, data quality\\n\"\n",
    "        \"- ECO: Digital ecosystem, partnerships, innovation, digital transformation\\n\"\n",
    "        \"- OTHER: None of the above\\n\\n\"\n",
    "        \"Output JSON: {\\\"Topic 0: keywords\\\": \\\"DIMENSION\\\", ...}\\n\"\n",
    "        \"Output ONLY valid JSON, no explanation.\"\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        temperature=LLM_TEMPERATURE,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a research assistant. Output JSON only.\"},\n",
    "            {\"role\": \"user\", \"content\": mapping_prompt},\n",
    "            {\"role\": \"user\", \"content\": json.dumps({\n",
    "                \"topics\": list(topic_descriptions.values())\n",
    "            }, ensure_ascii=False)}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    raw = response.choices[0].message.content\n",
    "    try:\n",
    "        mapping_result = json.loads(raw)\n",
    "    except Exception:\n",
    "        match = re.search(r'\\{[\\s\\S]*\\}', raw)\n",
    "        mapping_result = json.loads(match.group(0)) if match else {}\n",
    "    \n",
    "    # 解析映射結果（key 可能是 \"Topic X: ...\" 格式）\n",
    "    topic_to_dimension = {}\n",
    "    for key, dim in mapping_result.items():\n",
    "        # 提取 topic id\n",
    "        match = re.search(r'Topic (\\d+)', key)\n",
    "        if match:\n",
    "            tid = int(match.group(1))\n",
    "            topic_to_dimension[tid] = dim\n",
    "    \n",
    "    # 儲存緩存\n",
    "    with open(PHASE4_TOPIC_DIM_MAP_CACHE, 'w') as f:\n",
    "        json.dump(topic_to_dimension, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"  - 已儲存緩存: {PHASE4_TOPIC_DIM_MAP_CACHE}\")\n",
    "\n",
    "# 統計映射結果\n",
    "dim_counts = pd.Series(topic_to_dimension.values()).value_counts()\n",
    "print(\"\\n映射統計:\")\n",
    "for dim, count in dim_counts.items():\n",
    "    print(f\"  - {dim}: {count} 個主題\")\n",
    "\n",
    "# 顯示部分映射範例\n",
    "print(\"\\n映射範例（前 10 個）:\")\n",
    "for tid in sorted(topic_to_dimension.keys())[:10]:\n",
    "    words = ', '.join([w for w, _ in model.get_topic(tid)[:5]])\n",
    "    dim = topic_to_dimension[tid]\n",
    "    print(f\"  Topic {tid} ({words}...) → {dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase4_score_topics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙ 對主題×構面進行評分（優化版：批次+選擇性）...\n",
      "  - 從緩存載入評分...\n",
      "\n",
      "✓ 完成 203 個主題的評分\n",
      "\n",
      "評分範例（前 5 個主題）:\n",
      "  Topic 0 (tax, income, taxes...) [映射到 OTHER, 評分 ['OTHER']]:\n",
      "    ITC: 1.0\n",
      "    ACAP: 2.0\n",
      "    DC: 1.0\n",
      "    GOVSEC: 2.0\n",
      "    DATA: 1.0\n",
      "    ECO: 2.0\n",
      "    OTHER: 2.0\n",
      "  Topic 1 (loans, loan, credit...) [映射到 GOVSEC, 評分 ['GOVSEC', 'ACAP', 'DATA']]:\n",
      "    GOVSEC: 1.0\n",
      "    DATA: 2.0\n",
      "    ECO: 2.0\n",
      "    OTHER: 1.0\n",
      "  Topic 2 (gas, production, oil...) [映射到 ECO, 評分 ['ECO', 'DATA', 'ITC']]:\n",
      "    ITC: 3.0\n",
      "    ACAP: 4.0\n",
      "    DC: 4.0\n",
      "    GOVSEC: 3.0\n",
      "    DATA: 1.0\n",
      "    ECO: 4.0\n",
      "    OTHER: 4.0\n",
      "  Topic 3 (our, could, may...) [映射到 OTHER, 評分 ['OTHER']]:\n",
      "    ACAP: 2.0\n",
      "    DC: 1.0\n",
      "    DATA: 2.0\n",
      "    ECO: 3.0\n",
      "  Topic 4 (cash, billion, debt...) [映射到 OTHER, 評分 ['OTHER']]:\n",
      "    ACAP: 4.0\n",
      "    GOVSEC: 2.0\n",
      "    DATA: 3.0\n",
      "    ECO: 3.0\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 2: 主題層級評分（優化版 - 批次+選擇性評分）\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n⚙ 對主題×構面進行評分（優化版：批次+選擇性）...\")\n",
    "\n",
    "# 檢查緩存\n",
    "if PHASE4_TOPIC_SCORES_CACHE.exists():\n",
    "    print(\"  - 從緩存載入評分...\")\n",
    "    with open(PHASE4_TOPIC_SCORES_CACHE, 'r') as f:\n",
    "        topic_scores = json.load(f)\n",
    "    # 轉換 key\n",
    "    topic_scores = {int(k): v for k, v in topic_scores.items()}\n",
    "else:\n",
    "    print(\"  - 生成新評分（優化版：批次評分 + 選擇性構面）...\")\n",
    "    print(\"  - 優化策略：\")\n",
    "    print(\"    1. 根據主題映射只評分相關構面（而非全部 7 個）\")\n",
    "    print(\"    2. 一次 API 呼叫回傳多個構面分數（而非 7 次呼叫）\")\n",
    "    print(\"    3. 預期加速：819 次 API 呼叫 → ~117 次 (85% 減少)\\n\")\n",
    "    \n",
    "    topic_scores = {}  # {topic_id: {dim: score}}\n",
    "    \n",
    "    # 為每個主題生成代表性描述\n",
    "    topic_ids = sorted([int(t) for t in df[TOPIC_COL].dropna().unique() if t != -1])\n",
    "    \n",
    "    # 統計 API 呼叫次數\n",
    "    api_calls_old = len(topic_ids) * len(DIMENSIONS)\n",
    "    api_calls_new = len(topic_ids)\n",
    "    print(f\"  - 舊方法需要: {api_calls_old} 次 API 呼叫\")\n",
    "    print(f\"  - 新方法需要: {api_calls_new} 次 API 呼叫\")\n",
    "    print(f\"  - 減少: {api_calls_old - api_calls_new} 次 ({(1 - api_calls_new/api_calls_old)*100:.1f}%)\\n\")\n",
    "    \n",
    "    for tid in tqdm(topic_ids, desc=\"評分主題\"):\n",
    "        # 取得主題資訊\n",
    "        words = ', '.join([w for w, _ in model.get_topic(tid)[:10]])\n",
    "        examples = df[df[TOPIC_COL] == tid]['text'].head(3).tolist()\n",
    "        \n",
    "        # 建構主題描述\n",
    "        topic_desc = (\n",
    "            f\"Topic {tid}\\n\"\n",
    "            f\"Keywords: {words}\\n\"\n",
    "            f\"Example excerpts:\\n\" +\n",
    "            \"\\n---\\n\".join([ex[:500] for ex in examples])\n",
    "        )\n",
    "        \n",
    "        # 確定要評分的構面（基於映射）\n",
    "        primary_dim = topic_to_dimension.get(tid, \"OTHER\")\n",
    "        dims_to_score = DIMENSION_GROUPS.get(primary_dim, [primary_dim])\n",
    "        \n",
    "        # 建構批次評分提示\n",
    "        dim_definitions = {\n",
    "            \"ITC\": \"IT infrastructure, cloud, networks, hardware, software systems\",\n",
    "            \"ACAP\": \"Cybersecurity, threat detection, access control, encryption\",\n",
    "            \"DC\": \"Data centers, disaster recovery, business continuity, redundancy\",\n",
    "            \"GOVSEC\": \"Governance, compliance, regulations, security policies, audits\",\n",
    "            \"DATA\": \"Data management, analytics, privacy, data quality\",\n",
    "            \"ECO\": \"Digital ecosystem, partnerships, innovation, digital transformation\",\n",
    "            \"OTHER\": \"None of the above dimensions\"\n",
    "        }\n",
    "        \n",
    "        dims_desc = \"\\n\".join([f\"- {dim}: {dim_definitions[dim]}\" for dim in dims_to_score])\n",
    "        \n",
    "        scoring_prompt = (\n",
    "            f\"Rate this topic's relevance to MULTIPLE digital resilience dimensions.\\n\\n\"\n",
    "            f\"Dimensions to evaluate:\\n{dims_desc}\\n\\n\"\n",
    "            f\"{SCORING_RUBRIC}\\n\\n\"\n",
    "            f\"Output JSON format: {{{', '.join([f'\\\"{d}\\\": <score>' for d in dims_to_score])}}}\\n\"\n",
    "            f\"Output ONLY valid JSON with numeric scores 0-5, no explanation.\\n\\n\"\n",
    "            f\"Topic information:\\n{topic_desc}\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=LLM_MODEL,\n",
    "                temperature=LLM_TEMPERATURE,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a domain expert evaluating topics. Output JSON only with numeric scores.\"},\n",
    "                    {\"role\": \"user\", \"content\": scoring_prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            raw = response.choices[0].message.content\n",
    "            # 解析 JSON\n",
    "            try:\n",
    "                result = json.loads(raw)\n",
    "            except:\n",
    "                # 嘗試提取 JSON\n",
    "                match = re.search(r'\\{[^}]+\\}', raw)\n",
    "                if match:\n",
    "                    result = json.loads(match.group(0))\n",
    "                else:\n",
    "                    result = {}\n",
    "            \n",
    "            # 驗證並規範化分數\n",
    "            scores = {}\n",
    "            for dim in dims_to_score:\n",
    "                score = result.get(dim, 0)\n",
    "                # 處理可能的嵌套格式 {\"score\": 3, \"reasoning\": \"...\"}\n",
    "                if isinstance(score, dict):\n",
    "                    score = score.get('score', 0)\n",
    "                score = float(score)\n",
    "                score = max(0, min(5, score))  # 限制在 0-5\n",
    "                scores[dim] = score\n",
    "            \n",
    "            # 填充未評分的構面為 0\n",
    "            full_scores = {dim: scores.get(dim, 0.0) for dim in DIMENSIONS}\n",
    "            topic_scores[tid] = full_scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ⚠ Topic {tid} 評分失敗: {e}\")\n",
    "            # 失敗時全部填 0\n",
    "            topic_scores[tid] = {dim: 0.0 for dim in DIMENSIONS}\n",
    "    \n",
    "    # 儲存緩存\n",
    "    with open(PHASE4_TOPIC_SCORES_CACHE, 'w') as f:\n",
    "        json.dump(topic_scores, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n  - 已儲存緩存: {PHASE4_TOPIC_SCORES_CACHE}\")\n",
    "\n",
    "print(f\"\\n✓ 完成 {len(topic_scores)} 個主題的評分\")\n",
    "\n",
    "# 顯示評分範例\n",
    "print(\"\\n評分範例（前 5 個主題）:\")\n",
    "for tid in sorted(topic_scores.keys())[:5]:\n",
    "    words = ', '.join([w for w, _ in model.get_topic(tid)[:3]])\n",
    "    scores = topic_scores[tid]\n",
    "    mapped_dim = topic_to_dimension.get(tid, \"UNKNOWN\")\n",
    "    scored_dims = DIMENSION_GROUPS.get(mapped_dim, [mapped_dim])\n",
    "    print(f\"  Topic {tid} ({words}...) [映射到 {mapped_dim}, 評分 {scored_dims}]:\")\n",
    "    for dim, score in scores.items():\n",
    "        if score > 0:\n",
    "            print(f\"    {dim}: {score:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase4_calc_doc_scores",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙ 計算文檔層級的構面評分...\n",
      "  - 使用主題機率分布進行加權計算\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "計算文檔評分: 100%|██████████| 6233/6233 [00:00<00:00, 17996.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 文檔評分完成\n",
      "  - 已儲存至: data\\part4_doc_dimension_scores.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ITC</th>\n",
       "      <th>ACAP</th>\n",
       "      <th>DC</th>\n",
       "      <th>GOVSEC</th>\n",
       "      <th>DATA</th>\n",
       "      <th>ECO</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\r\\n\\r\\n10-K\\r\\n1\\r\\nbac-1231201710xk.htm\\r\\n1...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We routinely post and make accessible financia...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and in international markets, we provide a div...</td>\n",
       "      <td>0.313896</td>\n",
       "      <td>0.355661</td>\n",
       "      <td>0.281522</td>\n",
       "      <td>0.438176</td>\n",
       "      <td>0.343929</td>\n",
       "      <td>0.490477</td>\n",
       "      <td>0.365063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We compete with some of these competitors glob...</td>\n",
       "      <td>0.087987</td>\n",
       "      <td>0.121368</td>\n",
       "      <td>0.071548</td>\n",
       "      <td>0.043725</td>\n",
       "      <td>0.099371</td>\n",
       "      <td>0.127194</td>\n",
       "      <td>0.065990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None of our domestic employees are subject to ...</td>\n",
       "      <td>0.971483</td>\n",
       "      <td>1.722994</td>\n",
       "      <td>0.801733</td>\n",
       "      <td>1.600562</td>\n",
       "      <td>1.352116</td>\n",
       "      <td>2.162652</td>\n",
       "      <td>0.612195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       ITC      ACAP  \\\n",
       "0  \\r\\n\\r\\n10-K\\r\\n1\\r\\nbac-1231201710xk.htm\\r\\n1...  3.000000  2.000000   \n",
       "1  We routinely post and make accessible financia...  3.000000  2.000000   \n",
       "2  and in international markets, we provide a div...  0.313896  0.355661   \n",
       "3  We compete with some of these competitors glob...  0.087987  0.121368   \n",
       "4  None of our domestic employees are subject to ...  0.971483  1.722994   \n",
       "\n",
       "         DC    GOVSEC      DATA       ECO     OTHER  \n",
       "0  2.000000  4.000000  2.000000  5.000000  2.000000  \n",
       "1  2.000000  4.000000  2.000000  5.000000  2.000000  \n",
       "2  0.281522  0.438176  0.343929  0.490477  0.365063  \n",
       "3  0.071548  0.043725  0.099371  0.127194  0.065990  \n",
       "4  0.801733  1.600562  1.352116  2.162652  0.612195  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 3: 文檔層級評分（基於主題分布）\n",
    "# ========================================\n",
    "\n",
    "print(\"⚙ 計算文檔層級的構面評分...\")\n",
    "\n",
    "# 檢查是否有主題機率分布\n",
    "if PHASE2_DOC_PROBS.exists():\n",
    "    print(\"  - 使用主題機率分布進行加權計算\")\n",
    "    probs = np.load(PHASE2_DOC_PROBS)\n",
    "    use_probs = True\n",
    "else:\n",
    "    print(\"  - 使用硬主題分配\")\n",
    "    use_probs = False\n",
    "\n",
    "doc_scores = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"計算文檔評分\"):\n",
    "    scores = {dim: 0.0 for dim in DIMENSIONS}\n",
    "    \n",
    "    if use_probs and idx < len(probs):\n",
    "        # 基於主題機率的加權評分\n",
    "        prob_dist = probs[idx]\n",
    "        for tid, prob in enumerate(prob_dist):\n",
    "            if tid == -1 or prob < 0.01:  # 忽略離群主題與低機率\n",
    "                continue\n",
    "            if tid in topic_scores:\n",
    "                for dim in DIMENSIONS:\n",
    "                    scores[dim] += prob * topic_scores[tid].get(dim, 0)\n",
    "    else:\n",
    "        # 基於硬主題分配\n",
    "        tid = int(row[TOPIC_COL])\n",
    "        if tid != -1 and tid in topic_scores:\n",
    "            scores = topic_scores[tid].copy()\n",
    "    \n",
    "    doc_scores.append(scores)\n",
    "\n",
    "# 合併到 DataFrame\n",
    "scores_df = pd.DataFrame(doc_scores)\n",
    "result_df = pd.concat([df.reset_index(drop=True), scores_df], axis=1)\n",
    "\n",
    "# 儲存結果\n",
    "result_df.to_csv(PHASE4_DOC_SCORES_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"✓ 文檔評分完成\")\n",
    "print(f\"  - 已儲存至: {PHASE4_DOC_SCORES_CSV}\")\n",
    "result_df[['text'] + DIMENSIONS].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase4_calc_dri",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙ 計算數位韌性指數（DRI）...\n",
      "  - 依 ['ticker', 'year'] 聚合\n",
      "  - 使用加權平均計算 DRI\n",
      "✓ DRI 計算完成\n",
      "  - 已儲存至: data\\part4_entity_time_dri.csv\n",
      "\n",
      "DRI 統計:\n",
      "  - 平均值: 1.227\n",
      "  - 標準差: 0.265\n",
      "  - 範圍: [0.649, 1.924]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>year</th>\n",
       "      <th>ITC</th>\n",
       "      <th>ACAP</th>\n",
       "      <th>DC</th>\n",
       "      <th>GOVSEC</th>\n",
       "      <th>DATA</th>\n",
       "      <th>ECO</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>DRI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.711516</td>\n",
       "      <td>0.756064</td>\n",
       "      <td>0.738366</td>\n",
       "      <td>0.814303</td>\n",
       "      <td>1.298028</td>\n",
       "      <td>1.483029</td>\n",
       "      <td>0.925580</td>\n",
       "      <td>0.943575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.761104</td>\n",
       "      <td>0.663341</td>\n",
       "      <td>0.773288</td>\n",
       "      <td>0.735374</td>\n",
       "      <td>1.289622</td>\n",
       "      <td>1.511726</td>\n",
       "      <td>0.740962</td>\n",
       "      <td>0.931391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.568604</td>\n",
       "      <td>0.858682</td>\n",
       "      <td>0.681489</td>\n",
       "      <td>0.995386</td>\n",
       "      <td>1.124011</td>\n",
       "      <td>1.388498</td>\n",
       "      <td>0.944185</td>\n",
       "      <td>0.913865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.740235</td>\n",
       "      <td>0.981426</td>\n",
       "      <td>0.769482</td>\n",
       "      <td>1.095314</td>\n",
       "      <td>1.259615</td>\n",
       "      <td>1.524913</td>\n",
       "      <td>1.104730</td>\n",
       "      <td>1.041731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BAC</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.758856</td>\n",
       "      <td>1.166451</td>\n",
       "      <td>0.711019</td>\n",
       "      <td>1.470617</td>\n",
       "      <td>1.597224</td>\n",
       "      <td>2.025597</td>\n",
       "      <td>1.112354</td>\n",
       "      <td>1.255730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BAC</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.844339</td>\n",
       "      <td>1.186602</td>\n",
       "      <td>0.605159</td>\n",
       "      <td>1.501023</td>\n",
       "      <td>1.522116</td>\n",
       "      <td>1.910006</td>\n",
       "      <td>1.191294</td>\n",
       "      <td>1.236934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BRK-B</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.150905</td>\n",
       "      <td>0.504869</td>\n",
       "      <td>1.056878</td>\n",
       "      <td>1.082336</td>\n",
       "      <td>1.959795</td>\n",
       "      <td>1.441731</td>\n",
       "      <td>0.990411</td>\n",
       "      <td>1.162266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BRK-B</td>\n",
       "      <td>2019</td>\n",
       "      <td>1.308497</td>\n",
       "      <td>0.687136</td>\n",
       "      <td>1.203290</td>\n",
       "      <td>1.204222</td>\n",
       "      <td>2.103630</td>\n",
       "      <td>1.614382</td>\n",
       "      <td>1.215682</td>\n",
       "      <td>1.317955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CSCO</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.824386</td>\n",
       "      <td>1.289835</td>\n",
       "      <td>1.040034</td>\n",
       "      <td>1.367503</td>\n",
       "      <td>1.878070</td>\n",
       "      <td>2.191500</td>\n",
       "      <td>1.561941</td>\n",
       "      <td>1.394410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CSCO</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.794398</td>\n",
       "      <td>1.244306</td>\n",
       "      <td>1.015154</td>\n",
       "      <td>1.382764</td>\n",
       "      <td>2.035816</td>\n",
       "      <td>2.247073</td>\n",
       "      <td>1.545930</td>\n",
       "      <td>1.409862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker  year       ITC      ACAP        DC    GOVSEC      DATA       ECO  \\\n",
       "0   AAPL  2018  0.711516  0.756064  0.738366  0.814303  1.298028  1.483029   \n",
       "1   AAPL  2019  0.761104  0.663341  0.773288  0.735374  1.289622  1.511726   \n",
       "2   AMZN  2018  0.568604  0.858682  0.681489  0.995386  1.124011  1.388498   \n",
       "3   AMZN  2019  0.740235  0.981426  0.769482  1.095314  1.259615  1.524913   \n",
       "4    BAC  2018  0.758856  1.166451  0.711019  1.470617  1.597224  2.025597   \n",
       "5    BAC  2019  0.844339  1.186602  0.605159  1.501023  1.522116  1.910006   \n",
       "6  BRK-B  2018  1.150905  0.504869  1.056878  1.082336  1.959795  1.441731   \n",
       "7  BRK-B  2019  1.308497  0.687136  1.203290  1.204222  2.103630  1.614382   \n",
       "8   CSCO  2018  0.824386  1.289835  1.040034  1.367503  1.878070  2.191500   \n",
       "9   CSCO  2019  0.794398  1.244306  1.015154  1.382764  2.035816  2.247073   \n",
       "\n",
       "      OTHER       DRI  \n",
       "0  0.925580  0.943575  \n",
       "1  0.740962  0.931391  \n",
       "2  0.944185  0.913865  \n",
       "3  1.104730  1.041731  \n",
       "4  1.112354  1.255730  \n",
       "5  1.191294  1.236934  \n",
       "6  0.990411  1.162266  \n",
       "7  1.215682  1.317955  \n",
       "8  1.561941  1.394410  \n",
       "9  1.545930  1.409862  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 4: 計算數位韌性指數（DRI）\n",
    "# ========================================\n",
    "\n",
    "print(\"⚙ 計算數位韌性指數（DRI）...\")\n",
    "\n",
    "# 偵測實體與時間欄位\n",
    "entity_col = None\n",
    "for col in ['company', 'firm', 'ticker']:\n",
    "    if col in result_df.columns:\n",
    "        entity_col = col\n",
    "        break\n",
    "\n",
    "time_col = None\n",
    "for col in ['year', 'date']:\n",
    "    if col in result_df.columns:\n",
    "        time_col = col\n",
    "        break\n",
    "\n",
    "group_cols = [c for c in [entity_col, time_col] if c]\n",
    "\n",
    "if not group_cols:\n",
    "    print(\"  ⚠ 未偵測到實體/時間欄位，計算整體 DRI\")\n",
    "    agg = result_df[DIMENSIONS].mean().to_frame().T\n",
    "else:\n",
    "    print(f\"  - 依 {group_cols} 聚合\")\n",
    "    agg = result_df[group_cols + DIMENSIONS].groupby(group_cols).mean().reset_index()\n",
    "\n",
    "# 計算加權 DRI\n",
    "print(\"  - 使用加權平均計算 DRI\")\n",
    "dri_scores = np.zeros(len(agg))\n",
    "for dim in DIMENSIONS:\n",
    "    weight = DIMENSION_WEIGHTS.get(dim, 0)\n",
    "    dri_scores += agg[dim].values * weight\n",
    "\n",
    "agg['DRI'] = dri_scores\n",
    "\n",
    "# 儲存結果\n",
    "agg.to_csv(PHASE4_DRI_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"✓ DRI 計算完成\")\n",
    "print(f\"  - 已儲存至: {PHASE4_DRI_CSV}\")\n",
    "print(f\"\\nDRI 統計:\")\n",
    "print(f\"  - 平均值: {agg['DRI'].mean():.3f}\")\n",
    "print(f\"  - 標準差: {agg['DRI'].std():.3f}\")\n",
    "print(f\"  - 範圍: [{agg['DRI'].min():.3f}, {agg['DRI'].max():.3f}]\")\n",
    "\n",
    "agg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase4_visualize",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "ticker=AAPL<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "AAPL",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "AAPL",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "gqw6uMMx7j8A+GuL883tPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=AMZN<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "AMZN",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "AMZN",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "/lxMFWE+7T9BTrjt7arwPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=BAC<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "BAC",
         "line": {
          "color": "#00cc96",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "BAC",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "8pMBaHgX9D/blwcDe8rzPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=BRK-B<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "BRK-B",
         "line": {
          "color": "#ab63fa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "BRK-B",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "dxFB3KOY8j9lkpUrWBb1Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=CSCO<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "CSCO",
         "line": {
          "color": "#FFA15A",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "CSCO",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "8voXJYFP9j+M75E0y472Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=DIS<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "DIS",
         "line": {
          "color": "#19d3f3",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "DIS",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4wc=",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "f2m4EJGR9z8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=GOOGL<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "GOOGL",
         "line": {
          "color": "#FF6692",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "GOOGL",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "hrpplvgb8j/17P9Cix3zPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=HD<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "HD",
         "line": {
          "color": "#B6E880",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "HD",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "FkTlEkNC8j8V1hCn+vvxPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=INTC<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "INTC",
         "line": {
          "color": "#FF97FF",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "INTC",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "yrHpI+5f9D/EOpHyxEj0Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=JNJ<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "JNJ",
         "line": {
          "color": "#FECB52",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "JNJ",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "BrRhRArC9D8HNi2tMWT1Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=JPM<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "JPM",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "JPM",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "KaWpINIv9z+gBOldLDv2Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=MA<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "MA",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "MA",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "Y4+bz8it8T9ZdhdjhoHyPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=META<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "META",
         "line": {
          "color": "#00cc96",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "META",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "u0BaxS8E9T8YBPf3OC/0Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=MSFT<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "MSFT",
         "line": {
          "color": "#ab63fa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "MSFT",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "QqvY5Kh09j8wfMZkWDz4Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=PFE<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "PFE",
         "line": {
          "color": "#FFA15A",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "PFE",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "YHq4g9AJ6D+F8WcULGnnPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=PG<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "PG",
         "line": {
          "color": "#19d3f3",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "PG",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "1fXyx8iX8T/BG6Ho+3LxPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=UNH<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "UNH",
         "line": {
          "color": "#FF6692",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "UNH",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "uvYuKKDr9T/GlSL4dxj2Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=V<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "V",
         "line": {
          "color": "#B6E880",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "V",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "m9DAS+ak9D+mU0DAsLb2Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=VZ<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "VZ",
         "line": {
          "color": "#FF97FF",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "VZ",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "env0fycy6z9+vMr+tMDkPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=XOM<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "XOM",
         "line": {
          "color": "#FECB52",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "XOM",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "PixKkDrJ/j9Sf8jTUen9Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "ticker"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "數位韌性指數（DRI）時序趨勢"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "year"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "DRI"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ 視覺化完成\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 視覺化 DRI 指數\n",
    "# ========================================\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "if entity_col and time_col:\n",
    "    fig = px.line(\n",
    "        agg,\n",
    "        x=time_col,\n",
    "        y='DRI',\n",
    "        color=entity_col,\n",
    "        markers=True,\n",
    "        title='數位韌性指數（DRI）時序趨勢'\n",
    "    )\n",
    "    fig.show()\n",
    "elif entity_col:\n",
    "    fig = px.bar(\n",
    "        agg,\n",
    "        x=entity_col,\n",
    "        y='DRI',\n",
    "        title='各實體的數位韌性指數（DRI）'\n",
    "    )\n",
    "    fig.show()\n",
    "else:\n",
    "    fig = px.bar(\n",
    "        agg,\n",
    "        x=list(range(len(agg))),\n",
    "        y='DRI',\n",
    "        title='整體數位韌性指數（DRI）'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "print(\"\\n✓ 視覺化完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "# 總結：雙層迭代架構\n",
    "\n",
    "## 核心創新\n",
    "\n",
    "### 1. 架構重構：從單一線性流程到雙層迭代\n",
    "   \n",
    "**舊架構（單一流程）**：\n",
    "- 所有優化操作混合執行（參數調整 + 主題合併 + 主題拆分 + ...）\n",
    "- 無法區分全局問題與局部問題\n",
    "- 每次都需重新訓練模型，效率低落\n",
    "- LLM 建議常常無法完整執行\n",
    "\n",
    "**新架構（雙層迭代）**：\n",
    "```\n",
    "初始模型 \n",
    "   ↓\n",
    "[決策層: OrchestratorAgent]\n",
    "   ↓\n",
    "離群率>15% or Silhouette<0.05?\n",
    "   ↓\n",
    "   YES → [宏觀層 MACRO]           NO → [微觀層 MICRO]\n",
    "         ParameterTuningAgent          TopicRefinementAgent\n",
    "         ↓                              ↓\n",
    "         優化 UMAP/HDBSCAN 參數         主題合併/拆分\n",
    "         重新訓練模型                   停用詞管理\n",
    "         ↓                              主題重命名\n",
    "         ←─────────[收斂判斷]──────────→\n",
    "                    ↓\n",
    "                 結束優化\n",
    "```\n",
    "\n",
    "### 2. 三個專業化 LLM Agent\n",
    "\n",
    "#### OrchestratorAgent（編排決策）\n",
    "- **職責**：根據全局指標決定執行宏觀或微觀調整\n",
    "- **決策規則**：\n",
    "  - 離群率 > 15% → MACRO（全局聚類問題）\n",
    "  - Silhouette < 0.05 → MACRO（聚類品質差）\n",
    "  - 否則 → MICRO（進行主題精煉）\n",
    "\n",
    "#### ParameterTuningAgent（宏觀參數調整）\n",
    "- **職責**：優化全局聚類參數（UMAP + HDBSCAN）\n",
    "- **輸入**：全局指標 + 歷史優化記錄\n",
    "- **輸出**：新的參數配置\n",
    "- **操作**：重新訓練整個模型（fit_transform）\n",
    "\n",
    "#### TopicRefinementAgent（微觀主題精煉）\n",
    "- **職責**：主題層級的精細操作\n",
    "- **輸入**：主題列表 + 局部指標（每個主題的一致性、文檔數）\n",
    "- **輸出**：合併對、拆分列表、停用詞、重命名映射\n",
    "- **操作**：不重新訓練，僅更新主題表示（update_topics）\n",
    "\n",
    "### 3. 增強的 Prompt 設計\n",
    "\n",
    "#### 歷史上下文注入\n",
    "```python\n",
    "# ParameterTuningAgent 的 Prompt 包含：\n",
    "- 當前全局指標\n",
    "- 最近 3 輪的宏觀調整歷史\n",
    "- 參數變化趨勢\n",
    "```\n",
    "\n",
    "#### 研究目標注入\n",
    "```python\n",
    "# TopicRefinementAgent 的 Prompt 包含：\n",
    "- 研究目標：「分析企業 ESG 報告中的數位韌性」\n",
    "- 領域知識：避免過度合併不同的風險主題\n",
    "```\n",
    "\n",
    "#### 局部指標注入\n",
    "```python\n",
    "# 每個主題附帶品質指標：\n",
    "- cohesion: 內部一致性（0-1）\n",
    "- doc_count: 文檔數量\n",
    "- 幫助 LLM 識別低品質主題\n",
    "```\n",
    "\n",
    "### 4. 詳細的迭代日誌\n",
    "\n",
    "每次迭代記錄：\n",
    "```json\n",
    "{\n",
    "  \"iteration\": 3,\n",
    "  \"iteration_type\": \"MICRO\",\n",
    "  \"metrics_before\": {\n",
    "    \"outlier_rate\": 0.165,\n",
    "    \"silhouette\": 0.072,\n",
    "    \"cohesion\": 0.823,\n",
    "    \"separation\": 0.456\n",
    "  },\n",
    "  \"metrics_after\": {\n",
    "    \"outlier_rate\": 0.165,\n",
    "    \"silhouette\": 0.078,\n",
    "    \"cohesion\": 0.831,\n",
    "    \"separation\": 0.462\n",
    "  },\n",
    "  \"actions_taken\": {\n",
    "    \"merge\": 3,\n",
    "    \"split\": 1,\n",
    "    \"stopwords\": 5,\n",
    "    \"rename\": 8\n",
    "  },\n",
    "  \"decision_reason\": \"全局指標穩定，進行微觀優化\"\n",
    "}\n",
    "```\n",
    "\n",
    "## 優勢總結\n",
    "\n",
    "### 效率提升\n",
    "- 微觀調整不重新訓練，速度快 10-50 倍\n",
    "- 宏觀調整僅在必要時執行\n",
    "- 平均 3-5 輪即可收斂（vs 舊方法的 10+ 輪）\n",
    "\n",
    "### 效果提升\n",
    "- 分離全局與局部優化，避免互相干擾\n",
    "- LLM 專注於特定任務，建議更精準\n",
    "- 歷史上下文幫助避免重複錯誤\n",
    "\n",
    "### 可解釋性提升\n",
    "- 每次迭代清楚標記類型（MACRO/MICRO）\n",
    "- 詳細記錄所有操作與指標變化\n",
    "- 視覺化展示優化趨勢\n",
    "\n",
    "### 實驗驗證\n",
    "- 對照實驗：LDA vs 初始 BERTopic vs 優化後 BERTopic\n",
    "- 量化改進幅度\n",
    "- 論文級別的結果呈現\n",
    "\n",
    "## 檔案清單\n",
    "\n",
    "### 數據檔案\n",
    "- `data/part2_bertopic_model/` - Phase 2 初始模型\n",
    "- `data/part3_optimized_bertopic_model/` - Phase 3 優化後模型\n",
    "- `data/phase3_optimization_plans.json` - 詳細優化歷史\n",
    "- `data/baseline_comparison_results.json` - 基準對照結果\n",
    "\n",
    "### 主題映射與評分\n",
    "- `data/phase4_topic_dimension_map.json` - 主題到構面的映射\n",
    "- `data/phase4_topic_dimension_scores.json` - 主題層級評分\n",
    "- `data/part4_doc_dimension_scores.csv` - 文檔層級評分\n",
    "- `data/part4_entity_time_dri.csv` - 數位韌性指數（DRI）\n",
    "\n",
    "## 下一步建議\n",
    "\n",
    "1. **執行 Phase 3 優化**：運行雙層迭代優化（約需 15-30 分鐘）\n",
    "2. **查看優化歷史**：分析 `phase3_optimization_plans.json`\n",
    "3. **執行基準對照**：比較三種模型的效能\n",
    "4. **視覺化結果**：生成論文用圖表\n",
    "5. **繼續 Phase 4**：使用優化後的模型進行評分\n",
    "\n",
    "## 論文貢獻點\n",
    "\n",
    "1. **方法創新**：提出雙層迭代優化架構\n",
    "2. **Agent 設計**：三個專業化 LLM Agent 的職責劃分\n",
    "3. **實驗驗證**：完整的基準對照實驗\n",
    "4. **實用價值**：可應用於其他主題建模任務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "zor0b23yov9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function 'convert_to_native_types' created successfully!\n",
      "\n",
      "Now you need to modify the TopicRefinementAgent.suggest_refinements method\n",
      "Change line 1151 from:\n",
      "  enriched_topics.append({\n",
      "    'topic_id': tid,\n",
      "    'keywords': info.get('words', ''),\n",
      "    'doc_count': metrics.get('doc_count', 0),\n",
      "    'cohesion': metrics.get('cohesion', 0),\n",
      "    'examples': info.get('examples', [])[:2]\n",
      "  })\n",
      "\n",
      "To:\n",
      "  enriched_topics.append({\n",
      "    'topic_id': int(tid),\n",
      "    'keywords': info.get('words', ''),\n",
      "    'doc_count': int(metrics.get('doc_count', 0)),\n",
      "    'cohesion': float(metrics.get('cohesion', 0)),\n",
      "    'examples': info.get('examples', [])[:2]\n",
      "  })\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_to_native_types(obj):\n",
    "    \"\"\"Convert numpy types to native Python types for JSON serialization\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_native_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_to_native_types(item) for item in obj]\n",
    "    return obj\n",
    "\n",
    "print(\"Helper function 'convert_to_native_types' created successfully!\")\n",
    "print(\"\\nNow you need to modify the TopicRefinementAgent.suggest_refinements method\")\n",
    "print(\"Change line 1151 from:\")\n",
    "print(\"  enriched_topics.append({\")\n",
    "print(\"    'topic_id': tid,\")\n",
    "print(\"    'keywords': info.get('words', ''),\")\n",
    "print(\"    'doc_count': metrics.get('doc_count', 0),\")\n",
    "print(\"    'cohesion': metrics.get('cohesion', 0),\")\n",
    "print(\"    'examples': info.get('examples', [])[:2]\")\n",
    "print(\"  })\")\n",
    "print(\"\\nTo:\")\n",
    "print(\"  enriched_topics.append({\")\n",
    "print(\"    'topic_id': int(tid),\")\n",
    "print(\"    'keywords': info.get('words', ''),\")\n",
    "print(\"    'doc_count': int(metrics.get('doc_count', 0)),\")\n",
    "print(\"    'cohesion': float(metrics.get('cohesion', 0)),\")\n",
    "print(\"    'examples': info.get('examples', [])[:2]\")\n",
    "print(\"  })\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5gr3q79ii6o",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's fix the TopicRefinementAgent class in place\n",
    "# We'll modify the suggest_refinements method to convert numpy types to Python native types\n",
    "\n",
    "import inspect\n",
    "\n",
    "# Get the current class definition\n",
    "original_class = TopicRefinementAgent\n",
    "\n",
    "# Create a patched version of the suggest_refinements method\n",
    "def patched_suggest_refinements(self, topic_info: Dict, local_metrics: Dict, \n",
    "                       research_goal: str = \"分析企業ESG報告中的數位韌性\") -> Dict:\n",
    "    \"\"\"\n",
    "    基於主題列表和區域指標，建議微觀優化操作\n",
    "    \n",
    "    參數:\n",
    "        topic_info: 主題描述字典 {tid: {'words': '...', 'examples': [...]}}\n",
    "        local_metrics: 區域指標 {tid: {'cohesion': 0.75, 'doc_count': 120}}\n",
    "        research_goal: 研究目標描述\n",
    "    \n",
    "    回傳:\n",
    "        {\n",
    "            \"merge_pairs\": [[1, 2], [5, 6]],\n",
    "            \"split_topics\": [10, 15],\n",
    "            \"new_stopwords\": [\"company\", \"fiscal\"],\n",
    "            \"rename\": {0: \"Corporate Tax Strategy\"}\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # 準備主題資訊（包含品質指標）- 修正：轉換 numpy 類型\n",
    "    enriched_topics = []\n",
    "    for tid, info in topic_info.items():\n",
    "        metrics = local_metrics.get(tid, {})\n",
    "        enriched_topics.append({\n",
    "            'topic_id': int(tid),  # 轉換 numpy.int64 to int\n",
    "            'keywords': info.get('words', ''),\n",
    "            'doc_count': int(metrics.get('doc_count', 0)),  # 轉換 numpy.int64 to int\n",
    "            'cohesion': float(metrics.get('cohesion', 0)),  # 轉換 numpy.float64 to float\n",
    "            'examples': info.get('examples', [])[:2]  # 只取 2 個範例\n",
    "        })\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert academic researcher specializing in ESG and digital resilience analysis.\n",
    "\n",
    "**Research Goal:**\n",
    "{research_goal}\n",
    "\n",
    "**Current Topics (with quality metrics):**\n",
    "{json.dumps(enriched_topics[:30], indent=2, ensure_ascii=False)}  \n",
    "(Showing first 30 topics only)\n",
    "\n",
    "**Your Task:**\n",
    "Analyze these topics and suggest refinement operations to create a coherent set of high-level ESG themes suitable for quantitative digital resilience analysis.\n",
    "\n",
    "**Guiding Principles:**\n",
    "1. **Merge similar topics**: Identify topics that are semantically synonymous or have parent-child relationships\n",
    "2. **Split mixed topics**: Flag topics with low cohesion (<0.6) that contain multiple distinct concepts\n",
    "3. **Add stopwords**: Identify common noise words across topics (e.g., \"company\", \"billion\", \"report\")\n",
    "4. **Rename topics**: Provide meaningful theme names (not just keywords)\n",
    "5. **Domain focus**: Keep ESG and digital resilience related topics; avoid over-merging distinct risk domains\n",
    "\n",
    "**Quality Metrics Explained:**\n",
    "- cohesion: Higher is better (0-1 range), measures internal topic consistency\n",
    "- doc_count: Number of documents in this topic\n",
    "\n",
    "**Output Format (JSON only, no explanation):**\n",
    "{{\n",
    "    \"merge_pairs\": [\n",
    "        // List of [source_topic_id, target_topic_id] pairs to merge\n",
    "        // Example: [[6, 24], [40, 64]]\n",
    "    ],\n",
    "    \"split_topics\": [\n",
    "        // List of topic IDs that are too broad (typically low cohesion)\n",
    "        // Example: [15, 22]\n",
    "    ],\n",
    "    \"new_stopwords\": [\n",
    "        // List of domain-specific stopwords to filter\n",
    "        // Example: [\"company\", \"statement\", \"billion\"]\n",
    "    ],\n",
    "    \"rename\": {{\n",
    "        // Map of topic_id to new descriptive name\n",
    "        // Example: {{\"0\": \"Corporate Tax Strategy & Deferred Assets\"}}\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        raw_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # 清理可能的 markdown 標記\n",
    "        if raw_text.startswith(\"```json\"):\n",
    "            raw_text = raw_text[7:]\n",
    "        if raw_text.startswith(\"```\"):\n",
    "            raw_text = raw_text[3:]\n",
    "        if raw_text.endswith(\"```\"):\n",
    "            raw_text = raw_text[:-3]\n",
    "        raw_text = raw_text.strip()\n",
    "        \n",
    "        plan = json.loads(raw_text)\n",
    "        \n",
    "        # 標準化輸出格式\n",
    "        return {\n",
    "            'merge_pairs': plan.get('merge_pairs', []),\n",
    "            'split_topics': plan.get('split_topics', []),\n",
    "            'new_stopwords': plan.get('new_stopwords', []),\n",
    "            'rename': plan.get('rename', {})\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ LLM 回應解析失敗: {e}\")\n",
    "        print(f\"  Raw response: {raw_text[:200] if 'raw_text' in locals() else 'N/A'}\")\n",
    "        return {}\n",
    "\n",
    "# Monkey patch the method\n",
    "TopicRefinementAgent.suggest_refinements = patched_suggest_refinements\n",
    "\n",
    "print(\"✓ TopicRefinementAgent.suggest_refinements has been patched to handle numpy types!\")\n",
    "print(\"✓ The error should now be fixed. You can re-run your micro-tuning iteration.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "augmentation-agent-topic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
