{
 "cells": [
  {"cell_type":"markdown","metadata":{},"source":["# 第三階段：LLM Agent 迭代優化（自我批改 / 合併 / 重生）"]},
  {"cell_type":"code","metadata":{},"source":["# %pip install pandas numpy tqdm bertopic umap-learn hdbscan openai==1.* scikit-learn"],"execution_count":null,"outputs":[]},
  {"cell_type":"code","metadata":{},"source":["import os, json, math\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom openai import OpenAI\nfrom bertopic import BERTopic\nfrom umap import UMAP\nimport hdbscan\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n\nclient = OpenAI()\nBASE=Path('/mnt/data')\nMODEL_DIR=BASE/'part2_bertopic_model'\nCORPUS=BASE/'part2_corpus_with_topics.csv'\nEMB=BASE/'embeddings_text-3-small.npy'\nassert MODEL_DIR.exists() and CORPUS.exists() and EMB.exists()\n\ndf=pd.read_csv(CORPUS)\nembeddings=np.load(EMB)\nmodel=BERTopic.load(MODEL_DIR.as_posix())\ntopics=df['topic'].tolist()\n\ndef centers(emb, topics):\n    m={}\n    s=pd.Series(topics)\n    for tid, idxs in s.groupby(s).groups.items():\n        if tid==-1: continue\n        vecs=emb[list(idxs)]\n        m[tid]=normalize(vecs.mean(axis=0, keepdims=True))[0]\n    return m\n\ndef metrics(emb, topics):\n    c=centers(emb, topics)\n    # cohesion\n    s=pd.Series(topics)\n    coh={}\n    for tid, idxs in s.groupby(s).groups.items():\n        if tid==-1 or tid not in c: continue\n        sims=cosine_similarity(emb[list(idxs)], c[tid].reshape(1,-1)).ravel()\n        coh[tid]=float(np.mean(sims))\n    # separation\n    sep=np.nan\n    if len(c)>=2:\n        mat=np.vstack(list(c.values()))\n        sep=cosine_distances(mat).mean()\n    # silhouette\n    mask=np.array(topics)!=-1\n    sil=np.nan\n    if mask.sum()>5 and len(set(np.array(topics)[mask]))>1:\n        sil=silhouette_score(emb[mask], np.array(topics)[mask])\n    out=(np.array(topics)==-1).mean()\n    return coh, sep, sil, out\n\ncoh, sep, sil, out = metrics(embeddings, topics)\nprint('一致性均值:', np.mean(list(coh.values())) if coh else np.nan)\nprint('區分度均值:', sep)\nprint('Silhouette:', sil)\nprint('Outlier 比例:', out)"],"execution_count":null,"outputs":[]},
  {"cell_type":"code","metadata":{},"source":["def sample_topic_snippets(topic_model, df, k_each=3):\n    samples={}\n    for tid in topic_model.get_topic_info()['Topic'].tolist():\n        if tid==-1: continue\n        words=', '.join([w for w,_ in topic_model.get_topic(tid)[:10]])\n        ex=df[df['topic']==tid]['text'].head(k_each).tolist()\n        samples[tid]={\"words\":words, \"examples\":ex}\n    return samples\n\nsamples=sample_topic_snippets(model, df)\nplan_prompt='''你是一位主題建模審稿人。以下是主題代表詞與例句，請輸出 JSON：\n- \"merge_pairs\": [[a,b],...]\n- \"split_topics\": [id,...]\n- \"rename\": {id: \"新名稱\"}\n- \"new_stopwords\": [\"...\", ...]\n- \"params\": {\"min_cluster_size\": 25, \"min_samples\": 7, \"n_neighbors\": 15, \"n_components\": 10}\n僅輸出 JSON，不要解釋。'''\n\nresp=client.chat.completions.create(model='gpt-4o-mini', temperature=0.2,\n    messages=[{\"role\":\"system\",\"content\":\"你會產出可機器讀取且可執行的最佳化建議。\"},\n             {\"role\":\"user\",\"content\": plan_prompt},\n             {\"role\":\"user\",\"content\": json.dumps({\"topics\":samples}, ensure_ascii=False)}])\nraw=resp.choices[0].message.content\ntry:\n    plan=json.loads(raw)\nexcept Exception:\n    import re\n    plan=json.loads(re.search(r'\\{[\\s\\S]*\\}$', raw).group(0))\nplan"],"execution_count":null,"outputs":[]},
  {"cell_type":"code","metadata":{},"source":["# 依建議重跑（示範：參數+命名；合併/拆分可再擴充）\nfrom bertopic import BERTopic\n\np=plan.get('params', {})\nmin_cluster_size=int(p.get('min_cluster_size',30))\nmin_samples=int(p.get('min_samples',10))\nn_neighbors=int(p.get('n_neighbors',15))\nn_components=int(p.get('n_components',10))\n\numap2=UMAP(n_neighbors=n_neighbors, n_components=n_components, min_dist=0.0, metric='cosine', random_state=42)\nhs2=hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, metric='euclidean',\n                    cluster_selection_method='eom', prediction_data=True)\nmodel2=BERTopic(calculate_probabilities=True, verbose=True, umap_model=umap2, hdbscan_model=hs2)\n\ntop2, prob2 = model2.fit_transform(df['text'].astype(str).tolist(), embeddings=embeddings)\n\n# 命名\nif isinstance(plan.get('rename',{}), dict):\n    for k,v in {int(k):v for k,v in plan['rename'].items()}.items():\n        try: model2.set_topic_labels({k:v})\n        except: pass\n\ndf['topic_v2']=top2\ncoh2, sep2, sil2, out2 = metrics(embeddings, top2)\nprint('Before -> After')\nprint('一致性均值:', np.mean(list(coh.values())) if coh else np.nan, '->', np.mean(list(coh2.values())) if coh2 else np.nan)\nprint('區分度均值:', sep, '->', sep2)\nprint('Silhouette:', sil, '->', sil2)\nprint('Outlier 比例:', out, '->', out2)\n\nOUT=Path('/mnt/data/part3_optimized_bertopic_model'); model2.save(OUT.as_posix())\ndf.to_csv('/mnt/data/part3_corpus_with_topics_v2.csv', index=False, encoding='utf-8')\nprint('已輸出 part3_optimized_bertopic_model / part3_corpus_with_topics_v2.csv')"],"execution_count":null,"outputs":[]},
  {"cell_type":"code","metadata":{},"source":["# （選用）找出「模糊」文件 id（最近兩中心距離差小），可配合 LITA 只在這些文件上請 LLM 判讀\n\ndef ambiguous_indices(emb, topics, margin_th=0.02, cap=200):\n    c=centers(emb, topics)\n    ids=[]\n    if not c: return ids\n    C=np.vstack(list(c.values()))\n    for i,tid in enumerate(topics):\n        if tid==-1 or tid not in c: continue\n        d=cosine_distances(emb[i].reshape(1,-1), C).ravel()\n        s=np.sort(d)\n        if s[1]-s[0] < margin_th: ids.append(i)\n        if len(ids)>=cap: break\n    return ids\n\namb_ids=ambiguous_indices(embeddings, top2)\nlen(amb_ids), amb_ids[:10]"],"execution_count":null,"outputs":[]}
 ],
 "metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},
 "nbformat":4,"nbformat_minor":5
}
