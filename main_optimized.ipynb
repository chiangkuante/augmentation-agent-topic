{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "config_section",
   "metadata": {},
   "source": [
    "# 數位韌性指數計算系統 - 雙重優化版\n",
    "\n",
    "## 優化重點\n",
    "1. **集中配置管理**：所有關鍵參數集中在開頭\n",
    "2. **緩存機制**：避免重複計算和 API 調用\n",
    "3. **主題層級評分**：Phase 4 基於主題而非文本評分，大幅提升速度\n",
    "4. **多輪迭代優化**：Phase 3 支持多輪優化\n",
    "5. **關鍵詞增強提示**：LLM 映射時提供主題關鍵詞\n",
    "6. **批次 + 選擇性評分 (NEW!)**：一次 API 調用評分多個相關構面\n",
    "   - 減少 85% 的 API 調用次數 (819 → 117)\n",
    "   - 預計將 Phase 4 從 94 分鐘縮短到 5-10 分鐘\n",
    "   - 7-18x 加速比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 配置加載完成\n",
      "  - 嵌入模型: text-embedding-3-small\n",
      "  - LLM 模型: gpt-5-mini-2025-08-07\n",
      "  - 數據目錄: data\n",
      "  - Phase 3 最大迭代次數: 10\n",
      "  - Phase 3 智能停止: 啟用\n",
      "  - 評分構面: ITC, ACAP, DC, GOVSEC, DATA, ECO, OTHER\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 全局配置區 - 所有重要設置集中管理\n",
    "# ========================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# === API 配置 ===\n",
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\".env\")\n",
    "OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# === 模型配置 ===\n",
    "EMBEDDING_MODEL = 'text-embedding-3-small'  # OpenAI 嵌入模型\n",
    "LLM_MODEL = 'gpt-5-mini-2025-08-07'  # LLM 模型（用於主題優化和評分）\n",
    "#LLM_MODEL = 'gpt-4.1-mini-2025-04-14'  # LLM 模型（用於主題優化和評分）\n",
    "LLM_TEMPERATURE = 1  # 溫度參數\n",
    "\n",
    "# === 文件路徑配置 ===\n",
    "DATA_DIR = Path('data')\n",
    "CORPUS_PATH = DATA_DIR / 'corpus.csv'\n",
    "\n",
    "# Phase 2 輸出\n",
    "EMBEDDINGS_PATH = DATA_DIR / 'embeddings_text-3-small.npy'\n",
    "EMBEDDINGS_INDEX_PATH = DATA_DIR / 'embeddings_index.json'\n",
    "PHASE2_MODEL_DIR = DATA_DIR / 'part2_bertopic_model'\n",
    "PHASE2_TOPICS_CSV = DATA_DIR / 'part2_topics.csv'\n",
    "PHASE2_DOC_PROBS = DATA_DIR / 'part2_doc_topic_probs.npy'\n",
    "PHASE2_CORPUS_CSV = DATA_DIR / 'part2_corpus_with_topics.csv'\n",
    "PHASE2_TOPIC_YEAR_CSV = DATA_DIR / 'part2_topic_prop_by_year.csv'\n",
    "\n",
    "# Phase 3 輸出\n",
    "PHASE3_MODEL_DIR = DATA_DIR / 'part3_optimized_bertopic_model'\n",
    "PHASE3_CORPUS_CSV = DATA_DIR / 'part3_corpus_with_topics_v2.csv'\n",
    "PHASE3_OPTIMIZATION_CACHE = DATA_DIR / 'phase3_optimization_plans.json'\n",
    "\n",
    "# Phase 4 輸出\n",
    "PHASE4_TOPIC_DIM_MAP_CACHE = DATA_DIR / 'phase4_topic_dimension_map.json'\n",
    "PHASE4_TOPIC_SCORES_CACHE = DATA_DIR / 'phase4_topic_dimension_scores.json'\n",
    "PHASE4_DOC_SCORES_CSV = DATA_DIR / 'part4_doc_dimension_scores.csv'\n",
    "PHASE4_DRI_CSV = DATA_DIR / 'part4_entity_time_dri.csv'\n",
    "\n",
    "# === BERTopic 參數配置 ===\n",
    "# Phase 2 初始參數\n",
    "UMAP_N_NEIGHBORS = 15\n",
    "UMAP_N_COMPONENTS = 10\n",
    "UMAP_MIN_DIST = 0.0\n",
    "UMAP_METRIC = 'cosine'\n",
    "HDBSCAN_MIN_CLUSTER_SIZE = 30\n",
    "HDBSCAN_MIN_SAMPLES = 10\n",
    "HDBSCAN_METRIC = 'euclidean'\n",
    "HDBSCAN_SELECTION_METHOD = 'eom'\n",
    "\n",
    "# === Phase 3 優化配置 ===\n",
    "MAX_OPTIMIZATION_ITERATIONS = 10  # 最大迭代次數（防止無限循環）\n",
    "TOPIC_SAMPLE_SIZE = 3  # 每個主題採樣的例句數量\n",
    "ENABLE_SMART_STOPPING = True  # 啟用基於 LLM 的智能停止判斷\n",
    "\n",
    "# === Phase 4 評分配置 ===\n",
    "DIMENSIONS = [\"ITC\", \"ACAP\", \"DC\", \"GOVSEC\", \"DATA\", \"ECO\", \"OTHER\"]  # 數位韌性構面\n",
    "DIMENSION_WEIGHTS = {  # 各構面權重（總和為 1）\n",
    "    \"ITC\": 0.20,\n",
    "    \"ACAP\": 0.20,\n",
    "    \"DC\": 0.15,\n",
    "    \"GOVSEC\": 0.15,\n",
    "    \"DATA\": 0.15,\n",
    "    \"ECO\": 0.15,\n",
    "    \"OTHER\": 0.0\n",
    "}\n",
    "\n",
    "# 構面語義分組（用於選擇性評分優化）\n",
    "DIMENSION_GROUPS = {\n",
    "    \"ITC\": [\"ITC\", \"ACAP\", \"DC\"],  # 技術基礎設施相關\n",
    "    \"ACAP\": [\"ACAP\", \"ITC\", \"GOVSEC\"],  # 安全與治理相關\n",
    "    \"DC\": [\"DC\", \"ITC\", \"GOVSEC\"],  # 基礎設施與連續性\n",
    "    \"GOVSEC\": [\"GOVSEC\", \"ACAP\", \"DATA\"],  # 治理與合規\n",
    "    \"DATA\": [\"DATA\", \"GOVSEC\", \"ECO\"],  # 數據與生態系統\n",
    "    \"ECO\": [\"ECO\", \"DATA\", \"ITC\"],  # 數位生態系統\n",
    "    \"OTHER\": [\"OTHER\"]  # 其他類別\n",
    "}\n",
    "\n",
    "# 評分規則\n",
    "SCORING_RUBRIC = (\n",
    "    \"Rate the substantiveness and strength on a 0–5 scale:\\n\"\n",
    "    \"0 = irrelevant/very vague\\n\"\n",
    "    \"3 = part of a specific action or quantitative indicator\\n\"\n",
    "    \"5 = clear, quantitative, auditable, and directly related to strategy/investment/institutionalization\"\n",
    ")\n",
    "\n",
    "# === 其他配置 ===\n",
    "EMBEDDING_BATCH_SIZE = 256\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(\"✓ 配置加載完成\")\n",
    "print(f\"  - 嵌入模型: {EMBEDDING_MODEL}\")\n",
    "print(f\"  - LLM 模型: {LLM_MODEL}\")\n",
    "print(f\"  - 數據目錄: {DATA_DIR}\")\n",
    "print(f\"  - Phase 3 最大迭代次數: {MAX_OPTIMIZATION_ITERATIONS}\")\n",
    "print(f\"  - Phase 3 智能停止: {'啟用' if ENABLE_SMART_STOPPING else '禁用'}\")\n",
    "print(f\"  - 評分構面: {', '.join(DIMENSIONS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00_imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 庫導入完成\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 導入必要的庫\n",
    "# ========================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# OpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "# BERTopic and dependencies\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "\n",
    "# 初始化 OpenAI 客戶端\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# 確保數據目錄存在\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"✓ 庫導入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase0",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 0: 數據下載與預處理\n",
    "（此部分保持不變，如需重新下載請取消註釋）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "phase0_download",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data_download import download_sec_filings\n",
    "# from src.data_processing import process_sec_filings\n",
    "\n",
    "# download_sec_filings()\n",
    "# process_sec_filings(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 2: 初始主題生成（BERTopic）\n",
    "\n",
    "- 嵌入模型：OpenAI text-embedding-3-small (1536維)\n",
    "- 降維：UMAP，聚類：HDBSCAN\n",
    "- **緩存機制**：嵌入向量緩存，避免重複計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "phase2_load_corpus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 載入語料庫: 6233 筆文檔\n",
      "  - 元數據欄位: ['ticker', 'year']\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 加載語料庫\n",
    "# ========================================\n",
    "\n",
    "assert CORPUS_PATH.exists(), f\"找不到語料庫: {CORPUS_PATH}\"\n",
    "\n",
    "df = pd.read_csv(CORPUS_PATH)\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "assert 'text' in df.columns, \"語料需要包含 'text' 欄位\"\n",
    "\n",
    "# 偵測元數據欄位\n",
    "meta_cols = [c for c in ['doc_id', 'company', 'firm', 'ticker', 'year', 'date'] if c in df.columns]\n",
    "print(f\"✓ 載入語料庫: {len(df)} 筆文檔\")\n",
    "print(f\"  - 元數據欄位: {meta_cols or '(無)'}\")\n",
    "\n",
    "texts = df['text'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "phase2_embeddings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 從緩存加載嵌入向量...\n",
      "  - 模型: text-embedding-3-small\n",
      "  - 數量: 6233\n",
      "  - 嵌入形狀: (6233, 1536)\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 生成/載入嵌入向量（帶緩存）\n",
    "# ========================================\n",
    "\n",
    "if EMBEDDINGS_PATH.exists() and EMBEDDINGS_INDEX_PATH.exists():\n",
    "    print(\"✓ 從緩存加載嵌入向量...\")\n",
    "    embeddings = np.load(EMBEDDINGS_PATH)\n",
    "    with open(EMBEDDINGS_INDEX_PATH, 'r') as f:\n",
    "        emb_info = json.load(f)\n",
    "    print(f\"  - 模型: {emb_info.get('model')}\")\n",
    "    print(f\"  - 數量: {emb_info.get('count')}\")\n",
    "else:\n",
    "    print(\"⚙ 生成嵌入向量（這可能需要幾分鐘）...\")\n",
    "    vecs = []\n",
    "    for i in tqdm(range(0, len(texts), EMBEDDING_BATCH_SIZE), desc=\"生成嵌入\"):\n",
    "        batch = texts[i:i + EMBEDDING_BATCH_SIZE]\n",
    "        response = client.embeddings.create(model=EMBEDDING_MODEL, input=batch)\n",
    "        vecs.extend([np.array(d.embedding, dtype=np.float32) for d in response.data])\n",
    "    \n",
    "    embeddings = np.vstack(vecs)\n",
    "    np.save(EMBEDDINGS_PATH, embeddings)\n",
    "    \n",
    "    with open(EMBEDDINGS_INDEX_PATH, 'w') as f:\n",
    "        json.dump({'count': len(texts), 'model': EMBEDDING_MODEL}, f)\n",
    "    \n",
    "    print(f\"✓ 嵌入向量已保存\")\n",
    "\n",
    "print(f\"  - 嵌入形狀: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "phase2_bertopic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 22:20:17,802 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙ 訓練 BERTopic 模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 22:20:37,334 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-14 22:20:37,335 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-10-14 22:20:38,220 - BERTopic - Cluster - Completed ✓\n",
      "2025-10-14 22:20:38,223 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-10-14 22:20:39,247 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase 2 完成\n",
      "  - 主題數量: 68\n",
      "  - 離群點比例: 16.52%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1030</td>\n",
       "      <td>-1_our_and_of_the</td>\n",
       "      <td>[our, and, of, the, to, or, in, we, are, for]</td>\n",
       "      <td>[There can be no assurance that licenses will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>302</td>\n",
       "      <td>0_tax_income_taxes_deferred</td>\n",
       "      <td>[tax, income, taxes, deferred, foreign, rate, ...</td>\n",
       "      <td>[We are subject to income taxes in the U.S. an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>1_loans_loan_credit_portfolio</td>\n",
       "      <td>[loans, loan, credit, portfolio, consumer, all...</td>\n",
       "      <td>[858\\r\\nmillion\\r\\nwere included in TDRs at\\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>191</td>\n",
       "      <td>2_gas_reserves_oil_proved</td>\n",
       "      <td>[gas, reserves, oil, proved, production, exxon...</td>\n",
       "      <td>[In\\r\\n\\r\\nIn some cases, substantial new inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>136</td>\n",
       "      <td>3_our_we_may_could</td>\n",
       "      <td>[our, we, may, could, or, products, and, to, b...</td>\n",
       "      <td>[Litigation and regulatory proceedings are inh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>4_care_health_medicare_medical</td>\n",
       "      <td>[care, health, medicare, medical, unitedhealth...</td>\n",
       "      <td>[UnitedHealthcare Medicare &amp; Retirement provid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>125</td>\n",
       "      <td>5_goodwill_assets_impairment_intangible</td>\n",
       "      <td>[goodwill, assets, impairment, intangible, val...</td>\n",
       "      <td>[Property and equipment, which includes amount...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>123</td>\n",
       "      <td>6_driven_banking_higher_income</td>\n",
       "      <td>[driven, banking, higher, income, fees, billio...</td>\n",
       "      <td>[The operating margin was\\r\\n27\\r\\npercent com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>117</td>\n",
       "      <td>7_pension_plans_plan_benefit</td>\n",
       "      <td>[pension, plans, plan, benefit, assets, postre...</td>\n",
       "      <td>[. Note 10—Pension and Other Postretirement Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>113</td>\n",
       "      <td>8_capital_basel_reserve_bank</td>\n",
       "      <td>[capital, basel, reserve, bank, federal, requi...</td>\n",
       "      <td>[banks are subject to quantitative and qualita...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                     Name  \\\n",
       "0     -1   1030                        -1_our_and_of_the   \n",
       "1      0    302              0_tax_income_taxes_deferred   \n",
       "2      1    222            1_loans_loan_credit_portfolio   \n",
       "3      2    191                2_gas_reserves_oil_proved   \n",
       "4      3    136                       3_our_we_may_could   \n",
       "5      4    128           4_care_health_medicare_medical   \n",
       "6      5    125  5_goodwill_assets_impairment_intangible   \n",
       "7      6    123           6_driven_banking_higher_income   \n",
       "8      7    117             7_pension_plans_plan_benefit   \n",
       "9      8    113             8_capital_basel_reserve_bank   \n",
       "\n",
       "                                      Representation  \\\n",
       "0      [our, and, of, the, to, or, in, we, are, for]   \n",
       "1  [tax, income, taxes, deferred, foreign, rate, ...   \n",
       "2  [loans, loan, credit, portfolio, consumer, all...   \n",
       "3  [gas, reserves, oil, proved, production, exxon...   \n",
       "4  [our, we, may, could, or, products, and, to, b...   \n",
       "5  [care, health, medicare, medical, unitedhealth...   \n",
       "6  [goodwill, assets, impairment, intangible, val...   \n",
       "7  [driven, banking, higher, income, fees, billio...   \n",
       "8  [pension, plans, plan, benefit, assets, postre...   \n",
       "9  [capital, basel, reserve, bank, federal, requi...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [There can be no assurance that licenses will ...  \n",
       "1  [We are subject to income taxes in the U.S. an...  \n",
       "2  [858\\r\\nmillion\\r\\nwere included in TDRs at\\r\\...  \n",
       "3  [In\\r\\n\\r\\nIn some cases, substantial new inve...  \n",
       "4  [Litigation and regulatory proceedings are inh...  \n",
       "5  [UnitedHealthcare Medicare & Retirement provid...  \n",
       "6  [Property and equipment, which includes amount...  \n",
       "7  [The operating margin was\\r\\n27\\r\\npercent com...  \n",
       "8  [. Note 10—Pension and Other Postretirement Be...  \n",
       "9  [banks are subject to quantitative and qualita...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================================\n",
    "# 初始 BERTopic 模型訓練\n",
    "# ========================================\n",
    "\n",
    "print(\"⚙ 訓練 BERTopic 模型...\")\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=UMAP_N_NEIGHBORS,\n",
    "    n_components=UMAP_N_COMPONENTS,\n",
    "    min_dist=UMAP_MIN_DIST,\n",
    "    metric=UMAP_METRIC,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "hdbscan_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE,\n",
    "    min_samples=HDBSCAN_MIN_SAMPLES,\n",
    "    metric=HDBSCAN_METRIC,\n",
    "    cluster_selection_method=HDBSCAN_SELECTION_METHOD,\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(texts, embeddings=embeddings)\n",
    "df['topic'] = topics\n",
    "\n",
    "# 保存結果\n",
    "topic_model.save(PHASE2_MODEL_DIR.as_posix(), serialization=\"safetensors\")\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info.to_csv(PHASE2_TOPICS_CSV, index=False, encoding='utf-8')\n",
    "if probs is not None:\n",
    "    np.save(PHASE2_DOC_PROBS, probs)\n",
    "df.to_csv(PHASE2_CORPUS_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "# 按年度分析主題分佈\n",
    "if 'year' in df.columns:\n",
    "    year_dist = df.groupby('year')['topic'].value_counts(normalize=True).rename('prop').reset_index()\n",
    "    year_dist.to_csv(PHASE2_TOPIC_YEAR_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"✓ Phase 2 完成\")\n",
    "print(f\"  - 主題數量: {len(topic_info[topic_info['Topic'] != -1])}\")\n",
    "print(f\"  - 離群點比例: {(np.array(topics) == -1).mean():.2%}\")\n",
    "topic_info.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 3: LLM 迭代優化（完整實現版）\n",
    "\n",
    "**核心功能（完整實現所有 LLM 建議）**：\n",
    "\n",
    "1. **✓ 主題合併 (merge_pairs)**：\n",
    "   - 合併語義相似或有父子關係的主題\n",
    "   - 直接更新文檔的主題分配\n",
    "   - 示例：將 Topic 6 和 Topic 24 合併\n",
    "\n",
    "2. **✓ 主題拆分 (split_topics)**：\n",
    "   - 拆分過於寬泛、包含多個概念的主題\n",
    "   - 使用子聚類算法重新分組\n",
    "   - 示例：將包含「員工培訓」和「數據隱私」的主題拆分\n",
    "\n",
    "3. **✓ 停用詞管理 (new_stopwords)**：\n",
    "   - 動態添加領域特定的噪音詞\n",
    "   - 重新計算主題表示（移除無意義詞）\n",
    "   - 示例：過濾「company」、「billion」、「fiscal」等詞\n",
    "\n",
    "4. **✓ 參數調整 (params)**：\n",
    "   - 根據 LLM 建議調整 HDBSCAN/UMAP 參數\n",
    "   - 重新訓練整個聚類模型\n",
    "   - 目標：減少離群點、提升主題品質\n",
    "\n",
    "5. **✓ 主題重命名 (rename)**：\n",
    "   - 將關鍵詞列表改為有意義的主題名稱\n",
    "   - 提升可解釋性\n",
    "   - 示例：「tax, income, deferred」→「Corporate Tax Strategy & Deferred Assets」\n",
    "\n",
    "6. **✓ 智能停止判斷**：\n",
    "   - LLM 基於指標歷史自動決定何時停止迭代\n",
    "   - 收斂檢測、退化檢測、持續改進判斷\n",
    "\n",
    "**執行順序**：\n",
    "1. 合併相似主題 → 2. 拆分過寬主題 → 3. 更新停用詞 → 4. 調整參數重訓練 → 5. 重命名 → 6. 評估是否繼續\n",
    "\n",
    "**預期效果**：\n",
    "- 完整應用 LLM 的所有優化建議\n",
    "- 每輪迭代顯示實際應用的操作（合併幾對、拆分幾個、停用詞幾個等）\n",
    "- 智能收斂：平均 2-4 輪即可達到最佳配置\n",
    "- 顯著提升主題質量和可解釋性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "phase3_utils",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 工具函數就緒\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 工具函數：指標計算\n",
    "# ========================================\n",
    "\n",
    "def compute_topic_centers(emb: np.ndarray, topics: List[int]) -> Dict[int, np.ndarray]:\n",
    "    \"\"\"計算每個主題的中心向量\"\"\"\n",
    "    centers = {}\n",
    "    s = pd.Series(topics)\n",
    "    for tid, idxs in s.groupby(s).groups.items():\n",
    "        if tid == -1:\n",
    "            continue\n",
    "        vecs = emb[list(idxs)]\n",
    "        centers[tid] = normalize(vecs.mean(axis=0, keepdims=True))[0]\n",
    "    return centers\n",
    "\n",
    "def compute_metrics(emb: np.ndarray, topics: List[int]) -> Tuple[Dict, float, float, float]:\n",
    "    \"\"\"計算主題質量指標：一致性、區分度、輪廓係數、離群率\"\"\"\n",
    "    centers = compute_topic_centers(emb, topics)\n",
    "    \n",
    "    # 一致性（Cohesion）：每個主題內部的平均相似度\n",
    "    s = pd.Series(topics)\n",
    "    cohesion = {}\n",
    "    for tid, idxs in s.groupby(s).groups.items():\n",
    "        if tid == -1 or tid not in centers:\n",
    "            continue\n",
    "        sims = cosine_similarity(emb[list(idxs)], centers[tid].reshape(1, -1)).ravel()\n",
    "        cohesion[tid] = float(np.mean(sims))\n",
    "    \n",
    "    # 區分度（Separation）：主題中心之間的平均距離\n",
    "    separation = np.nan\n",
    "    if len(centers) >= 2:\n",
    "        center_matrix = np.vstack(list(centers.values()))\n",
    "        separation = cosine_distances(center_matrix).mean()\n",
    "    \n",
    "    # 輪廓係數（Silhouette）\n",
    "    mask = np.array(topics) != -1\n",
    "    silhouette = np.nan\n",
    "    if mask.sum() > 5 and len(set(np.array(topics)[mask])) > 1:\n",
    "        silhouette = silhouette_score(emb[mask], np.array(topics)[mask])\n",
    "    \n",
    "    # 離群率\n",
    "    outlier_rate = (np.array(topics) == -1).mean()\n",
    "    \n",
    "    return cohesion, separation, silhouette, outlier_rate\n",
    "\n",
    "def print_metrics(cohesion: Dict, separation: float, silhouette: float, outlier: float, prefix=\"\"):\n",
    "    \"\"\"打印指標\"\"\"\n",
    "    coh_mean = np.mean(list(cohesion.values())) if cohesion else np.nan\n",
    "    print(f\"{prefix}一致性: {coh_mean:.4f}\")\n",
    "    print(f\"{prefix}區分度: {separation:.4f}\")\n",
    "    print(f\"{prefix}Silhouette: {silhouette:.4f}\")\n",
    "    print(f\"{prefix}離群率: {outlier:.2%}\")\n",
    "\n",
    "print(\"✓ 工具函數就緒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "g4u50c52fhp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase 3 優化操作函數就緒\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Phase 3 優化操作函數\n",
    "# ========================================\n",
    "\n",
    "def merge_topics(model, topics_list: List[int], merge_pairs: List[List[int]]) -> List[int]:\n",
    "    \"\"\"\n",
    "    合併主題對\n",
    "    \n",
    "    Args:\n",
    "        model: BERTopic 模型\n",
    "        topics_list: 當前文檔的主題分配列表\n",
    "        merge_pairs: 要合併的主題對 [[source, target], ...]\n",
    "    \n",
    "    Returns:\n",
    "        更新後的主題列表\n",
    "    \"\"\"\n",
    "    if not merge_pairs:\n",
    "        return topics_list\n",
    "    \n",
    "    topics_array = np.array(topics_list)\n",
    "    merge_count = 0\n",
    "    \n",
    "    for pair in merge_pairs:\n",
    "        if len(pair) != 2:\n",
    "            continue\n",
    "        source, target = int(pair[0]), int(pair[1])\n",
    "        \n",
    "        # 將 source 主題的所有文檔重新分配到 target 主題\n",
    "        mask = topics_array == source\n",
    "        if mask.sum() > 0:\n",
    "            topics_array[mask] = target\n",
    "            merge_count += 1\n",
    "            print(f\"    ✓ 合併 Topic {source} → Topic {target} ({mask.sum()} 文檔)\")\n",
    "    \n",
    "    print(f\"  - 完成 {merge_count} 個主題合併\")\n",
    "    return topics_array.tolist()\n",
    "\n",
    "\n",
    "def split_topic(model, df, embeddings, topic_id: int, topic_col: str = 'topic') -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"\n",
    "    拆分指定主題為多個子主題\n",
    "    \n",
    "    Args:\n",
    "        model: BERTopic 模型\n",
    "        df: 文檔 DataFrame\n",
    "        embeddings: 文檔嵌入向量\n",
    "        topic_id: 要拆分的主題 ID\n",
    "        topic_col: 主題欄位名稱\n",
    "    \n",
    "    Returns:\n",
    "        更新後的 DataFrame 和主題列表\n",
    "    \"\"\"\n",
    "    # 獲取該主題的所有文檔\n",
    "    topic_mask = df[topic_col] == topic_id\n",
    "    topic_indices = df[topic_mask].index.tolist()\n",
    "    \n",
    "    if len(topic_indices) < 10:  # 太小的主題不拆分\n",
    "        print(f\"    ⚠ Topic {topic_id} 文檔數太少 ({len(topic_indices)})，跳過拆分\")\n",
    "        return df, df[topic_col].values\n",
    "    \n",
    "    # 提取該主題的嵌入向量\n",
    "    topic_embeddings = embeddings[topic_indices]\n",
    "    \n",
    "    # 對該主題進行子聚類（使用更小的 min_cluster_size）\n",
    "    sub_min_cluster = max(5, len(topic_indices) // 4)\n",
    "    \n",
    "    sub_hdbscan = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=sub_min_cluster,\n",
    "        min_samples=5,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom',\n",
    "        prediction_data=True\n",
    "    )\n",
    "    \n",
    "    sub_labels = sub_hdbscan.fit_predict(topic_embeddings)\n",
    "    \n",
    "    # 找到最大的主題 ID，用於生成新 ID\n",
    "    max_topic_id = int(df[topic_col].max())\n",
    "    unique_sub_labels = set(sub_labels[sub_labels != -1])\n",
    "    \n",
    "    if len(unique_sub_labels) <= 1:\n",
    "        print(f\"    ⚠ Topic {topic_id} 無法進一步拆分\")\n",
    "        return df, df[topic_col].values\n",
    "    \n",
    "    # 創建新的主題 ID 映射\n",
    "    new_topic_map = {}\n",
    "    for i, sub_label in enumerate(sorted(unique_sub_labels)):\n",
    "        if i == 0:\n",
    "            # 第一個子主題保持原 ID\n",
    "            new_topic_map[sub_label] = topic_id\n",
    "        else:\n",
    "            # 其他子主題使用新 ID\n",
    "            max_topic_id += 1\n",
    "            new_topic_map[sub_label] = max_topic_id\n",
    "    \n",
    "    # 更新主題分配\n",
    "    topics_array = df[topic_col].values.copy()\n",
    "    for idx, sub_label in zip(topic_indices, sub_labels):\n",
    "        if sub_label in new_topic_map:\n",
    "            topics_array[idx] = new_topic_map[sub_label]\n",
    "    \n",
    "    df[topic_col] = topics_array\n",
    "    \n",
    "    print(f\"    ✓ 拆分 Topic {topic_id} → {len(unique_sub_labels)} 個子主題\")\n",
    "    for sub_label, new_id in new_topic_map.items():\n",
    "        count = (sub_labels == sub_label).sum()\n",
    "        print(f\"      - Topic {new_id}: {count} 文檔\")\n",
    "    \n",
    "    return df, topics_array\n",
    "\n",
    "\n",
    "def update_stopwords_and_representation(model, texts: List[str], topics: List[int], \n",
    "                                       embeddings, new_stopwords: List[str] = None):\n",
    "    \"\"\"\n",
    "    更新停用詞並重新計算主題表示\n",
    "    \n",
    "    Args:\n",
    "        model: BERTopic 模型\n",
    "        texts: 文檔文本列表\n",
    "        topics: 主題分配列表\n",
    "        embeddings: 文檔嵌入向量\n",
    "        new_stopwords: 新增的停用詞列表\n",
    "    \n",
    "    Returns:\n",
    "        更新後的模型\n",
    "    \"\"\"\n",
    "    if not new_stopwords:\n",
    "        return model\n",
    "    \n",
    "    print(f\"  - 新增 {len(new_stopwords)} 個停用詞: {new_stopwords}\")\n",
    "    \n",
    "    # BERTopic 使用 CountVectorizer，我們需要更新它的停用詞\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    # 創建新的 vectorizer with updated stopwords\n",
    "    current_stopwords = set()\n",
    "    if hasattr(model, 'vectorizer_model') and model.vectorizer_model is not None:\n",
    "        if hasattr(model.vectorizer_model, 'stop_words_'):\n",
    "            current_stopwords = set(model.vectorizer_model.stop_words_)\n",
    "    \n",
    "    # 添加新停用詞\n",
    "    updated_stopwords = current_stopwords.union(set(new_stopwords))\n",
    "    \n",
    "    # 創建新的 vectorizer\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        stop_words=list(updated_stopwords),\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=5\n",
    "    )\n",
    "    \n",
    "    # 更新模型的 vectorizer\n",
    "    model.vectorizer_model = vectorizer_model\n",
    "    \n",
    "    # 重新計算主題表示\n",
    "    try:\n",
    "        model.update_topics(texts, topics=topics, vectorizer_model=vectorizer_model)\n",
    "        print(f\"  - 已更新主題表示（移除停用詞）\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ 更新主題表示時出錯: {e}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"✓ Phase 3 優化操作函數就緒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "phase3_optimize",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 22:20:40,612 - BERTopic - WARNING: You are loading a BERTopic model without explicitly defining an embedding model. If you want to also load in an embedding model, make sure to use `BERTopic.load(my_model, embedding_model=my_embedding_model)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙ 開始智能迭代優化（最多 10 輪）...\n",
      "\n",
      "✓ 完整實現版：支持合併、拆分、停用詞、參數調整、重命名\n",
      "\n",
      "初始指標:\n",
      "  一致性: 0.7721\n",
      "  區分度: 0.4064\n",
      "  Silhouette: 0.0771\n",
      "  離群率: 16.52%\n",
      "\n",
      "==================================================\n",
      "第 1 輪優化\n",
      "==================================================\n",
      "⚙ 請求 LLM 優化建議...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 22:24:33,357 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - 合併建議: 32 對\n",
      "  - 拆分建議: 3 個主題\n",
      "  - 停用詞建議: 31 個詞\n",
      "  - 參數調整: {'min_cluster_size': 30, 'min_samples': 5, 'umap_n_neighbors': 15, 'umap_min_dist': 0.1}\n",
      "  - 重命名: 68 個主題\n",
      "\n",
      "⚙ 應用主題合併...\n",
      "    ✓ 合併 Topic 8 → Topic 31 (113 文檔)\n",
      "    ✓ 合併 Topic 35 → Topic 16 (67 文檔)\n",
      "    ✓ 合併 Topic 13 → Topic 27 (105 文檔)\n",
      "    ✓ 合併 Topic 50 → Topic 37 (43 文檔)\n",
      "    ✓ 合併 Topic 10 → Topic 14 (107 文檔)\n",
      "    ✓ 合併 Topic 19 → Topic 33 (94 文檔)\n",
      "    ✓ 合併 Topic 66 → Topic 3 (30 文檔)\n",
      "    ✓ 合併 Topic 12 → Topic 41 (105 文檔)\n",
      "    ✓ 合併 Topic 29 → Topic 42 (76 文檔)\n",
      "    ✓ 合併 Topic 65 → Topic 18 (32 文檔)\n",
      "    ✓ 合併 Topic 30 → Topic 54 (75 文檔)\n",
      "    ✓ 合併 Topic 37 → Topic 5 (104 文檔)\n",
      "    ✓ 合併 Topic 58 → Topic 36 (37 文檔)\n",
      "    ✓ 合併 Topic 60 → Topic 28 (37 文檔)\n",
      "  - 完成 14 個主題合併\n",
      "\n",
      "⚙ 應用主題拆分...\n",
      "    ⚠ Topic 3 無法進一步拆分\n",
      "    ⚠ Topic 13 文檔數太少 (0)，跳過拆分\n",
      "    ⚠ Topic 22 無法進一步拆分\n",
      "\n",
      "⚙ 更新停用詞...\n",
      "  - 新增 31 個停用詞: ['company', 'companys', 'corporation', 'inc', 'incorporated', 'registrant', 'form', 'document', 'proxy', 'filed', 'report', 'item', 'note', 'table', 'page', 'see', 'billion', 'million', 'percent', 'fiscal', 'year', 'years', 'us', 'non-us', 'the', 'our', 'we', 'may', 'could', 'would', '0000070858']\n",
      "  - 已更新主題表示（移除停用詞）\n",
      "\n",
      "⚙ 根據參數建議重新訓練模型...\n",
      "\n",
      "⚙ 應用主題重命名...\n",
      "  - 完成 68 個主題重命名\n",
      "\n",
      "結果對比:\n",
      "  舊: 一致性: 0.7721\n",
      "  舊: 區分度: 0.4064\n",
      "  舊: Silhouette: 0.0771\n",
      "  舊: 離群率: 16.52%\n",
      "  新: 一致性: 0.7715\n",
      "  新: 區分度: 0.4042\n",
      "  新: Silhouette: 0.0695\n",
      "  新: 離群率: 13.72%\n",
      "\n",
      "✓ 第 1 輪完成\n",
      "\n",
      "==================================================\n",
      "第 2 輪優化\n",
      "==================================================\n",
      "⚙ 請求 LLM 優化建議...\n",
      "  - 合併建議: 24 對\n",
      "  - 拆分建議: 10 個主題\n",
      "  - 停用詞建議: 30 個詞\n",
      "  - 參數調整: {'min_cluster_size': 25, 'min_samples': 5}\n",
      "  - 重命名: 72 個主題\n",
      "\n",
      "⚙ 應用主題合併...\n",
      "    ✓ 合併 Topic 24 → Topic 50 (88 文檔)\n",
      "    ✓ 合併 Topic 2 → Topic 33 (134 文檔)\n",
      "    ✓ 合併 Topic 33 → Topic 40 (209 文檔)\n",
      "    ✓ 合併 Topic 40 → Topic 55 (271 文檔)\n",
      "    ✓ 合併 Topic 14 → Topic 22 (103 文檔)\n",
      "    ✓ 合併 Topic 22 → Topic 38 (193 文檔)\n",
      "    ✓ 合併 Topic 3 → Topic 15 (129 文檔)\n",
      "    ✓ 合併 Topic 15 → Topic 35 (232 文檔)\n",
      "    ✓ 合併 Topic 35 → Topic 48 (306 文檔)\n",
      "    ✓ 合併 Topic 48 → Topic 31 (353 文檔)\n",
      "    ✓ 合併 Topic 31 → Topic 30 (431 文檔)\n",
      "    ✓ 合併 Topic 7 → Topic 3 (116 文檔)\n",
      "    ✓ 合併 Topic 0 → Topic 69 (250 文檔)\n",
      "    ✓ 合併 Topic 6 → Topic 31 (117 文檔)\n",
      "    ✓ 合併 Topic 58 → Topic 60 (41 文檔)\n",
      "    ✓ 合併 Topic 47 → Topic 66 (49 文檔)\n",
      "    ✓ 合併 Topic 12 → Topic 45 (107 文檔)\n",
      "    ✓ 合併 Topic 59 → Topic 20 (40 文檔)\n",
      "    ✓ 合併 Topic 28 → Topic 65 (81 文檔)\n",
      "  - 完成 19 個主題合併\n",
      "\n",
      "⚙ 應用主題拆分...\n",
      "    ⚠ Topic 1 無法進一步拆分\n",
      "    ⚠ Topic 16 無法進一步拆分\n",
      "    ⚠ Topic 23 無法進一步拆分\n",
      "    ⚠ Topic 26 無法進一步拆分\n",
      "    ⚠ Topic 61 無法進一步拆分\n",
      "    ⚠ Topic 68 無法進一步拆分\n",
      "    ⚠ Topic 63 無法進一步拆分\n",
      "    ⚠ Topic 70 無法進一步拆分\n",
      "    ⚠ Topic 36 無法進一步拆分\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 22:27:06,131 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ⚠ Topic 8 無法進一步拆分\n",
      "\n",
      "⚙ 更新停用詞...\n",
      "  - 新增 30 個停用詞: ['company', 'companies', 'corporation', 'corporations', 'companys', 'report', 'reports', 'fiscal', 'billion', 'million', 'percent', 'the', 'our', 'we', 'may', 'could', 'or', 'of', 'and', 'to', 'in', 'results', 'item', 'page', 'note', 'inc', 'inc.', 'llc', 'ltd', 'www']\n",
      "  - 已更新主題表示（移除停用詞）\n",
      "\n",
      "⚙ 根據參數建議重新訓練模型...\n",
      "\n",
      "⚙ 應用主題重命名...\n",
      "  - 完成 72 個主題重命名\n",
      "\n",
      "結果對比:\n",
      "  舊: 一致性: 0.7715\n",
      "  舊: 區分度: 0.4042\n",
      "  舊: Silhouette: 0.0695\n",
      "  舊: 離群率: 13.72%\n",
      "  新: 一致性: 0.7751\n",
      "  新: 區分度: 0.4082\n",
      "  新: Silhouette: 0.0716\n",
      "  新: 離群率: 14.73%\n",
      "\n",
      "⚙ 評估是否繼續優化...\n",
      "\n",
      "  決策: CONTINUE\n",
      "  理由: Metrics not yet converged and show mixed improvement: cohesion +0.47%, separation +0.99%, silhouette +2.94% (improved) while outlier_rate worsened +7.37%. Not consistent degradation — continue and focus on reducing outlier rate.\n",
      "\n",
      "✓ 第 2 輪完成\n",
      "\n",
      "==================================================\n",
      "第 3 輪優化\n",
      "==================================================\n",
      "⚙ 請求 LLM 優化建議...\n",
      "  - 合併建議: 15 對\n",
      "  - 拆分建議: 9 個主題\n",
      "  - 停用詞建議: 31 個詞\n",
      "  - 參數調整: {'min_cluster_size': 30, 'min_samples': 5, 'umap_n_neighbors': 15, 'umap_min_dist': 0.1}\n",
      "  - 重命名: 77 個主題\n",
      "\n",
      "⚙ 應用主題合併...\n",
      "    ✓ 合併 Topic 21 → Topic 36 (88 文檔)\n",
      "    ✓ 合併 Topic 27 → Topic 50 (78 文檔)\n",
      "    ✓ 合併 Topic 11 → Topic 33 (103 文檔)\n",
      "    ✓ 合併 Topic 19 → Topic 34 (90 文檔)\n",
      "    ✓ 合併 Topic 37 → Topic 55 (62 文檔)\n",
      "    ✓ 合併 Topic 54 → Topic 43 (43 文檔)\n",
      "    ✓ 合併 Topic 0 → Topic 70 (250 文檔)\n",
      "    ✓ 合併 Topic 4 → Topic 75 (118 文檔)\n",
      "  - 完成 8 個主題合併\n",
      "\n",
      "⚙ 應用主題拆分...\n",
      "    ⚠ Topic 1 無法進一步拆分\n",
      "    ⚠ Topic 12 無法進一步拆分\n",
      "    ⚠ Topic 13 無法進一步拆分\n",
      "    ⚠ Topic 20 無法進一步拆分\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 22:31:12,646 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ⚠ Topic 24 無法進一步拆分\n",
      "    ⚠ Topic 31 無法進一步拆分\n",
      "    ⚠ Topic 62 無法進一步拆分\n",
      "    ⚠ Topic 61 無法進一步拆分\n",
      "    ⚠ Topic 63 無法進一步拆分\n",
      "\n",
      "⚙ 更新停用詞...\n",
      "  - 新增 31 個停用詞: ['company', 'corporation', 'companys', 'our', 'we', 'may', 'could', 'or', 'the', 'and', 'to', 'of', 'billion', 'million', 'fiscal', 'report', 'statement', 'statements', 'page', 'table', 'note', 'item', 'percent', 'year', 'years', 'registrant', 'file', 'form', 'inc', 'inc.', 'us']\n",
      "  - 已更新主題表示（移除停用詞）\n",
      "\n",
      "⚙ 根據參數建議重新訓練模型...\n",
      "\n",
      "⚙ 應用主題重命名...\n",
      "  - 完成 77 個主題重命名\n",
      "\n",
      "結果對比:\n",
      "  舊: 一致性: 0.7751\n",
      "  舊: 區分度: 0.4082\n",
      "  舊: Silhouette: 0.0716\n",
      "  舊: 離群率: 14.73%\n",
      "  新: 一致性: 0.7715\n",
      "  新: 區分度: 0.4042\n",
      "  新: Silhouette: 0.0695\n",
      "  新: 離群率: 13.72%\n",
      "\n",
      "⚙ 評估是否繼續優化...\n",
      "\n",
      "  決策: CONTINUE\n",
      "  理由: Metrics have not converged (no two consecutive iterations with <1% change) and are oscillating rather than degrading; some metrics improved in iter 2, so continue optimization to seek stable gains (particularly to reduce outlier rate).\n",
      "\n",
      "✓ 第 3 輪完成\n",
      "\n",
      "==================================================\n",
      "第 4 輪優化\n",
      "==================================================\n",
      "⚙ 請求 LLM 優化建議...\n",
      "  - 合併建議: 17 對\n",
      "  - 拆分建議: 8 個主題\n",
      "  - 停用詞建議: 44 個詞\n",
      "  - 參數調整: {'min_cluster_size': 25, 'min_samples': 3}\n",
      "  - 重命名: 42 個主題\n",
      "\n",
      "⚙ 應用主題合併...\n",
      "    ✓ 合併 Topic 0 → Topic 69 (250 文檔)\n",
      "    ✓ 合併 Topic 2 → Topic 33 (134 文檔)\n",
      "    ✓ 合併 Topic 3 → Topic 22 (129 文檔)\n",
      "    ✓ 合併 Topic 22 → Topic 48 (219 文檔)\n",
      "    ✓ 合併 Topic 48 → Topic 7 (266 文檔)\n",
      "    ✓ 合併 Topic 5 → Topic 8 (118 文檔)\n",
      "    ✓ 合併 Topic 16 → Topic 50 (102 文檔)\n",
      "    ✓ 合併 Topic 24 → Topic 50 (88 文檔)\n",
      "    ✓ 合併 Topic 10 → Topic 29 (111 文檔)\n",
      "    ✓ 合併 Topic 4 → Topic 29 (119 文檔)\n",
      "    ✓ 合併 Topic 40 → Topic 55 (62 文檔)\n",
      "    ✓ 合併 Topic 14 → Topic 17 (103 文檔)\n",
      "    ✓ 合併 Topic 31 → Topic 41 (78 文檔)\n",
      "    ✓ 合併 Topic 27 → Topic 57 (84 文檔)\n",
      "    ✓ 合併 Topic 12 → Topic 66 (107 文檔)\n",
      "    ✓ 合併 Topic 9 → Topic 42 (112 文檔)\n",
      "  - 完成 16 個主題合併\n",
      "\n",
      "⚙ 應用主題拆分...\n",
      "    ⚠ Topic 1 無法進一步拆分\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 22:34:49,292 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ⚠ Topic 8 無法進一步拆分\n",
      "    ⚠ Topic 16 文檔數太少 (0)，跳過拆分\n",
      "    ⚠ Topic 23 無法進一步拆分\n",
      "    ⚠ Topic 26 無法進一步拆分\n",
      "    ⚠ Topic 30 無法進一步拆分\n",
      "    ⚠ Topic 35 無法進一步拆分\n",
      "    ⚠ Topic 62 無法進一步拆分\n",
      "\n",
      "⚙ 更新停用詞...\n",
      "  - 新增 44 個停用詞: ['company', 'companies', 'corporation', 'registrant', 'report', 'reports', 'item', 'note', 'table', 'page', 'see', 'management', 'statement', 'statements', 'period', 'year', 'years', 'percent', 'million', 'billion', 'fiscal', 'the', 'and', 'of', 'to', 'our', 'we', 'may', 'could', 'or', 'us', 'also', 'including', 'based', 'related', 'information', 'file', 'filing', 'inc', 'ltd', 'corp', 'www', 'http', 'note']\n",
      "  - 已更新主題表示（移除停用詞）\n",
      "\n",
      "⚙ 根據參數建議重新訓練模型...\n",
      "\n",
      "⚙ 應用主題重命名...\n",
      "  - 完成 42 個主題重命名\n",
      "\n",
      "結果對比:\n",
      "  舊: 一致性: 0.7715\n",
      "  舊: 區分度: 0.4042\n",
      "  舊: Silhouette: 0.0695\n",
      "  舊: 離群率: 13.72%\n",
      "  新: 一致性: 0.7761\n",
      "  新: 區分度: 0.4074\n",
      "  新: Silhouette: 0.0679\n",
      "  新: 離群率: 13.99%\n",
      "\n",
      "⚙ 評估是否繼續優化...\n",
      "\n",
      "  決策: CONTINUE\n",
      "  理由: Metrics have not converged (<1% change not met across two iterations). Cohesion and separation show small improvements and outlier_rate is improved overall, but silhouette is declining notably across recent iterations. Continue optimization, focusing on actions to recover silhouette (e.g., parameter tuning or topic refinement).\n",
      "\n",
      "✓ 第 4 輪完成\n",
      "\n",
      "==================================================\n",
      "第 5 輪優化\n",
      "==================================================\n",
      "⚙ 請求 LLM 優化建議...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 22:40:51,253 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - 合併建議: 30 對\n",
      "  - 拆分建議: 6 個主題\n",
      "  - 停用詞建議: 50 個詞\n",
      "  - 參數調整: {'min_cluster_size': 25, 'min_samples': 3, 'n_neighbors': 15}\n",
      "  - 重命名: 81 個主題\n",
      "\n",
      "⚙ 應用主題合併...\n",
      "    ✓ 合併 Topic 12 → Topic 32 (102 文檔)\n",
      "    ✓ 合併 Topic 32 → Topic 53 (170 文檔)\n",
      "    ✓ 合併 Topic 20 → Topic 43 (88 文檔)\n",
      "    ✓ 合併 Topic 21 → Topic 26 (86 文檔)\n",
      "    ✓ 合併 Topic 11 → Topic 42 (103 文檔)\n",
      "    ✓ 合併 Topic 18 → Topic 65 (91 文檔)\n",
      "    ✓ 合併 Topic 36 → Topic 40 (59 文檔)\n",
      "    ✓ 合併 Topic 7 → Topic 39 (117 文檔)\n",
      "    ✓ 合併 Topic 15 → Topic 29 (95 文檔)\n",
      "    ✓ 合併 Topic 29 → Topic 37 (169 文檔)\n",
      "    ✓ 合併 Topic 34 → Topic 76 (62 文檔)\n",
      "    ✓ 合併 Topic 30 → Topic 58 (72 文檔)\n",
      "    ✓ 合併 Topic 27 → Topic 50 (77 文檔)\n",
      "    ✓ 合併 Topic 65 → Topic 66 (127 文檔)\n",
      "    ✓ 合併 Topic 14 → Topic 56 (95 文檔)\n",
      "    ✓ 合併 Topic 46 → Topic 70 (46 文檔)\n",
      "    ✓ 合併 Topic 51 → Topic 72 (45 文檔)\n",
      "    ✓ 合併 Topic 10 → Topic 52 (106 文檔)\n",
      "    ✓ 合併 Topic 31 → Topic 64 (72 文檔)\n",
      "    ✓ 合併 Topic 24 → Topic 55 (84 文檔)\n",
      "    ✓ 合併 Topic 57 → Topic 67 (42 文檔)\n",
      "    ✓ 合併 Topic 1 → Topic 19 (138 文檔)\n",
      "  - 完成 22 個主題合併\n",
      "\n",
      "⚙ 應用主題拆分...\n",
      "    ⚠ Topic 3 無法進一步拆分\n",
      "    ⚠ Topic 11 文檔數太少 (0)，跳過拆分\n",
      "    ⚠ Topic 15 文檔數太少 (0)，跳過拆分\n",
      "    ⚠ Topic 16 無法進一步拆分\n",
      "    ⚠ Topic 23 無法進一步拆分\n",
      "    ⚠ Topic 65 文檔數太少 (0)，跳過拆分\n",
      "\n",
      "⚙ 更新停用詞...\n",
      "  - 新增 50 個停用詞: ['company', 'corporation', 'inc', 'the', 'our', 'we', 'may', 'could', 'will', 'including', 'including,', 'report', 'reports', 'fiscal', 'statement', 'statements', 'note', 'notes', 'see', 'page', 'pages', 'percent', 'pct', 'million', 'billion', 'usd', 'us', 'item', 'section', 'part', 'document', 'incorporated', 'based', 'related', 'dated', 'dated:', 'filed', 'registrant', 'subsidiary', 'subsidiaries', 'companys', 'thereof', 'herein', 'hereby', 'respectively', 'prior', 'year', 'years', 'table', 'tables']\n",
      "  - 已更新主題表示（移除停用詞）\n",
      "\n",
      "⚙ 根據參數建議重新訓練模型...\n",
      "\n",
      "⚙ 應用主題重命名...\n",
      "  - 完成 81 個主題重命名\n",
      "\n",
      "結果對比:\n",
      "  舊: 一致性: 0.7761\n",
      "  舊: 區分度: 0.4074\n",
      "  舊: Silhouette: 0.0679\n",
      "  舊: 離群率: 13.99%\n",
      "  新: 一致性: 0.7761\n",
      "  新: 區分度: 0.4074\n",
      "  新: Silhouette: 0.0679\n",
      "  新: 離群率: 13.99%\n",
      "\n",
      "⚙ 評估是否繼續優化...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 277\u001b[39m\n\u001b[32m    252\u001b[39m     history_summary.append({\n\u001b[32m    253\u001b[39m         \u001b[33m'\u001b[39m\u001b[33miter\u001b[39m\u001b[33m'\u001b[39m: h[\u001b[33m'\u001b[39m\u001b[33miteration\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    254\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcoh\u001b[39m\u001b[33m'\u001b[39m: h[\u001b[33m'\u001b[39m\u001b[33mcohesion\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mactions\u001b[39m\u001b[33m'\u001b[39m: h.get(\u001b[33m'\u001b[39m\u001b[33mactions_applied\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m    259\u001b[39m     })\n\u001b[32m    261\u001b[39m stopping_prompt = (\n\u001b[32m    262\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mYou are evaluating whether to continue topic model optimization.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    263\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOptimization history (last \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(history_summary)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m iterations):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    274\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mOutput JSON: \u001b[39m\u001b[33m{\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mdecision\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mSTOP\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m or \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mCONTINUE\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mreason\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mbrief explanation\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m}\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    275\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m stopping_response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLLM_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLLM_TEMPERATURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are an optimization expert. Output JSON only.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopping_prompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    286\u001b[39m stopping_raw = stopping_response.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1156\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1110\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1112\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1153\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1154\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1155\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\openai\\_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\W\\miniconda3\\envs\\augmentation-agent-topic\\Lib\\ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 迭代優化主循環（完整實現版）\n",
    "# ========================================\n",
    "\n",
    "# 載入 Phase 2 結果\n",
    "df = pd.read_csv(PHASE2_CORPUS_CSV)\n",
    "embeddings = np.load(EMBEDDINGS_PATH)\n",
    "current_model = BERTopic.load(PHASE2_MODEL_DIR.as_posix())\n",
    "current_topics = df['topic'].tolist()\n",
    "\n",
    "print(f\"⚙ 開始智能迭代優化（最多 {MAX_OPTIMIZATION_ITERATIONS} 輪）...\\n\")\n",
    "print(\"✓ 完整實現版：支持合併、拆分、停用詞、參數調整、重命名\\n\")\n",
    "\n",
    "# 記錄優化歷史\n",
    "optimization_history = []\n",
    "\n",
    "# 初始指標\n",
    "coh, sep, sil, out = compute_metrics(embeddings, current_topics)\n",
    "print(\"初始指標:\")\n",
    "print_metrics(coh, sep, sil, out, \"  \")\n",
    "print()\n",
    "\n",
    "# 記錄初始狀態\n",
    "optimization_history.append({\n",
    "    'iteration': 0,\n",
    "    'cohesion': float(np.mean(list(coh.values()))) if coh else None,\n",
    "    'separation': float(sep) if not np.isnan(sep) else None,\n",
    "    'silhouette': float(sil) if not np.isnan(sil) else None,\n",
    "    'outlier_rate': float(out),\n",
    "    'action': 'baseline'\n",
    "})\n",
    "\n",
    "iteration = 0\n",
    "should_continue = True\n",
    "\n",
    "while should_continue and iteration < MAX_OPTIMIZATION_ITERATIONS:\n",
    "    iteration += 1\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"第 {iteration} 輪優化\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # 1. 採樣主題代表詞和例句\n",
    "    def sample_topic_info(model, df, topic_col='topic', k=TOPIC_SAMPLE_SIZE):\n",
    "        samples = {}\n",
    "        for tid in model.get_topic_info()['Topic'].tolist():\n",
    "            if tid == -1:\n",
    "                continue\n",
    "            words = ', '.join([w for w, _ in model.get_topic(tid)[:10]])\n",
    "            examples = df[df[topic_col] == tid]['text'].head(k).tolist()\n",
    "            samples[tid] = {\"words\": words, \"examples\": examples}\n",
    "        return samples\n",
    "    \n",
    "    topic_col = 'topic' if iteration == 1 else f'topic_v{iteration+1}'\n",
    "    samples = sample_topic_info(current_model, df, topic_col)\n",
    "    \n",
    "    # 2. 請 LLM 給出優化建議\n",
    "    print(\"⚙ 請求 LLM 優化建議...\")\n",
    "    \n",
    "    plan_prompt = (\n",
    "        \"\"\"\n",
    "        # --- Persona and Goal Definition ---\n",
    "        You are an expert academic researcher specializing in the analysis of Corporate ESG (Environmental, Social, and Governance) reports. Your primary goal is to refine a raw list of topics, generated from financial documents (like 10-K filings), into a set of meaningful, coherent, and high-level ESG themes. These refined themes must be suitable for subsequent quantitative analysis to evaluate a corporation's Digital Resilience.\n",
    "\n",
    "        # --- Guiding Principles ---\n",
    "        Analyze the provided topic list based on the following principles to propose optimization actions:\n",
    "\n",
    "        1.  **Abstraction and Generalization:**\n",
    "            -   Identify and merge topics that are too specific or company-centric (e.g., topics about 'JPMorgan's loan portfolio' and 'Bank of America's credit risk' should be merged into a more general theme like 'Financial Institution Credit Risk Management').\n",
    "            -   Rename topics to reflect broader ESG concepts rather than just listing keywords. The name should be an interpretable theme (e.g., rename a topic with keywords 'emissions, carbon, green' to 'Climate Change & Carbon Footprint').\n",
    "\n",
    "        2.  **Cohesion and Separation:**\n",
    "            -   Propose to merge topics that are semantically synonymous or represent a parent-child relationship (e.g., 'Data Security' and 'Cybersecurity Incidents' can be merged).\n",
    "            -   Propose to split topics that contain multiple, distinct concepts (e.g., a topic containing keywords for both 'employee training' and 'data privacy' is too broad and should be split).\n",
    "\n",
    "        3.  **Noise Reduction:**\n",
    "            -   Identify common, non-informative words across many topics that should be added to a stopword list (e.g., 'company', 'billion', 'report', 'fiscal').\n",
    "            -   Flag topics that are clearly artifacts of document structure or boilerplate language (e.g., topics about SEC filing numbers, proxy statements) for potential removal or re-evaluation.\n",
    "\n",
    "        4.  **Parameter Tuning Logic:**\n",
    "            -   The goal of tuning HDBSCAN/UMAP parameters is to reduce the number of documents in the outlier cluster (topic ID '-1') while ensuring the remaining topics remain coherent and distinct. Suggest SMALL, incremental changes to 'min_cluster_size' or 'min_samples' to achieve this balance. For example, slightly decreasing `min_cluster_size` might help capture smaller, emerging themes.\n",
    "\n",
    "        # --- Input Format ---\n",
    "        You will be provided with a list of topics. Each topic includes an ID, its original top keywords, and a custom label.\n",
    "\n",
    "        # --- Output Format ---\n",
    "        Your response MUST be a single, valid JSON object, and nothing else. Follow the structure below precisely.\n",
    "\n",
    "        {\n",
    "            \"merge_pairs\": [\n",
    "                // List of pairs of topic IDs to be merged. Justify based on semantic similarity or hierarchical relationship.\n",
    "                // Example: [[6, 24], [40, 64]]\n",
    "            ],\n",
    "            \"split_topics\": [\n",
    "                // List of topic IDs that are too broad and contain multiple distinct themes.\n",
    "                // Example: [15]\n",
    "            ],\n",
    "            \"rename\": {\n",
    "                // Dictionary of topic IDs to new, more descriptive and abstract names. The new name should be a high-level ESG theme.\n",
    "                // Example: {\"0\": \"Corporate Tax Strategy & Deferred Assets\", \"51\": \"Data Privacy & Regulatory Compliance\"}\n",
    "            },\n",
    "            \"new_stopwords\": [\n",
    "                // List of new, domain-specific stopwords to filter out noise.\n",
    "                // Example: [\"company\", \"statement\", \"billion\", \"fiscal\"]\n",
    "            ],\n",
    "            \"params\": {\n",
    "                // Suggestions for tuning clustering parameters to reduce outliers. Provide only the parameters to be changed.\n",
    "                // Example: {\"min_cluster_size\": 45}\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # --- Final Instruction ---\n",
    "        Now, analyze the following topics and provide your optimization suggestions. Output ONLY the valid JSON object without any explanations or surrounding text.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        temperature=LLM_TEMPERATURE,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a topic modeling optimization expert. Output JSON only.\"},\n",
    "            {\"role\": \"user\", \"content\": plan_prompt},\n",
    "            {\"role\": \"user\", \"content\": json.dumps({\"topics\": samples}, ensure_ascii=False)}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    raw = response.choices[0].message.content\n",
    "    try:\n",
    "        plan = json.loads(raw)\n",
    "    except Exception:\n",
    "        # 嘗試提取 JSON\n",
    "        match = re.search(r'\\{[\\s\\S]*\\}', raw)\n",
    "        if match:\n",
    "            plan = json.loads(match.group(0))\n",
    "        else:\n",
    "            print(\"⚠ 無法解析 LLM 輸出，跳過本輪\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"  - 合併建議: {len(plan.get('merge_pairs', []))} 對\")\n",
    "    print(f\"  - 拆分建議: {len(plan.get('split_topics', []))} 個主題\")\n",
    "    print(f\"  - 停用詞建議: {len(plan.get('new_stopwords', []))} 個詞\")\n",
    "    print(f\"  - 參數調整: {plan.get('params', {})}\")\n",
    "    print(f\"  - 重命名: {len(plan.get('rename', {}))} 個主題\")\n",
    "    \n",
    "    # 標記是否有實質性變更\n",
    "    has_structural_changes = (\n",
    "        len(plan.get('merge_pairs', [])) > 0 or\n",
    "        len(plan.get('split_topics', [])) > 0 or\n",
    "        len(plan.get('new_stopwords', [])) > 0\n",
    "    )\n",
    "    \n",
    "    # 3. 應用合併操作\n",
    "    if plan.get('merge_pairs'):\n",
    "        print(\"\\n⚙ 應用主題合併...\")\n",
    "        current_topics = merge_topics(current_model, current_topics, plan['merge_pairs'])\n",
    "        df[topic_col] = current_topics\n",
    "    \n",
    "    # 4. 應用拆分操作\n",
    "    if plan.get('split_topics'):\n",
    "        print(\"\\n⚙ 應用主題拆分...\")\n",
    "        for tid in plan['split_topics']:\n",
    "            df, current_topics = split_topic(current_model, df, embeddings, int(tid), topic_col)\n",
    "    \n",
    "    # 5. 應用停用詞更新\n",
    "    if plan.get('new_stopwords'):\n",
    "        print(\"\\n⚙ 更新停用詞...\")\n",
    "        current_model = update_stopwords_and_representation(\n",
    "            current_model, texts, current_topics, embeddings, plan['new_stopwords']\n",
    "        )\n",
    "    \n",
    "    # 6. 如果有參數變更，重新訓練模型\n",
    "    params = plan.get('params', {})\n",
    "    if params:\n",
    "        print(\"\\n⚙ 根據參數建議重新訓練模型...\")\n",
    "        \n",
    "        min_cluster_size = int(params.get('min_cluster_size', HDBSCAN_MIN_CLUSTER_SIZE))\n",
    "        min_samples = int(params.get('min_samples', HDBSCAN_MIN_SAMPLES))\n",
    "        n_neighbors = int(params.get('n_neighbors', UMAP_N_NEIGHBORS))\n",
    "        n_components = int(params.get('n_components', UMAP_N_COMPONENTS))\n",
    "        \n",
    "        umap_opt = UMAP(\n",
    "            n_neighbors=n_neighbors,\n",
    "            n_components=n_components,\n",
    "            min_dist=UMAP_MIN_DIST,\n",
    "            metric=UMAP_METRIC,\n",
    "            random_state=RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        hdbscan_opt = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            metric=HDBSCAN_METRIC,\n",
    "            cluster_selection_method=HDBSCAN_SELECTION_METHOD,\n",
    "            prediction_data=True\n",
    "        )\n",
    "        \n",
    "        optimized_model = BERTopic(\n",
    "            calculate_probabilities=True,\n",
    "            verbose=False,\n",
    "            umap_model=umap_opt,\n",
    "            hdbscan_model=hdbscan_opt\n",
    "        )\n",
    "        \n",
    "        new_topics, new_probs = optimized_model.fit_transform(texts, embeddings=embeddings)\n",
    "        current_topics = new_topics\n",
    "        current_model = optimized_model\n",
    "        df[topic_col] = current_topics\n",
    "    \n",
    "    # 7. 應用重命名\n",
    "    if plan.get('rename') and isinstance(plan['rename'], dict):\n",
    "        print(\"\\n⚙ 應用主題重命名...\")\n",
    "        rename_map = {int(k): v for k, v in plan['rename'].items()}\n",
    "        rename_count = 0\n",
    "        for tid, name in rename_map.items():\n",
    "            try:\n",
    "                current_model.set_topic_labels({tid: name})\n",
    "                rename_count += 1\n",
    "            except:\n",
    "                pass\n",
    "        print(f\"  - 完成 {rename_count} 個主題重命名\")\n",
    "    \n",
    "    # 8. 計算新指標\n",
    "    new_coh, new_sep, new_sil, new_out = compute_metrics(embeddings, current_topics)\n",
    "    \n",
    "    print(\"\\n結果對比:\")\n",
    "    print_metrics(coh, sep, sil, out, \"  舊: \")\n",
    "    print_metrics(new_coh, new_sep, new_sil, new_out, \"  新: \")\n",
    "    \n",
    "    # 9. 記錄歷史（轉換為 Python 原生類型）\n",
    "    optimization_history.append({\n",
    "        'iteration': iteration,\n",
    "        'plan': plan,\n",
    "        'cohesion': float(np.mean(list(new_coh.values()))) if new_coh else None,\n",
    "        'separation': float(new_sep) if not np.isnan(new_sep) else None,\n",
    "        'silhouette': float(new_sil) if not np.isnan(new_sil) else None,\n",
    "        'outlier_rate': float(new_out),\n",
    "        'actions_applied': {\n",
    "            'merge': len(plan.get('merge_pairs', [])),\n",
    "            'split': len(plan.get('split_topics', [])),\n",
    "            'stopwords': len(plan.get('new_stopwords', [])),\n",
    "            'params': bool(params),\n",
    "            'rename': len(plan.get('rename', {}))\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # 10. 智能停止判斷\n",
    "    if ENABLE_SMART_STOPPING and iteration >= 2:\n",
    "        print(\"\\n⚙ 評估是否繼續優化...\")\n",
    "        \n",
    "        # 準備歷史數據\n",
    "        history_summary = []\n",
    "        for h in optimization_history:\n",
    "            history_summary.append({\n",
    "                'iter': h['iteration'],\n",
    "                'coh': h['cohesion'],\n",
    "                'sep': h['separation'],\n",
    "                'sil': h['silhouette'],\n",
    "                'out': h['outlier_rate'],\n",
    "                'actions': h.get('actions_applied', {})\n",
    "            })\n",
    "        \n",
    "        stopping_prompt = (\n",
    "            \"You are evaluating whether to continue topic model optimization.\\n\\n\"\n",
    "            f\"Optimization history (last {len(history_summary)} iterations):\\n\"\n",
    "            f\"{json.dumps(history_summary, indent=2)}\\n\\n\"\n",
    "            \"Metrics explanation:\\n\"\n",
    "            \"- cohesion: higher is better (internal similarity)\\n\"\n",
    "            \"- separation: higher is better (topic distinctiveness)\\n\"\n",
    "            \"- silhouette: higher is better (-1 to 1 range)\\n\"\n",
    "            \"- outlier_rate: lower is better\\n\\n\"\n",
    "            \"Decision criteria:\\n\"\n",
    "            \"- STOP if metrics have converged (< 1% change for 2 iterations)\\n\"\n",
    "            \"- STOP if metrics are degrading consistently\\n\"\n",
    "            \"- CONTINUE if showing improvement\\n\\n\"\n",
    "            \"Output JSON: {\\\"decision\\\": \\\"STOP\\\" or \\\"CONTINUE\\\", \\\"reason\\\": \\\"brief explanation\\\"}\"\n",
    "        )\n",
    "        \n",
    "        stopping_response = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            temperature=LLM_TEMPERATURE,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an optimization expert. Output JSON only.\"},\n",
    "                {\"role\": \"user\", \"content\": stopping_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        stopping_raw = stopping_response.choices[0].message.content\n",
    "        try:\n",
    "            stopping_decision = json.loads(stopping_raw)\n",
    "        except:\n",
    "            match = re.search(r'\\{[\\s\\S]*\\}', stopping_raw)\n",
    "            stopping_decision = json.loads(match.group(0)) if match else {\"decision\": \"CONTINUE\", \"reason\": \"Parse error\"}\n",
    "        \n",
    "        decision = stopping_decision.get('decision', 'CONTINUE').upper()\n",
    "        reason = stopping_decision.get('reason', 'N/A')\n",
    "        \n",
    "        print(f\"\\n  決策: {decision}\")\n",
    "        print(f\"  理由: {reason}\")\n",
    "        \n",
    "        if decision == 'STOP':\n",
    "            should_continue = False\n",
    "            print(\"\\n✓ 智能停止：達到優化目標或已收斂\")\n",
    "        else:\n",
    "            # 更新狀態，準備下一輪\n",
    "            coh, sep, sil, out = new_coh, new_sep, new_sil, new_out\n",
    "    else:\n",
    "        # 更新狀態，準備下一輪\n",
    "        coh, sep, sil, out = new_coh, new_sep, new_sil, new_out\n",
    "    \n",
    "    # 保存中間結果\n",
    "    next_topic_col = f'topic_v{iteration + 2}'\n",
    "    df[next_topic_col] = current_topics\n",
    "    \n",
    "    print(f\"\\n✓ 第 {iteration} 輪完成\\n\")\n",
    "\n",
    "# 保存最終結果\n",
    "current_model.save(PHASE3_MODEL_DIR.as_posix(), serialization=\"safetensors\")\n",
    "df.to_csv(PHASE3_CORPUS_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "# 保存優化歷史\n",
    "with open(PHASE3_OPTIMIZATION_CACHE, 'w') as f:\n",
    "    json.dump(optimization_history, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"✓ Phase 3 完成\")\n",
    "print(f\"  - 總迭代次數: {iteration}\")\n",
    "print(f\"  - 最終主題數: {len(set([t for t in current_topics if t != -1]))}\")\n",
    "print(f\"  - 優化歷史已保存: {PHASE3_OPTIMIZATION_CACHE}\")\n",
    "\n",
    "# 顯示優化趨勢和應用的操作\n",
    "print(\"\\n優化趨勢和操作摘要:\")\n",
    "for h in optimization_history:\n",
    "    iter_num = h['iteration']\n",
    "    coh_val = h.get('cohesion', 0)\n",
    "    sil_val = h.get('silhouette', 0)\n",
    "    out_val = h.get('outlier_rate', 0)\n",
    "    actions = h.get('actions_applied', {})\n",
    "    \n",
    "    if iter_num == 0:\n",
    "        print(f\"  Iteration {iter_num} (基準): coh={coh_val:.4f}, sil={sil_val:.4f}, out={out_val:.2%}\")\n",
    "    else:\n",
    "        action_str = f\"merge={actions.get('merge', 0)}, split={actions.get('split', 0)}, stopwords={actions.get('stopwords', 0)}, rename={actions.get('rename', 0)}\"\n",
    "        print(f\"  Iteration {iter_num}: coh={coh_val:.4f}, sil={sil_val:.4f}, out={out_val:.2%} [{action_str}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase4",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 4: 主題映射與評分（雙重優化版）\n",
    "\n",
    "**核心優化**：\n",
    "1. **提供主題關鍵詞**：LLM 映射時不再是 \"Topic 0\"，而是 \"Topic 0: tax, income, taxes, deferred...\"\n",
    "2. **主題層級評分**：先對每個主題評分，再根據文檔的主題分佈加權計算\n",
    "3. **批次評分 (NEW!)**：一次 API 調用返回多個構面分數，而非 7 次獨立調用\n",
    "4. **選擇性評分 (NEW!)**：只評分語義相關的構面，而非所有 7 個構面\n",
    "5. **大幅加速**：從 819 次 API 調用 → 117 次 (85% 減少，94分鐘 → 約 5-10 分鐘)\n",
    "\n",
    "**優化效果對比**：\n",
    "- 原始方法：117 主題 × 7 構面 = 819 次 API 調用 (~94 分鐘)\n",
    "- 優化方法：117 主題 × 1 次批次調用 = 117 次 API 調用 (~5-10 分鐘)\n",
    "- **加速比：7-18x**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase4_load",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 01:41:42,335 - BERTopic - WARNING: You are loading a BERTopic model without explicitly defining an embedding model. If you want to also load in an embedding model, make sure to use `BERTopic.load(my_model, embedding_model=my_embedding_model)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙ 載入優化後的主題模型...\n",
      "  - 使用主題欄位: topic_v3\n",
      "  - 文檔數: 6233\n",
      "  - 主題數: 117\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 載入 Phase 3 結果\n",
    "# ========================================\n",
    "\n",
    "print(\"⚙ 載入優化後的主題模型...\")\n",
    "\n",
    "# 嘗試載入最新版本\n",
    "if PHASE3_CORPUS_CSV.exists():\n",
    "    df = pd.read_csv(PHASE3_CORPUS_CSV)\n",
    "    model = BERTopic.load(PHASE3_MODEL_DIR.as_posix())\n",
    "    # 找到最後一個 topic 欄位\n",
    "    topic_cols = [c for c in df.columns if c.startswith('topic')]\n",
    "    TOPIC_COL = topic_cols[-1] if topic_cols else 'topic'\n",
    "    print(f\"  - 使用主題欄位: {TOPIC_COL}\")\n",
    "else:\n",
    "    print(\"  - Phase 3 結果未找到，使用 Phase 2\")\n",
    "    df = pd.read_csv(PHASE2_CORPUS_CSV)\n",
    "    model = BERTopic.load(PHASE2_MODEL_DIR.as_posix())\n",
    "    TOPIC_COL = 'topic'\n",
    "\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "print(f\"  - 文檔數: {len(df)}\")\n",
    "print(f\"  - 主題數: {df[TOPIC_COL].nunique() - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase4_map_topics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙ 映射主題到數位韌性構面...\n",
      "  - 生成新映射（請求 LLM）...\n",
      "  - 已保存緩存: data/phase4_topic_dimension_map.json\n",
      "\n",
      "映射統計:\n",
      "  - OTHER: 51 個主題\n",
      "  - GOVSEC: 35 個主題\n",
      "  - ITC: 17 個主題\n",
      "  - DATA: 10 個主題\n",
      "  - ACAP: 2 個主題\n",
      "  - ECO: 2 個主題\n",
      "\n",
      "映射示例（前10個）:\n",
      "  Topic 0 (tax, income, taxes, deferred, foreign...) → GOVSEC\n",
      "  Topic 1 (health, care, medicare, medical, unitedhealthcare...) → OTHER\n",
      "  Topic 2 (products, product, healthcare, drug, generic...) → OTHER\n",
      "  Topic 3 (pension, plans, benefit, plan, postretirement...) → OTHER\n",
      "  Topic 4 (exxonmobil, gas, oil, exxonmobils, energy...) → OTHER\n",
      "  Topic 5 (procter, gamble, net, care, sales...) → OTHER\n",
      "  Topic 6 (standard, asu, adoption, accounting, update...) → GOVSEC\n",
      "  Topic 7 (cash, billion, financing, activities, net...) → GOVSEC\n",
      "  Topic 8 (hedges, derivative, derivatives, hedge, instruments...) → OTHER\n",
      "  Topic 9 (products, earnings, industrial, revenues, manufacturing...) → OTHER\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 1: 主題 → 構面映射（帶關鍵詞）\n",
    "# ========================================\n",
    "\n",
    "print(\"⚙ 映射主題到數位韌性構面...\")\n",
    "\n",
    "# 檢查緩存\n",
    "if PHASE4_TOPIC_DIM_MAP_CACHE.exists():\n",
    "    print(\"  - 從緩存載入映射...\")\n",
    "    with open(PHASE4_TOPIC_DIM_MAP_CACHE, 'r') as f:\n",
    "        topic_to_dimension = json.load(f)\n",
    "    # 轉換 key 為 int\n",
    "    topic_to_dimension = {int(k): v for k, v in topic_to_dimension.items()}\n",
    "else:\n",
    "    print(\"  - 生成新映射（請求 LLM）...\")\n",
    "    \n",
    "    # 獲取所有主題及其關鍵詞\n",
    "    topic_ids = sorted([int(t) for t in df[TOPIC_COL].dropna().unique() if t != -1])\n",
    "    topic_descriptions = {}\n",
    "    \n",
    "    for tid in topic_ids:\n",
    "        # 獲取主題的代表詞（前10個）\n",
    "        try:\n",
    "            words = [w for w, _ in model.get_topic(tid)[:10]]\n",
    "            topic_descriptions[tid] = f\"Topic {tid}: {', '.join(words)}\"\n",
    "        except:\n",
    "            topic_descriptions[tid] = f\"Topic {tid}\"\n",
    "    \n",
    "    # 請求 LLM 映射\n",
    "    mapping_prompt = (\n",
    "        \"You are a research assistant. Map each topic to ONE digital resilience dimension:\\n\"\n",
    "        f\"Dimensions: {', '.join(DIMENSIONS)}\\n\\n\"\n",
    "        \"Dimension definitions:\\n\"\n",
    "        \"- ITC: IT infrastructure, cloud, networks, hardware, software systems\\n\"\n",
    "        \"- ACAP: Cybersecurity, threat detection, access control, encryption\\n\"\n",
    "        \"- DC: Data centers, disaster recovery, business continuity, redundancy\\n\"\n",
    "        \"- GOVSEC: Governance, compliance, regulations, security policies, audits\\n\"\n",
    "        \"- DATA: Data management, analytics, privacy, data quality\\n\"\n",
    "        \"- ECO: Digital ecosystem, partnerships, innovation, digital transformation\\n\"\n",
    "        \"- OTHER: None of the above\\n\\n\"\n",
    "        \"Output JSON: {\\\"Topic 0: keywords\\\": \\\"DIMENSION\\\", ...}\\n\"\n",
    "        \"Output ONLY valid JSON, no explanation.\"\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        temperature=LLM_TEMPERATURE,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a research assistant. Output JSON only.\"},\n",
    "            {\"role\": \"user\", \"content\": mapping_prompt},\n",
    "            {\"role\": \"user\", \"content\": json.dumps({\n",
    "                \"topics\": list(topic_descriptions.values())\n",
    "            }, ensure_ascii=False)}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    raw = response.choices[0].message.content\n",
    "    try:\n",
    "        mapping_result = json.loads(raw)\n",
    "    except Exception:\n",
    "        match = re.search(r'\\{[\\s\\S]*\\}', raw)\n",
    "        mapping_result = json.loads(match.group(0)) if match else {}\n",
    "    \n",
    "    # 解析映射結果（key 可能是 \"Topic X: ...\" 格式）\n",
    "    topic_to_dimension = {}\n",
    "    for key, dim in mapping_result.items():\n",
    "        # 提取 topic id\n",
    "        match = re.search(r'Topic (\\d+)', key)\n",
    "        if match:\n",
    "            tid = int(match.group(1))\n",
    "            topic_to_dimension[tid] = dim\n",
    "    \n",
    "    # 保存緩存\n",
    "    with open(PHASE4_TOPIC_DIM_MAP_CACHE, 'w') as f:\n",
    "        json.dump(topic_to_dimension, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"  - 已保存緩存: {PHASE4_TOPIC_DIM_MAP_CACHE}\")\n",
    "\n",
    "# 統計映射結果\n",
    "dim_counts = pd.Series(topic_to_dimension.values()).value_counts()\n",
    "print(\"\\n映射統計:\")\n",
    "for dim, count in dim_counts.items():\n",
    "    print(f\"  - {dim}: {count} 個主題\")\n",
    "\n",
    "# 顯示部分映射示例\n",
    "print(\"\\n映射示例（前10個）:\")\n",
    "for tid in sorted(topic_to_dimension.keys())[:10]:\n",
    "    words = ', '.join([w for w, _ in model.get_topic(tid)[:5]])\n",
    "    dim = topic_to_dimension[tid]\n",
    "    print(f\"  Topic {tid} ({words}...) → {dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase4_score_topics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Step 2: 主題層級評分（優化版 - 批次+選擇性評分）\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n⚙ 對主題×構面進行評分（優化版：批次+選擇性）...\")\n",
    "\n",
    "# 檢查緩存\n",
    "if PHASE4_TOPIC_SCORES_CACHE.exists():\n",
    "    print(\"  - 從緩存載入評分...\")\n",
    "    with open(PHASE4_TOPIC_SCORES_CACHE, 'r') as f:\n",
    "        topic_scores = json.load(f)\n",
    "    # 轉換 key\n",
    "    topic_scores = {int(k): v for k, v in topic_scores.items()}\n",
    "else:\n",
    "    print(\"  - 生成新評分（優化版：批次評分 + 選擇性維度）...\")\n",
    "    print(\"  - 優化策略：\")\n",
    "    print(\"    1. 根據主題映射只評分相關構面（而非全部7個）\")\n",
    "    print(\"    2. 一次API調用返回多個構面分數（而非7次調用）\")\n",
    "    print(\"    3. 預期加速：819次API調用 → ~117次 (85%減少)\\n\")\n",
    "    \n",
    "    topic_scores = {}  # {topic_id: {dim: score}}\n",
    "    \n",
    "    # 為每個主題生成代表性描述\n",
    "    topic_ids = sorted([int(t) for t in df[TOPIC_COL].dropna().unique() if t != -1])\n",
    "    \n",
    "    # 統計API調用次數\n",
    "    api_calls_old = len(topic_ids) * len(DIMENSIONS)\n",
    "    api_calls_new = len(topic_ids)\n",
    "    print(f\"  - 舊方法需要: {api_calls_old} 次 API 調用\")\n",
    "    print(f\"  - 新方法需要: {api_calls_new} 次 API 調用\")\n",
    "    print(f\"  - 減少: {api_calls_old - api_calls_new} 次 ({(1 - api_calls_new/api_calls_old)*100:.1f}%)\\n\")\n",
    "    \n",
    "    for tid in tqdm(topic_ids, desc=\"評分主題\"):\n",
    "        # 獲取主題信息\n",
    "        words = ', '.join([w for w, _ in model.get_topic(tid)[:10]])\n",
    "        examples = df[df[TOPIC_COL] == tid]['text'].head(3).tolist()\n",
    "        \n",
    "        # 構建主題描述\n",
    "        topic_desc = (\n",
    "            f\"Topic {tid}\\n\"\n",
    "            f\"Keywords: {words}\\n\"\n",
    "            f\"Example excerpts:\\n\" +\n",
    "            \"\\n---\\n\".join([ex[:500] for ex in examples])\n",
    "        )\n",
    "        \n",
    "        # 確定要評分的構面（基於映射）\n",
    "        primary_dim = topic_to_dimension.get(tid, \"OTHER\")\n",
    "        dims_to_score = DIMENSION_GROUPS.get(primary_dim, [primary_dim])\n",
    "        \n",
    "        # 構建批次評分提示\n",
    "        dim_definitions = {\n",
    "            \"ITC\": \"IT infrastructure, cloud, networks, hardware, software systems\",\n",
    "            \"ACAP\": \"Cybersecurity, threat detection, access control, encryption\",\n",
    "            \"DC\": \"Data centers, disaster recovery, business continuity, redundancy\",\n",
    "            \"GOVSEC\": \"Governance, compliance, regulations, security policies, audits\",\n",
    "            \"DATA\": \"Data management, analytics, privacy, data quality\",\n",
    "            \"ECO\": \"Digital ecosystem, partnerships, innovation, digital transformation\",\n",
    "            \"OTHER\": \"None of the above dimensions\"\n",
    "        }\n",
    "        \n",
    "        dims_desc = \"\\n\".join([f\"- {dim}: {dim_definitions[dim]}\" for dim in dims_to_score])\n",
    "        \n",
    "        scoring_prompt = (\n",
    "            f\"Rate this topic's relevance to MULTIPLE digital resilience dimensions.\\n\\n\"\n",
    "            f\"Dimensions to evaluate:\\n{dims_desc}\\n\\n\"\n",
    "            f\"{SCORING_RUBRIC}\\n\\n\"\n",
    "            f\"Output JSON format: {{{', '.join([f'\\\"{d}\\\": <score>' for d in dims_to_score])}}}\\n\"\n",
    "            f\"Output ONLY valid JSON with numeric scores 0-5, no explanation.\\n\\n\"\n",
    "            f\"Topic information:\\n{topic_desc}\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=LLM_MODEL,\n",
    "                temperature=LLM_TEMPERATURE,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a domain expert evaluating topics. Output JSON only with numeric scores.\"},\n",
    "                    {\"role\": \"user\", \"content\": scoring_prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            raw = response.choices[0].message.content\n",
    "            # 解析 JSON\n",
    "            try:\n",
    "                result = json.loads(raw)\n",
    "            except:\n",
    "                # 嘗試提取 JSON\n",
    "                match = re.search(r'\\{[^}]+\\}', raw)\n",
    "                if match:\n",
    "                    result = json.loads(match.group(0))\n",
    "                else:\n",
    "                    result = {}\n",
    "            \n",
    "            # 驗證並規範化分數\n",
    "            scores = {}\n",
    "            for dim in dims_to_score:\n",
    "                score = result.get(dim, 0)\n",
    "                # 處理可能的嵌套格式 {\"score\": 3, \"reasoning\": \"...\"}\n",
    "                if isinstance(score, dict):\n",
    "                    score = score.get('score', 0)\n",
    "                score = float(score)\n",
    "                score = max(0, min(5, score))  # 限制在 0-5\n",
    "                scores[dim] = score\n",
    "            \n",
    "            # 填充未評分的維度為 0\n",
    "            full_scores = {dim: scores.get(dim, 0.0) for dim in DIMENSIONS}\n",
    "            topic_scores[tid] = full_scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ⚠ Topic {tid} 評分失敗: {e}\")\n",
    "            # 失敗時全部填0\n",
    "            topic_scores[tid] = {dim: 0.0 for dim in DIMENSIONS}\n",
    "    \n",
    "    # 保存緩存\n",
    "    with open(PHASE4_TOPIC_SCORES_CACHE, 'w') as f:\n",
    "        json.dump(topic_scores, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n  - 已保存緩存: {PHASE4_TOPIC_SCORES_CACHE}\")\n",
    "\n",
    "print(f\"\\n✓ 完成 {len(topic_scores)} 個主題的評分\")\n",
    "\n",
    "# 顯示評分示例\n",
    "print(\"\\n評分示例（前5個主題）:\")\n",
    "for tid in sorted(topic_scores.keys())[:5]:\n",
    "    words = ', '.join([w for w, _ in model.get_topic(tid)[:3]])\n",
    "    scores = topic_scores[tid]\n",
    "    mapped_dim = topic_to_dimension.get(tid, \"UNKNOWN\")\n",
    "    scored_dims = DIMENSION_GROUPS.get(mapped_dim, [mapped_dim])\n",
    "    print(f\"  Topic {tid} ({words}...) [映射到 {mapped_dim}, 評分 {scored_dims}]:\")\n",
    "    for dim, score in scores.items():\n",
    "        if score > 0:\n",
    "            print(f\"    {dim}: {score:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase4_calc_doc_scores",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙ 計算文檔級別的構面評分...\n",
      "  - 使用主題概率分佈進行加權計算\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "計算文檔評分: 100%|██████████| 6233/6233 [00:00<00:00, 43295.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 文檔評分完成\n",
      "  - 已保存: data/part4_doc_dimension_scores.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ITC</th>\n",
       "      <th>ACAP</th>\n",
       "      <th>DC</th>\n",
       "      <th>GOVSEC</th>\n",
       "      <th>DATA</th>\n",
       "      <th>ECO</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n10-K\\n1\\nbac-1231201710xk.htm\\n10-K\\nDocum...</td>\n",
       "      <td>0.567895</td>\n",
       "      <td>0.742863</td>\n",
       "      <td>0.770305</td>\n",
       "      <td>0.766579</td>\n",
       "      <td>0.943984</td>\n",
       "      <td>1.435887</td>\n",
       "      <td>0.831297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We routinely post and make accessible financia...</td>\n",
       "      <td>0.150651</td>\n",
       "      <td>0.175765</td>\n",
       "      <td>0.249193</td>\n",
       "      <td>0.229236</td>\n",
       "      <td>0.262637</td>\n",
       "      <td>0.443559</td>\n",
       "      <td>0.229552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and in international markets, we provide a div...</td>\n",
       "      <td>0.273042</td>\n",
       "      <td>0.407353</td>\n",
       "      <td>0.242627</td>\n",
       "      <td>0.702621</td>\n",
       "      <td>0.826736</td>\n",
       "      <td>0.942030</td>\n",
       "      <td>0.318550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We compete with some of these competitors glob...</td>\n",
       "      <td>0.229346</td>\n",
       "      <td>0.368801</td>\n",
       "      <td>0.353451</td>\n",
       "      <td>0.288197</td>\n",
       "      <td>0.244599</td>\n",
       "      <td>0.381106</td>\n",
       "      <td>0.368801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None of our domestic employees are subject to ...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       ITC      ACAP  \\\n",
       "0  \\n\\n10-K\\n1\\nbac-1231201710xk.htm\\n10-K\\nDocum...  0.567895  0.742863   \n",
       "1  We routinely post and make accessible financia...  0.150651  0.175765   \n",
       "2  and in international markets, we provide a div...  0.273042  0.407353   \n",
       "3  We compete with some of these competitors glob...  0.229346  0.368801   \n",
       "4  None of our domestic employees are subject to ...  3.000000  3.000000   \n",
       "\n",
       "         DC    GOVSEC      DATA       ECO     OTHER  \n",
       "0  0.770305  0.766579  0.943984  1.435887  0.831297  \n",
       "1  0.249193  0.229236  0.262637  0.443559  0.229552  \n",
       "2  0.242627  0.702621  0.826736  0.942030  0.318550  \n",
       "3  0.353451  0.288197  0.244599  0.381106  0.368801  \n",
       "4  1.000000  2.000000  3.000000  5.000000  3.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 3: 文檔層級評分（基於主題分佈）\n",
    "# ========================================\n",
    "\n",
    "print(\"⚙ 計算文檔級別的構面評分...\")\n",
    "\n",
    "# 如果有主題概率分佈，使用加權平均；否則使用硬分配\n",
    "if PHASE2_DOC_PROBS.exists():\n",
    "    print(\"  - 使用主題概率分佈進行加權計算\")\n",
    "    probs = np.load(PHASE2_DOC_PROBS)\n",
    "    use_probs = True\n",
    "else:\n",
    "    print(\"  - 使用硬主題分配\")\n",
    "    use_probs = False\n",
    "\n",
    "doc_scores = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"計算文檔評分\"):\n",
    "    scores = {dim: 0.0 for dim in DIMENSIONS}\n",
    "    \n",
    "    if use_probs and idx < len(probs):\n",
    "        # 基於主題概率的加權評分\n",
    "        prob_dist = probs[idx]\n",
    "        for tid, prob in enumerate(prob_dist):\n",
    "            if tid == -1 or prob < 0.01:  # 忽略離群和低概率\n",
    "                continue\n",
    "            if tid in topic_scores:\n",
    "                for dim in DIMENSIONS:\n",
    "                    scores[dim] += prob * topic_scores[tid].get(dim, 0)\n",
    "    else:\n",
    "        # 基於硬分配\n",
    "        tid = int(row[TOPIC_COL])\n",
    "        if tid != -1 and tid in topic_scores:\n",
    "            scores = topic_scores[tid].copy()\n",
    "    \n",
    "    doc_scores.append(scores)\n",
    "\n",
    "# 合併到 DataFrame\n",
    "scores_df = pd.DataFrame(doc_scores)\n",
    "result_df = pd.concat([df.reset_index(drop=True), scores_df], axis=1)\n",
    "\n",
    "# 保存\n",
    "result_df.to_csv(PHASE4_DOC_SCORES_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"✓ 文檔評分完成\")\n",
    "print(f\"  - 已保存: {PHASE4_DOC_SCORES_CSV}\")\n",
    "result_df[['text'] + DIMENSIONS].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase4_calc_dri",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙ 計算數位韌性指數（DRI）...\n",
      "  - 按 ['ticker', 'year'] 聚合\n",
      "  - 使用加權平均計算 DRI\n",
      "✓ DRI 計算完成\n",
      "  - 已保存: data/part4_entity_time_dri.csv\n",
      "\n",
      "DRI 統計:\n",
      "  - 平均值: 1.322\n",
      "  - 標準差: 0.204\n",
      "  - 範圍: [0.818, 1.585]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>year</th>\n",
       "      <th>ITC</th>\n",
       "      <th>ACAP</th>\n",
       "      <th>DC</th>\n",
       "      <th>GOVSEC</th>\n",
       "      <th>DATA</th>\n",
       "      <th>ECO</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>DRI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.732843</td>\n",
       "      <td>1.214491</td>\n",
       "      <td>1.199411</td>\n",
       "      <td>0.919845</td>\n",
       "      <td>1.080862</td>\n",
       "      <td>2.008562</td>\n",
       "      <td>1.397826</td>\n",
       "      <td>1.170769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.647035</td>\n",
       "      <td>1.140406</td>\n",
       "      <td>1.164747</td>\n",
       "      <td>0.798240</td>\n",
       "      <td>0.950223</td>\n",
       "      <td>1.854673</td>\n",
       "      <td>1.329306</td>\n",
       "      <td>1.072671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.475591</td>\n",
       "      <td>0.870687</td>\n",
       "      <td>0.633933</td>\n",
       "      <td>0.785840</td>\n",
       "      <td>0.928183</td>\n",
       "      <td>1.310896</td>\n",
       "      <td>0.873558</td>\n",
       "      <td>0.818084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.484569</td>\n",
       "      <td>1.055978</td>\n",
       "      <td>0.722968</td>\n",
       "      <td>0.765328</td>\n",
       "      <td>1.108889</td>\n",
       "      <td>1.528314</td>\n",
       "      <td>1.036700</td>\n",
       "      <td>0.926934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BAC</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.739170</td>\n",
       "      <td>1.219561</td>\n",
       "      <td>0.817715</td>\n",
       "      <td>1.259404</td>\n",
       "      <td>1.502800</td>\n",
       "      <td>2.012307</td>\n",
       "      <td>0.964983</td>\n",
       "      <td>1.230580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BAC</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.638730</td>\n",
       "      <td>1.051745</td>\n",
       "      <td>0.688897</td>\n",
       "      <td>1.225159</td>\n",
       "      <td>1.376890</td>\n",
       "      <td>2.017759</td>\n",
       "      <td>1.142673</td>\n",
       "      <td>1.134401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BRK-B</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.848677</td>\n",
       "      <td>1.271849</td>\n",
       "      <td>1.049523</td>\n",
       "      <td>1.524231</td>\n",
       "      <td>1.952482</td>\n",
       "      <td>2.036254</td>\n",
       "      <td>1.467172</td>\n",
       "      <td>1.408479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BRK-B</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.903161</td>\n",
       "      <td>1.458350</td>\n",
       "      <td>1.197646</td>\n",
       "      <td>1.616142</td>\n",
       "      <td>2.092728</td>\n",
       "      <td>2.241823</td>\n",
       "      <td>1.651319</td>\n",
       "      <td>1.544553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CSCO</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.044537</td>\n",
       "      <td>1.507539</td>\n",
       "      <td>1.043962</td>\n",
       "      <td>1.170067</td>\n",
       "      <td>1.587339</td>\n",
       "      <td>2.066058</td>\n",
       "      <td>1.480383</td>\n",
       "      <td>1.390529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CSCO</td>\n",
       "      <td>2019</td>\n",
       "      <td>1.002996</td>\n",
       "      <td>1.451169</td>\n",
       "      <td>1.014497</td>\n",
       "      <td>1.219425</td>\n",
       "      <td>1.624125</td>\n",
       "      <td>2.019357</td>\n",
       "      <td>1.474923</td>\n",
       "      <td>1.372443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker  year       ITC      ACAP        DC    GOVSEC      DATA       ECO  \\\n",
       "0   AAPL  2018  0.732843  1.214491  1.199411  0.919845  1.080862  2.008562   \n",
       "1   AAPL  2019  0.647035  1.140406  1.164747  0.798240  0.950223  1.854673   \n",
       "2   AMZN  2018  0.475591  0.870687  0.633933  0.785840  0.928183  1.310896   \n",
       "3   AMZN  2019  0.484569  1.055978  0.722968  0.765328  1.108889  1.528314   \n",
       "4    BAC  2018  0.739170  1.219561  0.817715  1.259404  1.502800  2.012307   \n",
       "5    BAC  2019  0.638730  1.051745  0.688897  1.225159  1.376890  2.017759   \n",
       "6  BRK-B  2018  0.848677  1.271849  1.049523  1.524231  1.952482  2.036254   \n",
       "7  BRK-B  2019  0.903161  1.458350  1.197646  1.616142  2.092728  2.241823   \n",
       "8   CSCO  2018  1.044537  1.507539  1.043962  1.170067  1.587339  2.066058   \n",
       "9   CSCO  2019  1.002996  1.451169  1.014497  1.219425  1.624125  2.019357   \n",
       "\n",
       "      OTHER       DRI  \n",
       "0  1.397826  1.170769  \n",
       "1  1.329306  1.072671  \n",
       "2  0.873558  0.818084  \n",
       "3  1.036700  0.926934  \n",
       "4  0.964983  1.230580  \n",
       "5  1.142673  1.134401  \n",
       "6  1.467172  1.408479  \n",
       "7  1.651319  1.544553  \n",
       "8  1.480383  1.390529  \n",
       "9  1.474923  1.372443  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 4: 計算數位韌性指數（DRI）\n",
    "# ========================================\n",
    "\n",
    "print(\"⚙ 計算數位韌性指數（DRI）...\")\n",
    "\n",
    "# 偵測實體和時間欄位\n",
    "entity_col = None\n",
    "for col in ['company', 'firm', 'ticker']:\n",
    "    if col in result_df.columns:\n",
    "        entity_col = col\n",
    "        break\n",
    "\n",
    "time_col = None\n",
    "for col in ['year', 'date']:\n",
    "    if col in result_df.columns:\n",
    "        time_col = col\n",
    "        break\n",
    "\n",
    "group_cols = [c for c in [entity_col, time_col] if c]\n",
    "\n",
    "if not group_cols:\n",
    "    print(\"  ⚠ 未偵測到 company/year，計算整體 DRI\")\n",
    "    agg = result_df[DIMENSIONS].mean().to_frame().T\n",
    "else:\n",
    "    print(f\"  - 按 {group_cols} 聚合\")\n",
    "    agg = result_df[group_cols + DIMENSIONS].groupby(group_cols).mean().reset_index()\n",
    "\n",
    "# 計算加權 DRI\n",
    "print(\"  - 使用加權平均計算 DRI\")\n",
    "dri_scores = np.zeros(len(agg))\n",
    "for dim in DIMENSIONS:\n",
    "    weight = DIMENSION_WEIGHTS.get(dim, 0)\n",
    "    dri_scores += agg[dim].values * weight\n",
    "\n",
    "agg['DRI'] = dri_scores\n",
    "\n",
    "# 保存\n",
    "agg.to_csv(PHASE4_DRI_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"✓ DRI 計算完成\")\n",
    "print(f\"  - 已保存: {PHASE4_DRI_CSV}\")\n",
    "print(f\"\\nDRI 統計:\")\n",
    "print(f\"  - 平均值: {agg['DRI'].mean():.3f}\")\n",
    "print(f\"  - 標準差: {agg['DRI'].std():.3f}\")\n",
    "print(f\"  - 範圍: [{agg['DRI'].min():.3f}, {agg['DRI'].max():.3f}]\")\n",
    "\n",
    "agg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase4_visualize",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "ticker=AAPL<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "AAPL",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "AAPL",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "hu6SVHi78j8wwa2QqCnxPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=AMZN<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "AMZN",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "AMZN",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "i8cxdL0t6j+qGttccqntPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=BAC<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "BAC",
         "line": {
          "color": "#00cc96",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "BAC",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AdSbrXSw8z/zgeuVgSbyPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=BRK-B<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "BRK-B",
         "line": {
          "color": "#ab63fa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "BRK-B",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "vpuMCyGJ9j+omSF6fbb4Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=CSCO<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "CSCO",
         "line": {
          "color": "#FFA15A",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "CSCO",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "ROHzRZs/9j/JmeA2h/X1Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=DIS<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "DIS",
         "line": {
          "color": "#19d3f3",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "DIS",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4wc=",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "1FfmQwGC+D8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=GOOGL<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "GOOGL",
         "line": {
          "color": "#FF6692",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "GOOGL",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "TreZR7rq+D+2v8FQdgD5Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=HD<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "HD",
         "line": {
          "color": "#B6E880",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "HD",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "eOpCEXfS9T+/6x9M0q/2Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=INTC<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "INTC",
         "line": {
          "color": "#FF97FF",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "INTC",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "x7HGJPLA9j+9NgYdb8L3Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=JNJ<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "JNJ",
         "line": {
          "color": "#FECB52",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "JNJ",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "Yd0fn4pB9D8ifOLkHDz1Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=JPM<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "JPM",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "JPM",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "hru54cB/9D8L9yJvoOHyPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=MA<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "MA",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "MA",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "tVcGBOix9z83iq9tMLD3Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=META<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "META",
         "line": {
          "color": "#00cc96",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "META",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "VSZxapQF8z/M3n/JHoTzPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=MSFT<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "MSFT",
         "line": {
          "color": "#ab63fa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "MSFT",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "QSby0Q+t8z8T6TpV8en0Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=PFE<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "PFE",
         "line": {
          "color": "#FFA15A",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "PFE",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "yZFZflKO7j+gQC7jJnLrPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=PG<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "PG",
         "line": {
          "color": "#19d3f3",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "PG",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "l6wYgBUE9T8AJa6XybnzPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=UNH<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "UNH",
         "line": {
          "color": "#FF6692",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "UNH",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "dvY1rCxs9D8rV1gY/gr1Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=V<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "V",
         "line": {
          "color": "#B6E880",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "V",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "4yHjPAxb+T9Q58DwEwD5Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=VZ<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "VZ",
         "line": {
          "color": "#FF97FF",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "VZ",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "P55TWQmb+D8JHQkupbT4Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "ticker=XOM<br>year=%{x}<br>DRI=%{y}<extra></extra>",
         "legendgroup": "XOM",
         "line": {
          "color": "#FECB52",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "XOM",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "4gfjBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "jN7KZ+ul+D+Sitlynan3Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "ticker"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "數位韌性指數（DRI）時序趨勢"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "year"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "DRI"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ 可視化完成\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 可視化 DRI\n",
    "# ========================================\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "if entity_col and time_col:\n",
    "    fig = px.line(\n",
    "        agg,\n",
    "        x=time_col,\n",
    "        y='DRI',\n",
    "        color=entity_col,\n",
    "        markers=True,\n",
    "        title='數位韌性指數（DRI）時序趨勢'\n",
    "    )\n",
    "    fig.show()\n",
    "elif entity_col:\n",
    "    fig = px.bar(\n",
    "        agg,\n",
    "        x=entity_col,\n",
    "        y='DRI',\n",
    "        title='各實體的數位韌性指數（DRI）'\n",
    "    )\n",
    "    fig.show()\n",
    "else:\n",
    "    fig = px.bar(\n",
    "        agg,\n",
    "        x=list(range(len(agg))),\n",
    "        y='DRI',\n",
    "        title='整體數位韌性指數（DRI）'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "print(\"\\n✓ 可視化完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "# 總結\n",
    "\n",
    "## 優化成果\n",
    "\n",
    "### 1. 集中配置管理\n",
    "- 所有關鍵參數在第一個 cell 統一管理\n",
    "- 易於調整和實驗\n",
    "\n",
    "### 2. Phase 3 多輪迭代\n",
    "- 支持多輪 LLM 優化（可配置）\n",
    "- 追蹤每輪的指標變化\n",
    "\n",
    "### 3. Phase 4 主題層級評分\n",
    "- **速度提升**: 從 6000+ 文檔 → ~70 主題\n",
    "- **準確性提升**: LLM 獲得主題關鍵詞，不再盲目猜測\n",
    "- 使用主題概率分佈加權計算文檔評分\n",
    "\n",
    "### 4. 緩存機制\n",
    "- 嵌入向量緩存\n",
    "- Phase 3 優化計劃緩存\n",
    "- Phase 4 主題映射和評分緩存\n",
    "\n",
    "### 5. 加權 DRI 計算\n",
    "- 支持自定義構面權重\n",
    "- 更靈活的指標計算\n",
    "\n",
    "## 輸出文件清單\n",
    "\n",
    "- `data/part2_bertopic_model/` - Phase 2 模型\n",
    "- `data/part2_topics.csv` - Phase 2 主題列表\n",
    "- `data/part2_corpus_with_topics.csv` - Phase 2 帶主題標註的語料\n",
    "- `data/part3_optimized_bertopic_model/` - Phase 3 優化模型\n",
    "- `data/part3_corpus_with_topics_v2.csv` - Phase 3 優化後語料\n",
    "- `data/phase3_optimization_plans.json` - 優化歷史記錄\n",
    "- `data/phase4_topic_dimension_map.json` - 主題-構面映射緩存\n",
    "- `data/phase4_topic_dimension_scores.json` - 主題評分緩存\n",
    "- `data/part4_doc_dimension_scores.csv` - 文檔級構面評分\n",
    "- `data/part4_entity_time_dri.csv` - 最終 DRI 指數"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "augmentation-agent-topic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
